{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/bin/python\n"
     ]
    }
   ],
   "source": [
    "#source /home/scarlett/github/ipython_notebook/Coding_pythons/myenv/bin/activate\n",
    "import sys\n",
    "print(sys.executable)\n",
    "\n",
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "from torch.nn import CrossEntropyLoss\n",
    "\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader, TensorDataset\n",
    "import torchmetrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To install new packages:\n",
    "- source /home/scarlett/github/Coding_pythons/newenv/bin/activate\n",
    "- pip install <package_name>\n",
    "- pip list\n",
    "- import <package_name>\n",
    "- deactivate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deep learning\n",
    "- deep learning is everywhere:\n",
    "    - language translation\n",
    "    - self-driving cars\n",
    "    - medical diagnostics\n",
    "    - chatbots\n",
    "\n",
    "- used on multiple data types: images, text and audio\n",
    "- traditional machine learning: relies on hand-crafted feature engineering\n",
    "- deep learning: enables feature learning from raw data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is deep learning?\n",
    "- An architecture of network consists of input, hidden layers and output\n",
    "- A network can have one or many hidden layers\n",
    "- Require 100K data\n",
    "- pytorch supports tabular data, also unstructral data like image (torchvision), audio data (torchaudio), text data (torchtext)\n",
    "- deep learning often requires a GPU, which, compard to a CPU can offer: parallel computing capabilities, faster training times and better performance\n",
    "- tensors are multidimensional representations of their elements"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### make tensor: load from list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1, 2, 3],\n",
       "        [4, 5, 6],\n",
       "        [7, 8, 9]])"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lst = [[1,2,3],[4,5,6],[7,8,9]]\n",
    "tensor = torch.tensor(lst)\n",
    "tensor "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 3])"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tensor.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.int64"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tensor.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cpu')"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tensor.device"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### load from NumPy array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "array = [[1,2,3],[4,5,6],[7,8,9]]\n",
    "np_array = np.array(array)\n",
    "np_tensor = torch.from_numpy(np_array)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### tensor operations - most numpy array operations can be performedon PyTorch tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 2,  4,  6],\n",
       "        [ 8, 10, 12],\n",
       "        [14, 16, 18]])"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tensor + np_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 1,  4,  9],\n",
       "        [16, 25, 36],\n",
       "        [49, 64, 81]])"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tensor * np_tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## neural net\n",
    "### basic two layer NN without hidden layer\n",
    "- first layer: input, second layer: linear layer\n",
    "- each linear layer has a .weight and .bias associated\n",
    "- nn.linear performs\n",
    "    - when input_tensor is passed to linear_layer, the linear operation performed is matrix multiplication of input_tensor and the weights, followed by adding in the bias\n",
    "    - input X, weights W0, bias b0 --> y0 = W0 * X +b0; in pytorch: output = W0 * input + b0\n",
    "    - initially, when we call nn.Linear(), weights and biases are initialized randomly, so they are not yet useful\n",
    "    - we have to tune these weights and biases\n",
    "- two-layer network summary:\n",
    "    - took 1 by 3 input as the first layer (1 linear layer with specific arguments as the second layer, and retured a 1 by 2 output)\n",
    "    - linear layers have connections (or arrows) between each input and output neuron, making them fully connected\n",
    "        - networks with only linear layers are called: \"fully connected networks\"\n",
    "        - each neuron in one layer is connected to each neuron in the next layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.4958, -0.3643]], grad_fn=<AddmmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "## create input_tensor with 3 features\n",
    "input_tensor = torch.tensor([[0.3471, 0.4547, -0.2356]], dtype=torch.float32)\n",
    "\n",
    "## define first linear layer ( a linear layer takes an input, applies a linear fxn, and returns output)\n",
    "linear_layer = nn.Linear(in_features=3, out_features=2)\n",
    "\n",
    "## pass input through linear layer\n",
    "output = linear_layer(input_tensor)\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "linear_layer.weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "linear_layer.bias"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multi-layer model: Stacking layers with nn.Sequential()\n",
    "- this model takes input, passes it to each linear layer in sequence, and returns output\n",
    "    - the first layer takes input with 10 features, and output with a tensor with 18 features\n",
    "    - the second layer takes input with 18 features, and output with a tensor with 20 features\n",
    "    - the third layer takess input with 20 featuresl and output with a tensor with 5 features\n",
    "- output is not linear until each layer has tuned weights and biases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = nn.Sequential(\n",
    "    nn.Linear(10,18), \n",
    "    nn.Linear(18,20), \n",
    "    nn.Linear(20,5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make a input_tensor with ten features (neurons)\n",
    "input_tensor = torch.rand(1,10)\n",
    "input_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pass input_tensor through model\n",
    "output = model(input_tensor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### In this exercise, you will implement a small neural network containing two linear layers. The first layer takes an eight-dimensional input, and the last layer outputs a one-dimensional tensor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_tensor = torch.Tensor([[2, 3, 6, 7, 9, 3, 2, 1]])\n",
    "\n",
    "# Implement a small neural network with exactly two linear layers\n",
    "model = nn.Sequential(nn.Linear(8,5),\n",
    "                      nn.Linear(5,1)\n",
    "                     )\n",
    "\n",
    "output = model(input_tensor)\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Activation functions\n",
    "-  used in the last layer of NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chapter 2: Training First NN with PyTorch\n",
    "### Section 2.1 Running a forward pass\n",
    "- input data is **passed forward** or **propagated** through a network\n",
    "- computations performed at each layer\n",
    "- outputs of each layer passed to each subsequent layer\n",
    "- output of final layer: \"prediction\"\n",
    "- used for both **training** and prediction\n",
    "- some possible outputs:\n",
    "    - binary classification (sum up to 1)\n",
    "    - multiclass classification (sum up to 1, value with highest probability is assigned predicted label in each row)\n",
    "    - regression values\n",
    "- results would not be meaningful untill we use backpropagation to update weights and biases\n",
    "- there is also a backward pass\n",
    "    - used to update weights and baises during training\n",
    "    - in the training loop: \n",
    "        1. propagate data forward\n",
    "        2. compare outputs to the true values (ground-truth)\n",
    "        3. backpropagate to update model weights and baises\n",
    "        4. repeaat weights and biases are tuned to produce useful outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### binary classificationl forward pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.3303,  0.2862, -1.0250,  0.2587, -0.0196, -0.1723],\n",
       "        [-0.8667,  1.0218, -2.2905, -0.2409, -0.4707,  1.5993],\n",
       "        [ 0.0264,  0.5633,  1.2191, -0.4299,  0.9548, -0.7300],\n",
       "        [ 0.1807,  0.9939,  1.2226, -0.6087,  0.9974, -1.3976],\n",
       "        [ 1.4309, -0.6085,  0.1766, -0.1891,  0.3819,  0.0889]])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create input data of shape 5*6\n",
    "data = torch.randn(5, 6)\n",
    "input_data = data\n",
    "input_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.4012],\n",
      "        [0.3571],\n",
      "        [0.3702],\n",
      "        [0.3924],\n",
      "        [0.4166]], grad_fn=<SigmoidBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# create binary classification model\n",
    "model = nn.Sequential(\n",
    "    nn.Linear(6, 3),\n",
    "    nn.Linear(3, 1),\n",
    "    nn.Sigmoid()\n",
    ")\n",
    "\n",
    "# pass input data through model\n",
    "output = model(input_data)\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### multi-class classificationl forward pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([5, 3])\n"
     ]
    }
   ],
   "source": [
    "# specifcy model has 3 clases\n",
    "n_classes = 3\n",
    "\n",
    "# create 2-layer multi-class classification model\n",
    "model = nn.Sequential(\n",
    "    nn.Linear(6, 4),\n",
    "    nn.Linear(4, n_classes),\n",
    "    nn.Softmax(dim=-1)\n",
    ")\n",
    "\n",
    "# pass input data through model\n",
    "output = model(input_data)\n",
    "print(output.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### regression forward pass\n",
    "- no activation layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.2892],\n",
      "        [-0.1950],\n",
      "        [-0.1918],\n",
      "        [-0.2454],\n",
      "        [-0.2936]], grad_fn=<AddmmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# create regression model\n",
    "model = nn.Sequential(\n",
    "    nn.Linear(6, 4),\n",
    "    nn.Linear(4, 1)\n",
    ")\n",
    "\n",
    "# pass input data through model\n",
    "output = model(input_data)\n",
    "\n",
    "# return output\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Section 2.2 Using loss function to assess model predictions\n",
    "#### Loss function\n",
    "- gives feedback to model during training\n",
    "- takes in model prediction y_hat and ground truth y\n",
    "- outputs a float\n",
    "- example: predicted class = 0\n",
    "    - if correct = low loss\n",
    "    - if wrong = high loss\n",
    "- Goal: minimize loss\n",
    "- one-hot encoding concepts: loss = F(y, y_hat)\n",
    "    - **y is a single integer** (class label) e.g. y = 0 when y is mammal\n",
    "    - **y_hat is a tensor** (output of softmax)\n",
    "        - if N is the number of classes, e.g. N = 3\n",
    "        - y_hat is a tensor with N dimensions, e.g. y_hat = [0.5,0.8,0.1]\n",
    "    - transform true label to tensor of zeros and ones\n",
    "        - ground truth y = 0\n",
    "        - number of classes N = 3\n",
    "        - class: 0  1  2 \n",
    "        - one-hot encoding: 1  0  0 / \n",
    "                            0  1  0 /\n",
    "                            0  1  1 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1, 0, 0],\n",
       "        [0, 1, 0],\n",
       "        [0, 0, 1]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "F.one_hot(torch.tensor([0, 1, 2]), num_classes=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1, 0, 0])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "F.one_hot(torch.tensor(0), num_classes=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### loss function takes\n",
    "- scores\n",
    "    - model predictions before the final softmax function\n",
    "- one_hot_target\n",
    "    - one hot encoded ground truth label\n",
    "#### loss function output\n",
    "- loss\n",
    "    - a single float"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.81307452917099\n"
     ]
    }
   ],
   "source": [
    "# Correct way to create tensors\n",
    "scores = torch.tensor([[-0.1211, 0.1059]])  # Raw scores (logits), not probabilities\n",
    "# Target should be the index of the correct class, not one-hot encoded\n",
    "class_target = torch.tensor([0])  # Class index, e.g., class 0\n",
    "\n",
    "# Instantiate the CrossEntropyLoss\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Compute the loss\n",
    "loss = criterion(scores, class_target)\n",
    "\n",
    "print(\"Loss:\", loss.item())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Creating one-hot encoded labels**\n",
    "- Manually create a one-hot encoded vector of the ground truth label y by filling in the NumPy array provided.\n",
    "- Create a one-hot encoded vector of the ground truth label y using PyTorch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = 1\n",
    "num_classes = 3\n",
    "\n",
    "# Create the one-hot encoded vector using NumPy\n",
    "one_hot_numpy = np.array([0, 1, 0])\n",
    "\n",
    "# Create the one-hot encoded vector using PyTorch\n",
    "one_hot_pytorch = F.one_hot(torch.tensor(y), num_classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Calculating cross entropy loss**\n",
    "- Create the one-hot encoded vector of the ground truth label y and assign it to one_hot_label.\n",
    "- Create the cross entropy loss function and store it as criterion.\n",
    "- Calculate the cross entropy loss using the one_hot_label vector and the scores vector, by calling the loss_function you created."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(8.0619, dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "y = [2]\n",
    "scores = torch.tensor([[0.1, 6.0, -2.0, 3.2]])\n",
    "\n",
    "# Create a one-hot encoded vector of the label y\n",
    "one_hot_label = F.one_hot(torch.tensor(y), scores.shape[1])\n",
    "\n",
    "# Create the cross entropy loss function\n",
    "criterion = CrossEntropyLoss()\n",
    "\n",
    "# Calculate the cross entropy loss\n",
    "loss = criterion(scores.double(), one_hot_label.double())\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Section 2.3 Using derivatives to update model parameters\n",
    "- goal: minimize the loss\n",
    "    - high loss: wrong prediction\n",
    "    - low loss: correct prediction\n",
    "- hiking down a mountain to the valley floor\n",
    "    - steep slopes: a step makes us lose a lot of elevation = derivative is high (red arrows)\n",
    "    - gentle slopes: a step makes us lose a little bit of elevation = detivative is low (green arrows)\n",
    "    - valley floor: not losing elevation by taking a step = derivative is null\n",
    "- connecting derivatives and model training\n",
    "    - model training: updating a model's parameters to minimize the loss\n",
    "        - we take a dataset with features X, and groud truth y. We run a forward pass using X and calculate loss by comparing model output, y_hat, with y. we compute gradients of the loss function and use them to update the model parameters with backpropagation so that weights are no longer random and biases are useful. we repeat until the layers are tuned.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Backpropagation concepts\n",
    "- consider a network made of 3 layers. L0, L1, L2\n",
    "    - calculate local gradients for L0, L1 and L2 using backpropagation\n",
    "    - calculate loss gradients with respect to L2, then use L2 gradients to calcualte L1 gradients, and so on"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the model and run a forward pass\n",
    "model = nn.Sequential(\n",
    "    nn.Linear(6, 4),\n",
    "    nn.Linear(4, 3),\n",
    "    nn.Linear(3,2))\n",
    "prediction = model(input_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "Target 2 is out of bounds.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[39], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# calculate the loss and compute the gradients\u001b[39;00m\n\u001b[1;32m      2\u001b[0m criterion \u001b[38;5;241m=\u001b[39m CrossEntropyLoss()\n\u001b[0;32m----> 3\u001b[0m loss \u001b[38;5;241m=\u001b[39m \u001b[43mcriterion\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprediction\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      4\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/loss.py:1188\u001b[0m, in \u001b[0;36mCrossEntropyLoss.forward\u001b[0;34m(self, input, target)\u001b[0m\n\u001b[1;32m   1187\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor, target: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m-> 1188\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcross_entropy\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1189\u001b[0m \u001b[43m                           \u001b[49m\u001b[43mignore_index\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mignore_index\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreduction\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreduction\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1190\u001b[0m \u001b[43m                           \u001b[49m\u001b[43mlabel_smoothing\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlabel_smoothing\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/functional.py:3104\u001b[0m, in \u001b[0;36mcross_entropy\u001b[0;34m(input, target, weight, size_average, ignore_index, reduce, reduction, label_smoothing)\u001b[0m\n\u001b[1;32m   3102\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m size_average \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m reduce \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   3103\u001b[0m     reduction \u001b[38;5;241m=\u001b[39m _Reduction\u001b[38;5;241m.\u001b[39mlegacy_get_string(size_average, reduce)\n\u001b[0;32m-> 3104\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_C\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_nn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcross_entropy_loss\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_Reduction\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_enum\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreduction\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mignore_index\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabel_smoothing\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mIndexError\u001b[0m: Target 2 is out of bounds."
     ]
    }
   ],
   "source": [
    "# calculate the loss and compute the gradients\n",
    "criterion = CrossEntropyLoss()\n",
    "loss = criterion(prediction, target)\n",
    "loss.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(None, None)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# access each layer's gradidents\n",
    "model[0].weight.grad, model[0].bias.grad\n",
    "model[1].weight.grad, model[1].bias.grad\n",
    "model[2].weight.grad, model[2].bias.grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### updating model parametes\n",
    "- update the weights by subtracting local gradients scaled by the learning rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "unsupported operand type(s) for *: 'float' and 'NoneType'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[41], line 7\u001b[0m\n\u001b[1;32m      5\u001b[0m weight \u001b[38;5;241m=\u001b[39m model[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mweight\n\u001b[1;32m      6\u001b[0m weight_grad \u001b[38;5;241m=\u001b[39m model[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mweight\u001b[38;5;241m.\u001b[39mgrad\n\u001b[0;32m----> 7\u001b[0m weight \u001b[38;5;241m=\u001b[39m weight \u001b[38;5;241m-\u001b[39m \u001b[43mlr\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mweight_grad\u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;66;03m# update the bias\u001b[39;00m\n\u001b[1;32m     10\u001b[0m bias \u001b[38;5;241m=\u001b[39m model[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mbias\n",
      "\u001b[0;31mTypeError\u001b[0m: unsupported operand type(s) for *: 'float' and 'NoneType'"
     ]
    }
   ],
   "source": [
    "# learning rate is typically small\n",
    "lr = 0.001\n",
    "\n",
    "# update the weights\n",
    "weight = model[0].weight\n",
    "weight_grad = model[0].weight.grad\n",
    "weight = weight - lr * weight_grad\n",
    "\n",
    "# update the bias\n",
    "bias = model[0].bias\n",
    "bias_grad = model[0].bias.grad\n",
    "bias = bias - lr * bias_grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### convex and non-convex funtions\n",
    "- convex function has one global minimum\n",
    "- non-convex function has more than 1 global minimum (loss function in DL)\n",
    "\n",
    "#### gradient descent\n",
    "- for non-convex functins, the iterative process used is GD\n",
    "- in PyTorch, an optimizer takes care of weight updates\n",
    "- the most common optimizer is stochastic gradient descent (SGD)\n",
    "- optimizer handles updating model parameters (or weights) after calculation of local gradients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the optimizer\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.0001)\n",
    "optimizer.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Question:\n",
    "- The following tensors are provided:\n",
    "    - weight: a 2 by 9 element tensor\n",
    "    - bias: a 2 element tensor\n",
    "    - preds: a 1 by 2 element tensor containing the model predictions\n",
    "    - target: a 1 by 2 element one-hot encoded tensor containing the ground-truth label\n",
    "\n",
    "- Use the criterion you have defined to calculate the loss value with respect to the predictions and target values.\n",
    "- Compute the gradients of the cross entropy loss.\n",
    "- Display the gradients of the weight and bias tensors, in that order."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Calculate the loss\n",
    "loss = criterion(preds, target)\n",
    "\n",
    "# Compute the gradients of the loss\n",
    "loss.backward()\n",
    "\n",
    "# Display gradients of the weight and bias tensors in order\n",
    "print(weight.grad)\n",
    "print(bias.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Question: A PyTorch model created with the nn.Sequential() is a module that contains the different layers of your network. Recall that each layer parameter can be accessed by indexing the created model directly. In this exercise, you will practice accessing the parameters of different linear layers of a neural network. You won't be accessing the sigmoid.\n",
    "- Access the weight parameter of the first linear layer.\n",
    "- Access the bias parameter of the second linear layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = nn.Sequential(nn.Linear(16, 8),\n",
    "                      nn.Sigmoid(),\n",
    "                      nn.Linear(8, 2))\n",
    "\n",
    "# Access the weight of the first linear layer\n",
    "weight_0 = model[0].weight\n",
    "\n",
    "# Access the bias of the second linear layer\n",
    "bias_1 = model[2].bias"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Question: Updating the weights manually\n",
    "- Create the gradient variables by accessing the local gradients of each weight tensor.\n",
    "- Update the weights using the gradients scaled by the learning rate.\n",
    "- Use optim to create an SGD optimizer with a learning rate of your choice (must be less than one) for the model provided.\n",
    "- Update the model's parameters using the optimizer.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'Sigmoid' object has no attribute 'weight'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[47], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m weight0 \u001b[38;5;241m=\u001b[39m model[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mweight\n\u001b[0;32m----> 2\u001b[0m weight1 \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\n\u001b[1;32m      3\u001b[0m weight2 \u001b[38;5;241m=\u001b[39m model[\u001b[38;5;241m2\u001b[39m]\u001b[38;5;241m.\u001b[39mweight\n\u001b[1;32m      5\u001b[0m \u001b[38;5;66;03m# Access the gradients of the weight of each linear layer\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1729\u001b[0m, in \u001b[0;36mModule.__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   1727\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m modules:\n\u001b[1;32m   1728\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m modules[name]\n\u001b[0;32m-> 1729\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(\u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m object has no attribute \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'Sigmoid' object has no attribute 'weight'"
     ]
    }
   ],
   "source": [
    "weight0 = model[0].weight\n",
    "weight1 = model[1].weight\n",
    "weight2 = model[2].weight\n",
    "\n",
    "# Access the gradients of the weight of each linear layer\n",
    "grads0 = weight0.grad\n",
    "grads1 = weight1.grad\n",
    "grads2 = weight2.grad\n",
    "\n",
    "# Update the weights using the learning rate and the gradients\n",
    "weight0 = weight0 - lr*grads0\n",
    "weight1 = weight1 - lr*grads1\n",
    "weight2 = weight2 - lr*grads2\n",
    "\n",
    "# Create the optimizer\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.001)\n",
    "\n",
    "loss = criterion(pred, target)\n",
    "loss.backward()\n",
    "\n",
    "# Update the model's parameters using the optimizer\n",
    "optimizer.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Section 2.4 Writing our first training loop\n",
    "- create a model\n",
    "- choose a loss function\n",
    "- create a dataset (normalize, missing, imbalance)\n",
    "- define an optimizer\n",
    "- run a training loop, where for each sample of the dataset, we repeat:\n",
    "    - calculating loss(forward pass)\n",
    "    - calculating local gradients\n",
    "    - updating model parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example: data science salary dataset (regression problem)\n",
    "- no softmax or sigmoid as last activation function\n",
    "- last layer is linear layer\n",
    "- **loss function: MSE loss** --> mean (sum(prediction - target)^2)\n",
    "    - prediction and targets must be a float tensor\n",
    "- regression problem does not need one-hot encoding and the final linear layer outputs a single float"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the dataset and the dataloader\n",
    "dataset = TensorDataset(torch.tensor(features).float(), torch.tensor(target).float())\n",
    "dataloader = DataLoader(dataset, batch_size=4, shuffle=True)\n",
    "\n",
    "# create the model\n",
    "model = nn.Sequential(nn.Linear(6, 4),\n",
    "                      nn.Linear(2,1))\n",
    "\n",
    "# create the loss and optimizer\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loop through the dataset multiple times - loop through data once is called an epoch\n",
    "# each iteration of the dataoader provides a batch of samples\n",
    "for epoch in range(num_epochs):\n",
    "    for data in dataLoader:\n",
    "        # set the gradients to zero\n",
    "        optimizer.zero_grad()\n",
    "        # get feature and target from the data loader\n",
    "        features, target = data\n",
    "        # run a forward pass\n",
    "        pred = model(features)\n",
    "        # calculate the loss and gradients\n",
    "        loss = criterion(pred, target)\n",
    "        loss.backward()\n",
    "        # update the weights\n",
    "        optimizer.step()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Practice: using the MSELoss\n",
    "- Calculate the MSELoss using NumPy.\n",
    "- Create a MSELoss function using PyTorch.\n",
    "- Convert y_hat and y to tensors and then float data types, and then use them to calculate MSELoss using PyTorch as mse_pytorch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_hat = np.array(10)\n",
    "y = np.array(1)\n",
    "\n",
    "# Calculate the MSELoss using NumPy\n",
    "mse_numpy = np.mean((y_hat - y)**2)\n",
    "\n",
    "# Create the MSELoss function\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "# Calculate the MSELoss using the created loss function\n",
    "mse_pytorch = criterion(torch.tensor(y_hat).float(), torch.tensor(y).float())\n",
    "print(mse_pytorch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Practice: writing a training loop\n",
    "- Write a for loop that iterates over the dataloader; this should be nested within a for loop that iterates over a range equal to the number of epochs.\n",
    "- Set the gradients of the optimizer to zero.\n",
    "- Write the forward pass.\n",
    "- Compute the MSE loss value using the criterion() function provided.\n",
    "- Compute the gradients.\n",
    "- Update the model's parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loop over the number of epochs and the dataloader\n",
    "for i in range(num_epochs):\n",
    "  for data in dataloader:\n",
    "    # Set the gradients to zero\n",
    "    optimizer.zero_grad()\n",
    "    # Run a forward pass\n",
    "    feature, target = data\n",
    "    prediction = model(feature)    \n",
    "    # Calculate the loss\n",
    "    loss = criterion(prediction, target)    \n",
    "    # Compute the gradients\n",
    "    loss.backward()\n",
    "    # Update the model's parameters\n",
    "    optimizer.step()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chapter 3: NN architecture and hyperparameters\n",
    "### Section 3.1 discover activation functions between layers\n",
    "- sigmoid: output will be bounded between 0 and 1\n",
    "    - gradient vanishing problem that gradient is close to zero for low or high values\n",
    "- softmax can only be used in the last layer\n",
    "- ReLU: rectified linear unit\n",
    "    - fx = max(x, 0)\n",
    "    - overcome vanishing gradients problem\n",
    "- leaky ReLU: for negative inputs, it multiplies the input by a small coefficient (default to 0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "relu = nn.ReLU()\n",
    "leaky_relu = nn.LeakyReLU(negative_slope = 0.05)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Question: In this exercise, you'll begin with a ReLU implementation in PyTorch. Next, you'll calculate the gradients of the function.\n",
    "- Calculate the gradient of the ReLU function for x using the relu_pytorch() function you defined, then running a backward pass.\n",
    "- Find the gradient at x."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a ReLU function with PyTorch\n",
    "relu_pytorch = nn.ReLU()\n",
    "\n",
    "# Apply your ReLU function on x, and calculate gradients\n",
    "x = torch.tensor(-1.0, requires_grad=True)\n",
    "y = relu_pytorch(x)\n",
    "y.backward()\n",
    "\n",
    "# Print the gradient of the ReLU function for x\n",
    "gradient = x.grad\n",
    "print(gradient)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Question: In this exercise, you will implement the leaky ReLU function in NumPy and PyTorch and practice using it. The numpy as np package, the torch package as well as the torch.nn as nn have already been imported.\n",
    "- Create a leaky ReLU function in PyTorch with a negative slope of 0.05.\n",
    "- Call the function on the tensor x, which has already been defined for you."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a leaky relu function in PyTorch\n",
    "leaky_relu_pytorch = nn.LeakyReLU(negative_slope = 0.05)\n",
    "\n",
    "x = torch.tensor(-2.0)\n",
    "# Call the above function on the tensor x\n",
    "output = leaky_relu_pytorch(x)\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Section 3.2 A deeper dive into NN architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Counting the number of parameters\n",
    "- Iterate through the model's parameters to update the total variable with the total number of parameters in the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "81\n"
     ]
    }
   ],
   "source": [
    "model = nn.Sequential(nn.Linear(16, 4),\n",
    "                      nn.Linear(4, 2),\n",
    "                      nn.Linear(2, 1))\n",
    "\n",
    "total = 0\n",
    "\n",
    "# Calculate the number of parameters in the model\n",
    "for parameter in model.parameters():\n",
    "  total += parameter.numel()\n",
    "\n",
    "print(total)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Manipulating the capacity of a network\n",
    "- Create a neural network with exactly three linear layers and less than 120 parameters, which takes n_features as inputs and outputs n_classes.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_capacity(model):\n",
    "  total = 0\n",
    "  for p in model.parameters():\n",
    "    total += p.numel()\n",
    "  return total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "92\n"
     ]
    }
   ],
   "source": [
    "n_features = 8\n",
    "n_classes = 2\n",
    "\n",
    "input_tensor = torch.Tensor([[3, 4, 6, 2, 3, 6, 8, 9]])\n",
    "\n",
    "# Create a neural network with less than 120 parameters\n",
    "model = nn.Sequential(nn.Linear(8, 6),\n",
    "                      nn.Linear(6, 4),\n",
    "                      nn.Linear(4, 2))\n",
    "\n",
    "output = model(input_tensor)\n",
    "\n",
    "print(calculate_capacity(model))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Section 3.3 Learning rate and momentum\n",
    "- model's architecture impacts the training process and the model's performance\n",
    "- training a NN is actualy solving an optimization problem:\n",
    "    - minimize loss function by tweaking model's parameters (SGD)\n",
    "- study the impact of the learning rate and momentum on the training process\n",
    "    - right learning rate:\n",
    "        - find the minimum of the function shown\n",
    "        - run SGD optimizer for 10 steps and start at X equals -2\n",
    "        - after 10 steps, the optimizer has almost found the minimum of the function \n",
    "        - the step size taken by the optimizer is getting smaller as we are getting closer to X = 0\n",
    "        - step size = gradient multiplied by the learning rate\n",
    "        - around zero, this function is not as steep and therefore the gradient is smaller\n",
    "    - Low: if we use the same algorithm for a learning rate 10 times smaller, we are still far from the minimum of the function after 10 steps. The optimizer will take much longer to find the functions minimum\n",
    "    - High: if high value for learning rate, the optimizer cannot find the minimum and bounces back and forth on both sides of the function \n",
    "- One challenege is that when try to find the minimum of a non-convex function is getting stuck in a local minimum\n",
    "    - without momentum - can only find local minimum \n",
    "    - with momentum - can find global minimum"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Experimenting with learning rate\n",
    "- In this exercise, your goal is to find the optimal learning rate such that the optimizer can find the minimum of the non-convex function \n",
    " in ten steps.\n",
    "- You will experiment with three different learning rate values. For this problem, try learning rate values between 0.001 to 0.1.\n",
    "- You are provided with the optimize_and_plot() function that takes the learning rate for the first argument. This function will run 10 steps of the SGD optimizer and display the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# smaller value - hard to find global minima    \n",
    "lr2 = 0.09\n",
    "optimize_and_plot(lr=lr2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Experimenting with momentum\n",
    "- In this exercise, your goal is to find the optimal momentum such that the optimizer can find the minimum of the following non-convex function \n",
    " in 20 steps. You will experiment with two different momentum values. For this problem, the learning rate is fixed at 0.01.\n",
    "- You are provided with the optimize_and_plot() function that takes the learning rate for the first argument. This function will run 20 steps of the SGD optimizer and display the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lower momentum value - harder to find global minimum\n",
    "mom1 = 0.95\n",
    "optimize_and_plot(momentum=mom1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Section 3.4 Layer initialization and transfer \n",
    "- layer initialization\n",
    "    - initialize small values\n",
    "    - sampling from uniform distribution (nn.init.uniform)\n",
    "- transfer learning: reusing a model trained on a first task for a second similar task, to accelerate the training process\n",
    "    - fine-tuning: smaller learning rate, not every layer is trained\n",
    "        - find a model trained on a similar task\n",
    "        - load pre-trained weights\n",
    "        - free none or some of the layers in the model\n",
    "        - train with a smaller learning rate\n",
    "        - look at the loss values and see if the learning rate needs to be adjusted"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Freeze layers of a model\n",
    "- You are about to fine-tune a model on a new task after loading pre-trained weights. The model contains three linear layers. However, because your dataset is small, you only want to train the last linear layer of this model and freeze the first two linear layers.\n",
    "- The model has already been created and exists under the variable model. You will be using the named_parameters method of the model to list the parameters of the model. Each parameter is described by a name. This name is a string with the following naming convention: x.name where x is the index of the layer.\n",
    "- Remember that a linear layer has two parameters: the weight and the bias."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for name, param in model.named_parameters():\n",
    "  \n",
    "    # Check if the parameters belong to the first layer\n",
    "    if name == '0.weight' or name == '0.bias':\n",
    "   \n",
    "        # Freeze the parameters\n",
    "        param.requires_grad = False\n",
    "        \n",
    "    # Check if the parameters belong to the second layer\n",
    "    if name == '1.weight' or name == '1.bias':\n",
    "      \n",
    "        # Freeze the parameters\n",
    "        param.requires_grad = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Layer initialization\n",
    "- The initialization of the weights of a neural network has been the focus of researchers for many years. When training a network, the method used to initialize the weights has a direct impact on the final performance of the network.\n",
    "- As a machine learning practitioner, you should be able to experiment with different initialization strategies. In this exercise, you are creating a small neural network made of two layers and you are deciding to initialize each layer's weights with the uniform method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "layer0 = nn.Linear(16, 32)\n",
    "layer1 = nn.Linear(32, 64)\n",
    "\n",
    "# Use uniform initialization for layer0 and layer1 weights\n",
    "nn.init.uniform_(layer0.weight)\n",
    "nn.init.uniform_(layer1.weight)\n",
    "\n",
    "model = nn.Sequential(layer0, layer1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chapter 4: Evaluating and improving models\n",
    "### Section 4.1 A deeper dive into loading data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[5.1, 3.5, 1.4, 0.2],\n",
       "       [4.9, 3. , 1.4, 0.2],\n",
       "       [4.7, 3.2, 1.3, 0.2],\n",
       "       [4.6, 3.1, 1.5, 0.2],\n",
       "       [5. , 3.6, 1.4, 0.2],\n",
       "       [5.4, 3.9, 1.7, 0.4],\n",
       "       [4.6, 3.4, 1.4, 0.3],\n",
       "       [5. , 3.4, 1.5, 0.2],\n",
       "       [4.4, 2.9, 1.4, 0.2],\n",
       "       [4.9, 3.1, 1.5, 0.1],\n",
       "       [5.4, 3.7, 1.5, 0.2],\n",
       "       [4.8, 3.4, 1.6, 0.2],\n",
       "       [4.8, 3. , 1.4, 0.1],\n",
       "       [4.3, 3. , 1.1, 0.1],\n",
       "       [5.8, 4. , 1.2, 0.2],\n",
       "       [5.7, 4.4, 1.5, 0.4],\n",
       "       [5.4, 3.9, 1.3, 0.4],\n",
       "       [5.1, 3.5, 1.4, 0.3],\n",
       "       [5.7, 3.8, 1.7, 0.3],\n",
       "       [5.1, 3.8, 1.5, 0.3],\n",
       "       [5.4, 3.4, 1.7, 0.2],\n",
       "       [5.1, 3.7, 1.5, 0.4],\n",
       "       [4.6, 3.6, 1. , 0.2],\n",
       "       [5.1, 3.3, 1.7, 0.5],\n",
       "       [4.8, 3.4, 1.9, 0.2],\n",
       "       [5. , 3. , 1.6, 0.2],\n",
       "       [5. , 3.4, 1.6, 0.4],\n",
       "       [5.2, 3.5, 1.5, 0.2],\n",
       "       [5.2, 3.4, 1.4, 0.2],\n",
       "       [4.7, 3.2, 1.6, 0.2],\n",
       "       [4.8, 3.1, 1.6, 0.2],\n",
       "       [5.4, 3.4, 1.5, 0.4],\n",
       "       [5.2, 4.1, 1.5, 0.1],\n",
       "       [5.5, 4.2, 1.4, 0.2],\n",
       "       [4.9, 3.1, 1.5, 0.2],\n",
       "       [5. , 3.2, 1.2, 0.2],\n",
       "       [5.5, 3.5, 1.3, 0.2],\n",
       "       [4.9, 3.6, 1.4, 0.1],\n",
       "       [4.4, 3. , 1.3, 0.2],\n",
       "       [5.1, 3.4, 1.5, 0.2],\n",
       "       [5. , 3.5, 1.3, 0.3],\n",
       "       [4.5, 2.3, 1.3, 0.3],\n",
       "       [4.4, 3.2, 1.3, 0.2],\n",
       "       [5. , 3.5, 1.6, 0.6],\n",
       "       [5.1, 3.8, 1.9, 0.4],\n",
       "       [4.8, 3. , 1.4, 0.3],\n",
       "       [5.1, 3.8, 1.6, 0.2],\n",
       "       [4.6, 3.2, 1.4, 0.2],\n",
       "       [5.3, 3.7, 1.5, 0.2],\n",
       "       [5. , 3.3, 1.4, 0.2],\n",
       "       [7. , 3.2, 4.7, 1.4],\n",
       "       [6.4, 3.2, 4.5, 1.5],\n",
       "       [6.9, 3.1, 4.9, 1.5],\n",
       "       [5.5, 2.3, 4. , 1.3],\n",
       "       [6.5, 2.8, 4.6, 1.5],\n",
       "       [5.7, 2.8, 4.5, 1.3],\n",
       "       [6.3, 3.3, 4.7, 1.6],\n",
       "       [4.9, 2.4, 3.3, 1. ],\n",
       "       [6.6, 2.9, 4.6, 1.3],\n",
       "       [5.2, 2.7, 3.9, 1.4],\n",
       "       [5. , 2. , 3.5, 1. ],\n",
       "       [5.9, 3. , 4.2, 1.5],\n",
       "       [6. , 2.2, 4. , 1. ],\n",
       "       [6.1, 2.9, 4.7, 1.4],\n",
       "       [5.6, 2.9, 3.6, 1.3],\n",
       "       [6.7, 3.1, 4.4, 1.4],\n",
       "       [5.6, 3. , 4.5, 1.5],\n",
       "       [5.8, 2.7, 4.1, 1. ],\n",
       "       [6.2, 2.2, 4.5, 1.5],\n",
       "       [5.6, 2.5, 3.9, 1.1],\n",
       "       [5.9, 3.2, 4.8, 1.8],\n",
       "       [6.1, 2.8, 4. , 1.3],\n",
       "       [6.3, 2.5, 4.9, 1.5],\n",
       "       [6.1, 2.8, 4.7, 1.2],\n",
       "       [6.4, 2.9, 4.3, 1.3],\n",
       "       [6.6, 3. , 4.4, 1.4],\n",
       "       [6.8, 2.8, 4.8, 1.4],\n",
       "       [6.7, 3. , 5. , 1.7],\n",
       "       [6. , 2.9, 4.5, 1.5],\n",
       "       [5.7, 2.6, 3.5, 1. ],\n",
       "       [5.5, 2.4, 3.8, 1.1],\n",
       "       [5.5, 2.4, 3.7, 1. ],\n",
       "       [5.8, 2.7, 3.9, 1.2],\n",
       "       [6. , 2.7, 5.1, 1.6],\n",
       "       [5.4, 3. , 4.5, 1.5],\n",
       "       [6. , 3.4, 4.5, 1.6],\n",
       "       [6.7, 3.1, 4.7, 1.5],\n",
       "       [6.3, 2.3, 4.4, 1.3],\n",
       "       [5.6, 3. , 4.1, 1.3],\n",
       "       [5.5, 2.5, 4. , 1.3],\n",
       "       [5.5, 2.6, 4.4, 1.2],\n",
       "       [6.1, 3. , 4.6, 1.4],\n",
       "       [5.8, 2.6, 4. , 1.2],\n",
       "       [5. , 2.3, 3.3, 1. ],\n",
       "       [5.6, 2.7, 4.2, 1.3],\n",
       "       [5.7, 3. , 4.2, 1.2],\n",
       "       [5.7, 2.9, 4.2, 1.3],\n",
       "       [6.2, 2.9, 4.3, 1.3],\n",
       "       [5.1, 2.5, 3. , 1.1],\n",
       "       [5.7, 2.8, 4.1, 1.3],\n",
       "       [6.3, 3.3, 6. , 2.5],\n",
       "       [5.8, 2.7, 5.1, 1.9],\n",
       "       [7.1, 3. , 5.9, 2.1],\n",
       "       [6.3, 2.9, 5.6, 1.8],\n",
       "       [6.5, 3. , 5.8, 2.2],\n",
       "       [7.6, 3. , 6.6, 2.1],\n",
       "       [4.9, 2.5, 4.5, 1.7],\n",
       "       [7.3, 2.9, 6.3, 1.8],\n",
       "       [6.7, 2.5, 5.8, 1.8],\n",
       "       [7.2, 3.6, 6.1, 2.5],\n",
       "       [6.5, 3.2, 5.1, 2. ],\n",
       "       [6.4, 2.7, 5.3, 1.9],\n",
       "       [6.8, 3. , 5.5, 2.1],\n",
       "       [5.7, 2.5, 5. , 2. ],\n",
       "       [5.8, 2.8, 5.1, 2.4],\n",
       "       [6.4, 3.2, 5.3, 2.3],\n",
       "       [6.5, 3. , 5.5, 1.8],\n",
       "       [7.7, 3.8, 6.7, 2.2],\n",
       "       [7.7, 2.6, 6.9, 2.3],\n",
       "       [6. , 2.2, 5. , 1.5],\n",
       "       [6.9, 3.2, 5.7, 2.3],\n",
       "       [5.6, 2.8, 4.9, 2. ],\n",
       "       [7.7, 2.8, 6.7, 2. ],\n",
       "       [6.3, 2.7, 4.9, 1.8],\n",
       "       [6.7, 3.3, 5.7, 2.1],\n",
       "       [7.2, 3.2, 6. , 1.8],\n",
       "       [6.2, 2.8, 4.8, 1.8],\n",
       "       [6.1, 3. , 4.9, 1.8],\n",
       "       [6.4, 2.8, 5.6, 2.1],\n",
       "       [7.2, 3. , 5.8, 1.6],\n",
       "       [7.4, 2.8, 6.1, 1.9],\n",
       "       [7.9, 3.8, 6.4, 2. ],\n",
       "       [6.4, 2.8, 5.6, 2.2],\n",
       "       [6.3, 2.8, 5.1, 1.5],\n",
       "       [6.1, 2.6, 5.6, 1.4],\n",
       "       [7.7, 3. , 6.1, 2.3],\n",
       "       [6.3, 3.4, 5.6, 2.4],\n",
       "       [6.4, 3.1, 5.5, 1.8],\n",
       "       [6. , 3. , 4.8, 1.8],\n",
       "       [6.9, 3.1, 5.4, 2.1],\n",
       "       [6.7, 3.1, 5.6, 2.4],\n",
       "       [6.9, 3.1, 5.1, 2.3],\n",
       "       [5.8, 2.7, 5.1, 1.9],\n",
       "       [6.8, 3.2, 5.9, 2.3],\n",
       "       [6.7, 3.3, 5.7, 2.5],\n",
       "       [6.7, 3. , 5.2, 2.3],\n",
       "       [6.3, 2.5, 5. , 1.9],\n",
       "       [6.5, 3. , 5.2, 2. ],\n",
       "       [6.2, 3.4, 5.4, 2.3],\n",
       "       [5.9, 3. , 5.1, 1.8]])"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "data = load_iris()\n",
    "X = data.data  # Features\n",
    "y = data.target  # Class labels\n",
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([5.1000, 3.5000, 1.4000, 0.2000]) tensor(0)\n"
     ]
    }
   ],
   "source": [
    "# Instantiate the dataset class\n",
    "dataset = TensorDataset(torch.tensor(X).float(), torch.tensor(y))\n",
    "\n",
    "# access an individual sample\n",
    "sample = dataset[0]\n",
    "input_sample, label_sample = sample\n",
    "print(input_sample, label_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 2\n",
    "shuffle = True\n",
    "\n",
    "# create a dataloader\n",
    "dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=shuffle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[4.8000, 3.0000, 1.4000, 0.1000],\n",
      "        [6.2000, 3.4000, 5.4000, 2.3000]]) tensor([0, 2])\n",
      "tensor([[5.2000, 3.4000, 1.4000, 0.2000],\n",
      "        [6.3000, 3.4000, 5.6000, 2.4000]]) tensor([0, 2])\n",
      "tensor([[6.3000, 2.7000, 4.9000, 1.8000],\n",
      "        [4.8000, 3.1000, 1.6000, 0.2000]]) tensor([2, 0])\n",
      "tensor([[5.6000, 2.9000, 3.6000, 1.3000],\n",
      "        [6.6000, 2.9000, 4.6000, 1.3000]]) tensor([1, 1])\n",
      "tensor([[5.7000, 3.8000, 1.7000, 0.3000],\n",
      "        [7.2000, 3.0000, 5.8000, 1.6000]]) tensor([0, 2])\n",
      "tensor([[4.9000, 3.1000, 1.5000, 0.2000],\n",
      "        [6.7000, 3.1000, 4.7000, 1.5000]]) tensor([0, 1])\n",
      "tensor([[5.0000, 3.2000, 1.2000, 0.2000],\n",
      "        [5.7000, 2.9000, 4.2000, 1.3000]]) tensor([0, 1])\n",
      "tensor([[4.6000, 3.4000, 1.4000, 0.3000],\n",
      "        [5.7000, 2.5000, 5.0000, 2.0000]]) tensor([0, 2])\n",
      "tensor([[5.7000, 4.4000, 1.5000, 0.4000],\n",
      "        [4.8000, 3.4000, 1.6000, 0.2000]]) tensor([0, 0])\n",
      "tensor([[5.1000, 3.8000, 1.6000, 0.2000],\n",
      "        [7.6000, 3.0000, 6.6000, 2.1000]]) tensor([0, 2])\n",
      "tensor([[6.3000, 3.3000, 6.0000, 2.5000],\n",
      "        [7.7000, 3.0000, 6.1000, 2.3000]]) tensor([2, 2])\n",
      "tensor([[5.5000, 3.5000, 1.3000, 0.2000],\n",
      "        [6.5000, 3.0000, 5.2000, 2.0000]]) tensor([0, 2])\n",
      "tensor([[5.0000, 3.5000, 1.3000, 0.3000],\n",
      "        [5.8000, 2.7000, 5.1000, 1.9000]]) tensor([0, 2])\n",
      "tensor([[6.7000, 2.5000, 5.8000, 1.8000],\n",
      "        [4.5000, 2.3000, 1.3000, 0.3000]]) tensor([2, 0])\n",
      "tensor([[5.7000, 2.8000, 4.1000, 1.3000],\n",
      "        [6.0000, 2.7000, 5.1000, 1.6000]]) tensor([1, 1])\n",
      "tensor([[4.9000, 3.6000, 1.4000, 0.1000],\n",
      "        [5.5000, 2.4000, 3.8000, 1.1000]]) tensor([0, 1])\n",
      "tensor([[6.3000, 2.3000, 4.4000, 1.3000],\n",
      "        [5.4000, 3.4000, 1.7000, 0.2000]]) tensor([1, 0])\n",
      "tensor([[4.7000, 3.2000, 1.3000, 0.2000],\n",
      "        [4.9000, 2.4000, 3.3000, 1.0000]]) tensor([0, 1])\n",
      "tensor([[6.0000, 2.2000, 5.0000, 1.5000],\n",
      "        [5.0000, 3.0000, 1.6000, 0.2000]]) tensor([2, 0])\n",
      "tensor([[4.4000, 3.0000, 1.3000, 0.2000],\n",
      "        [6.8000, 2.8000, 4.8000, 1.4000]]) tensor([0, 1])\n",
      "tensor([[6.3000, 3.3000, 4.7000, 1.6000],\n",
      "        [6.9000, 3.1000, 5.4000, 2.1000]]) tensor([1, 2])\n",
      "tensor([[5.5000, 4.2000, 1.4000, 0.2000],\n",
      "        [5.8000, 4.0000, 1.2000, 0.2000]]) tensor([0, 0])\n",
      "tensor([[5.9000, 3.2000, 4.8000, 1.8000],\n",
      "        [4.3000, 3.0000, 1.1000, 0.1000]]) tensor([1, 0])\n",
      "tensor([[5.1000, 3.5000, 1.4000, 0.2000],\n",
      "        [7.2000, 3.2000, 6.0000, 1.8000]]) tensor([0, 2])\n",
      "tensor([[6.0000, 3.4000, 4.5000, 1.6000],\n",
      "        [6.8000, 3.2000, 5.9000, 2.3000]]) tensor([1, 2])\n",
      "tensor([[4.9000, 3.0000, 1.4000, 0.2000],\n",
      "        [5.1000, 3.4000, 1.5000, 0.2000]]) tensor([0, 0])\n",
      "tensor([[6.4000, 3.2000, 5.3000, 2.3000],\n",
      "        [5.4000, 3.0000, 4.5000, 1.5000]]) tensor([2, 1])\n",
      "tensor([[6.7000, 3.1000, 4.4000, 1.4000],\n",
      "        [4.4000, 3.2000, 1.3000, 0.2000]]) tensor([1, 0])\n",
      "tensor([[5.0000, 2.3000, 3.3000, 1.0000],\n",
      "        [6.7000, 3.0000, 5.2000, 2.3000]]) tensor([1, 2])\n",
      "tensor([[5.7000, 2.8000, 4.5000, 1.3000],\n",
      "        [6.4000, 2.7000, 5.3000, 1.9000]]) tensor([1, 2])\n",
      "tensor([[6.1000, 2.6000, 5.6000, 1.4000],\n",
      "        [7.7000, 3.8000, 6.7000, 2.2000]]) tensor([2, 2])\n",
      "tensor([[6.1000, 2.8000, 4.0000, 1.3000],\n",
      "        [6.4000, 2.8000, 5.6000, 2.1000]]) tensor([1, 2])\n",
      "tensor([[5.8000, 2.6000, 4.0000, 1.2000],\n",
      "        [4.4000, 2.9000, 1.4000, 0.2000]]) tensor([1, 0])\n",
      "tensor([[5.3000, 3.7000, 1.5000, 0.2000],\n",
      "        [5.5000, 2.5000, 4.0000, 1.3000]]) tensor([0, 1])\n",
      "tensor([[5.6000, 2.5000, 3.9000, 1.1000],\n",
      "        [5.8000, 2.7000, 4.1000, 1.0000]]) tensor([1, 1])\n",
      "tensor([[6.3000, 2.5000, 4.9000, 1.5000],\n",
      "        [5.1000, 3.7000, 1.5000, 0.4000]]) tensor([1, 0])\n",
      "tensor([[5.0000, 2.0000, 3.5000, 1.0000],\n",
      "        [5.0000, 3.6000, 1.4000, 0.2000]]) tensor([1, 0])\n",
      "tensor([[6.7000, 3.1000, 5.6000, 2.4000],\n",
      "        [6.1000, 3.0000, 4.9000, 1.8000]]) tensor([2, 2])\n",
      "tensor([[6.5000, 3.0000, 5.8000, 2.2000],\n",
      "        [6.8000, 3.0000, 5.5000, 2.1000]]) tensor([2, 2])\n",
      "tensor([[5.6000, 2.8000, 4.9000, 2.0000],\n",
      "        [6.4000, 2.8000, 5.6000, 2.2000]]) tensor([2, 2])\n",
      "tensor([[6.2000, 2.9000, 4.3000, 1.3000],\n",
      "        [5.9000, 3.0000, 5.1000, 1.8000]]) tensor([1, 2])\n",
      "tensor([[4.9000, 3.1000, 1.5000, 0.1000],\n",
      "        [6.7000, 3.3000, 5.7000, 2.5000]]) tensor([0, 2])\n",
      "tensor([[5.7000, 3.0000, 4.2000, 1.2000],\n",
      "        [6.5000, 3.0000, 5.5000, 1.8000]]) tensor([1, 2])\n",
      "tensor([[7.0000, 3.2000, 4.7000, 1.4000],\n",
      "        [4.6000, 3.6000, 1.0000, 0.2000]]) tensor([1, 0])\n",
      "tensor([[7.4000, 2.8000, 6.1000, 1.9000],\n",
      "        [6.6000, 3.0000, 4.4000, 1.4000]]) tensor([2, 1])\n",
      "tensor([[6.5000, 2.8000, 4.6000, 1.5000],\n",
      "        [6.9000, 3.1000, 4.9000, 1.5000]]) tensor([1, 1])\n",
      "tensor([[7.2000, 3.6000, 6.1000, 2.5000],\n",
      "        [6.2000, 2.2000, 4.5000, 1.5000]]) tensor([2, 1])\n",
      "tensor([[4.6000, 3.2000, 1.4000, 0.2000],\n",
      "        [5.8000, 2.7000, 5.1000, 1.9000]]) tensor([0, 2])\n",
      "tensor([[6.4000, 2.9000, 4.3000, 1.3000],\n",
      "        [6.2000, 2.8000, 4.8000, 1.8000]]) tensor([1, 2])\n",
      "tensor([[6.4000, 3.2000, 4.5000, 1.5000],\n",
      "        [5.4000, 3.9000, 1.3000, 0.4000]]) tensor([1, 0])\n",
      "tensor([[5.6000, 3.0000, 4.5000, 1.5000],\n",
      "        [5.7000, 2.6000, 3.5000, 1.0000]]) tensor([1, 1])\n",
      "tensor([[4.8000, 3.4000, 1.9000, 0.2000],\n",
      "        [5.0000, 3.4000, 1.6000, 0.4000]]) tensor([0, 0])\n",
      "tensor([[6.3000, 2.8000, 5.1000, 1.5000],\n",
      "        [5.2000, 2.7000, 3.9000, 1.4000]]) tensor([2, 1])\n",
      "tensor([[5.1000, 3.8000, 1.5000, 0.3000],\n",
      "        [5.0000, 3.3000, 1.4000, 0.2000]]) tensor([0, 0])\n",
      "tensor([[4.7000, 3.2000, 1.6000, 0.2000],\n",
      "        [6.1000, 3.0000, 4.6000, 1.4000]]) tensor([0, 1])\n",
      "tensor([[6.5000, 3.2000, 5.1000, 2.0000],\n",
      "        [5.9000, 3.0000, 4.2000, 1.5000]]) tensor([2, 1])\n",
      "tensor([[7.7000, 2.6000, 6.9000, 2.3000],\n",
      "        [6.4000, 3.1000, 5.5000, 1.8000]]) tensor([2, 2])\n",
      "tensor([[5.5000, 2.6000, 4.4000, 1.2000],\n",
      "        [5.4000, 3.4000, 1.5000, 0.4000]]) tensor([1, 0])\n",
      "tensor([[6.0000, 3.0000, 4.8000, 1.8000],\n",
      "        [6.7000, 3.0000, 5.0000, 1.7000]]) tensor([2, 1])\n",
      "tensor([[6.9000, 3.2000, 5.7000, 2.3000],\n",
      "        [6.1000, 2.9000, 4.7000, 1.4000]]) tensor([2, 1])\n",
      "tensor([[5.6000, 2.7000, 4.2000, 1.3000],\n",
      "        [6.3000, 2.5000, 5.0000, 1.9000]]) tensor([1, 2])\n",
      "tensor([[6.9000, 3.1000, 5.1000, 2.3000],\n",
      "        [6.7000, 3.3000, 5.7000, 2.1000]]) tensor([2, 2])\n",
      "tensor([[5.8000, 2.8000, 5.1000, 2.4000],\n",
      "        [5.4000, 3.7000, 1.5000, 0.2000]]) tensor([2, 0])\n",
      "tensor([[7.7000, 2.8000, 6.7000, 2.0000],\n",
      "        [6.0000, 2.2000, 4.0000, 1.0000]]) tensor([2, 1])\n",
      "tensor([[7.1000, 3.0000, 5.9000, 2.1000],\n",
      "        [6.0000, 2.9000, 4.5000, 1.5000]]) tensor([2, 1])\n",
      "tensor([[6.3000, 2.9000, 5.6000, 1.8000],\n",
      "        [5.1000, 3.5000, 1.4000, 0.3000]]) tensor([2, 0])\n",
      "tensor([[5.0000, 3.5000, 1.6000, 0.6000],\n",
      "        [5.8000, 2.7000, 3.9000, 1.2000]]) tensor([0, 1])\n",
      "tensor([[5.6000, 3.0000, 4.1000, 1.3000],\n",
      "        [5.2000, 4.1000, 1.5000, 0.1000]]) tensor([1, 0])\n",
      "tensor([[5.4000, 3.9000, 1.7000, 0.4000],\n",
      "        [4.6000, 3.1000, 1.5000, 0.2000]]) tensor([0, 0])\n",
      "tensor([[5.5000, 2.3000, 4.0000, 1.3000],\n",
      "        [5.2000, 3.5000, 1.5000, 0.2000]]) tensor([1, 0])\n",
      "tensor([[6.1000, 2.8000, 4.7000, 1.2000],\n",
      "        [4.8000, 3.0000, 1.4000, 0.3000]]) tensor([1, 0])\n",
      "tensor([[5.5000, 2.4000, 3.7000, 1.0000],\n",
      "        [5.1000, 2.5000, 3.0000, 1.1000]]) tensor([1, 1])\n",
      "tensor([[5.0000, 3.4000, 1.5000, 0.2000],\n",
      "        [4.9000, 2.5000, 4.5000, 1.7000]]) tensor([0, 2])\n",
      "tensor([[7.9000, 3.8000, 6.4000, 2.0000],\n",
      "        [5.1000, 3.8000, 1.9000, 0.4000]]) tensor([2, 0])\n",
      "tensor([[5.1000, 3.3000, 1.7000, 0.5000],\n",
      "        [7.3000, 2.9000, 6.3000, 1.8000]]) tensor([0, 2])\n"
     ]
    }
   ],
   "source": [
    "# itearate through the dataloader\n",
    "for batch_inputs, batch_labels in dataloader:\n",
    "    print(batch_inputs, batch_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the TensorDataset class\n",
    "- Convert the NumPy arrays provided to PyTorch tensors.\n",
    "- Create a TensorDataset using the torch_features and the torch_target tensors provided (in this order).\n",
    "- Return the last element of the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import TensorDataset\n",
    "\n",
    "np_features = np.array(np.random.rand(12, 8))\n",
    "np_target = np.array(np.random.rand(12, 1))\n",
    "\n",
    "# Convert arrays to PyTorch tensors\n",
    "torch_features = torch.tensor(np_features).float()\n",
    "torch_target = torch.tensor(np_target)\n",
    "\n",
    "# Create a TensorDataset from two tensors\n",
    "dataset = TensorDataset(torch_features,torch_target)\n",
    "\n",
    "# Return the last element of this dataset\n",
    "print(dataset[-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From data loading to running a forward pass\n",
    "- Extract the features (ph, Sulfate, Conductivity, Organic_carbon) and target (Potability) values and load them into the appropriate tensors to represent features and targets.\n",
    "- Use both tensors to create a PyTorch dataset using the dataset class that's quickest to use when tensors don't require any additional preprocessing.\n",
    "- Create a PyTorch DataLoader from the created TensorDataset; this DataLoader should use a batch_size of two and shuffle the dataset.\n",
    "- Implement a small, fully connected neural network using exactly two linear layers and the nn.Sequential() API, where the final output size is 1.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the different columns into two PyTorch tensors\n",
    "features = torch.tensor(dataframe[['ph', 'Sulfate', 'Conductivity', 'Organic_carbon']].to_numpy()).float()\n",
    "target = torch.tensor(dataframe['Potability'].to_numpy()).float()\n",
    "\n",
    "# Create a dataset from the two generated tensors\n",
    "dataset = TensorDataset(features, target)\n",
    "\n",
    "# Create a dataloader using the above dataset\n",
    "dataloader = DataLoader(dataset, shuffle=True, batch_size=2)\n",
    "x, y = next(iter(dataloader))\n",
    "\n",
    "# Create a model using the nn.Sequential API\n",
    "model = nn.Sequential(\n",
    "  nn.Linear(4, 16), \n",
    "  nn.Linear(16, 1)\n",
    ")\n",
    "\n",
    "output = model(features)\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Section 4.2 Evaluating model performance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Writing the evaluation loop\n",
    "- Set the model to evaluation mode.\n",
    "- Sum the current batch loss to the validation_loss variable.\n",
    "- Calculate the mean loss value for the epoch.\n",
    "- Set the model back to training mode."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the model to evaluation mode\n",
    "model.eval()\n",
    "validation_loss = 0.0\n",
    "\n",
    "with torch.no_grad():\n",
    "  \n",
    "  for data in validationloader:\n",
    "    \n",
    "      outputs = model(data[0])\n",
    "      loss = criterion(outputs, data[1])\n",
    "      \n",
    "      # Sum the current loss to the validation_loss variable\n",
    "      validation_loss += loss.item()\n",
    "      \n",
    "# Calculate the mean loss value\n",
    "validation_loss_epoch = validation_loss/len(validationloader)\n",
    "print(validation_loss_epoch)\n",
    "\n",
    "# Set the model back to training mode\n",
    "model.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculating accuracy using torchmetrics\n",
    "- Create an accuracy metric for a \"multiclass\" problem with three classes.\n",
    "- Calculate the accuracy for each batch of the dataloader.\n",
    "- Calculate accuracy for the epoch.\n",
    "- Reset the metric for the next epoch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create accuracy metric using torch metrics\n",
    "metric = torchmetrics.Accuracy(task=\"multiclass\", num_classes=3)\n",
    "for data in dataloader:\n",
    "    features, labels = data\n",
    "    outputs = model(features)\n",
    "    \n",
    "    # Calculate accuracy over the batch\n",
    "    acc = metric(outputs.softmax(dim=-1), labels.argmax(dim=-1))\n",
    "    \n",
    "# Calculate accuracy over the whole epoch\n",
    "acc = metric.compute()\n",
    "\n",
    "# Reset the metric for the next epoch \n",
    "metric.reset()\n",
    "plot_errors(model, dataloader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Section 4.3 Fighting overfitting\n",
    "- causes: did not train model correctly, it memorize the training data\n",
    "- ways: drop out, weight decay, and data augmentation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "XYZ\n",
    "- Create a small neural network with one linear layer, one ReLU function, and one dropout layer, in that order. The model should take input_tensor as input and return an output of size 16."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a small neural network\n",
    "model = nn.Sequential(nn.Linear(3072, 16),\n",
    "                      nn.ReLU(),\n",
    "                      nn.Dropout())\n",
    "model(input_tensor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the same neural network, set the probability of zeroing out elements in the dropout layer to 0.8."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using the same model, set the dropout probability to 0.8\n",
    "model = nn.Sequential(nn.Linear(3072, 16),\n",
    "                      nn.ReLU(),\n",
    "                      nn.Dropout(p=0.8))\n",
    "model(input_tensor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Section 4.4 Improving model performance\n",
    "- overfit the training set\n",
    "    - minimize the training loss, create large enough model\n",
    "- reduce overfitting\n",
    "    - dropout\n",
    "    - data augmentation\n",
    "    - weight decay\n",
    "    - reduce model capacity\n",
    "- fine-tune hyperparameters\n",
    "    - grid search\n",
    "    - random search"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implementing random search\n",
    "- Randomly sample a learning rate factor between 2 and 4 so that the learning rate (lr) is bounded between 0.01 and 0.0001.\n",
    "- Randomly sample a momentum between 0.85 and 0.99."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "values = []\n",
    "for idx in range(10):\n",
    "    # Randomly sample a learning rate factor 2 and 4\n",
    "    factor = np.random.uniform(2, 4)\n",
    "    lr = 10 ** -factor\n",
    "    \n",
    "    # Randomly sample a momentum between 0.85 and 0.99\n",
    "    momentum = np.random.uniform(0.85, 0.99)\n",
    "    \n",
    "    values.append((lr, momentum))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
