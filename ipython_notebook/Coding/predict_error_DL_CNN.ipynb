{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.1.1\n"
     ]
    }
   ],
   "source": [
    "# Data handling and processing\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "import os \n",
    "# Data visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Model evaluation and utilities\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import log_loss\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
    "warnings.filterwarnings('ignore', message=\"is_sparse is deprecated\")\n",
    "from sklearn.model_selection import ParameterSampler\n",
    "from sklearn.metrics import roc_auc_score, brier_score_loss\n",
    "\n",
    "# XGBoost\n",
    "import xgboost as xgb\n",
    "print(xgb.__version__)\n",
    "\n",
    "# Statsmodels\n",
    "import statsmodels.api as sm\n",
    "\n",
    "# PyTorch\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset, random_split\n",
    "import torchmetrics\n",
    "from torch.nn import functional as F\n",
    "from torch.nn import CrossEntropyLoss\n",
    "\n",
    "# Optuna for hyperparameter optimization\n",
    "import optuna\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.metrics import log_loss\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import torch.nn.init as init"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare NN input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_sample(sample):\n",
    "    # Define the paths based on the sample name\n",
    "    qb_path = f'/data2/1000Genome/{sample}/qb/{sample}'\n",
    "    readcount_path = f'/data2/1000Genome/{sample}/beastie/runModel_phased_even100/chr1-22_alignBiasp0.05_s0.7_a0.05_sinCov0_totCov1_W1000K1000/tmp/{sample}'\n",
    "    \n",
    "    # Check if the 'beastie' folder exists (we already filtered for this, so this is optional now)\n",
    "    if not os.path.isdir(f'/data2/1000Genome/{sample}/beastie'):\n",
    "        return\n",
    "\n",
    "    # Read the qb file and extract qb_label\n",
    "    qb_data = pd.read_csv(f\"{qb_path}_qb_highestsite.tsv\", sep='\\t')\n",
    "    qb_data[\"qb_label\"] = qb_data[\"qb_mode\"]\n",
    "    qb_label = qb_data[[\"geneID\", \"qb_label\"]]\n",
    "\n",
    "    # Read the input readcount file\n",
    "    input_readcount = pd.read_csv(f\"{readcount_path}_real_alignBiasafterFilter.phasedByshapeit2.cleaned.tsv\", sep='\\t')\n",
    "    input_readcount = input_readcount[[\"geneID\", \"chrN\", \"pos\", \"refCount\", \"altCount\"]]\n",
    "\n",
    "    # Read the genetic features file\n",
    "    genetic_features = pd.read_csv(f\"{readcount_path}.meta.w_error.tsv\", sep='\\t')\n",
    "    genetic_features = genetic_features[[\"geneID\", \"pos\", \"MAF\", \"min_MAF\", \"diff_MAF\", \"d\", \"r2\", \"log10_distance\"]]\n",
    "\n",
    "    # Perform inner join of the genetic features and the input readcount\n",
    "    input_df = pd.merge(input_readcount, genetic_features, how=\"inner\", on=[\"geneID\", \"pos\"])\n",
    "    input_df[\"individual\"] = sample\n",
    "\n",
    "    # Read the ancestry file and get the first word\n",
    "    ancestry_file = f'/data2/1000Genome/{sample}/ancestry'\n",
    "    with open(ancestry_file, 'r') as file:\n",
    "        ancestry = file.readline().strip()\n",
    "\n",
    "    input_df[\"ancestry\"] = ancestry\n",
    "\n",
    "    # Format the geneID column\n",
    "    input_df[\"geneID\"] = input_df[\"geneID\"].str.split(\".\").str[0]\n",
    "\n",
    "    # Merge input_df with qb_label\n",
    "    input_df = pd.merge(input_df, qb_label, how=\"inner\", on=[\"geneID\"])\n",
    "\n",
    "    # Save the final DataFrame as a TSV file\n",
    "    output_file = f\"{qb_path}_NN_input.tsv\"\n",
    "    input_df.to_csv(output_file, sep='\\t', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Find only the samples with the 'beastie' directory\n",
    "# base_dir = '/data2/1000Genome/'\n",
    "# samples = [\n",
    "#     sample for sample in os.listdir(base_dir)\n",
    "#     if os.path.isdir(os.path.join(base_dir, sample)) and\n",
    "#     os.path.isdir(f'/data2/1000Genome/{sample}/beastie')\n",
    "# ]\n",
    "\n",
    "# # Process each sample with a progress bar\n",
    "# for sample in tqdm(samples, desc=\"Processing samples\"):\n",
    "#     process_sample(sample)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Randomly select some individuals for a training set and testing set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_and_split_samples(ancestry_groups, num_individuals_train=80, num_individuals_test=20, base_dir='/data2/1000Genome/'):\n",
    "    # Ensure ancestry_groups is a list\n",
    "    if isinstance(ancestry_groups, str):\n",
    "        ancestry_groups = [ancestry_groups]\n",
    "    \n",
    "    # Calculate per-group requirements\n",
    "    num_groups = len(ancestry_groups)\n",
    "    num_train_per_group = num_individuals_train // num_groups\n",
    "    num_test_per_group = num_individuals_test // num_groups\n",
    "    \n",
    "    # Validate that the train and test counts are evenly divisible\n",
    "    if num_individuals_train % num_groups != 0 or num_individuals_test % num_groups != 0:\n",
    "        raise ValueError(\"Train and test counts must be evenly divisible by the number of ancestry groups.\")\n",
    "    \n",
    "    # Collect output file paths for each ancestry group\n",
    "    ancestry_files = {group: [] for group in ancestry_groups}\n",
    "    for sample in os.listdir(base_dir):\n",
    "        sample_dir = os.path.join(base_dir, sample)\n",
    "        ancestry_file = os.path.join(sample_dir, 'ancestry')\n",
    "        output_file = os.path.join(sample_dir, 'qb', f\"{sample}_NN_input.tsv\")\n",
    "        \n",
    "        # Check if both ancestry file and output file exist\n",
    "        if os.path.isfile(ancestry_file) and os.path.isfile(output_file):\n",
    "            with open(ancestry_file, 'r') as file:\n",
    "                ancestry = file.readline().strip()\n",
    "            # Collect the file path if it matches any specified ancestry group\n",
    "            if ancestry in ancestry_files:\n",
    "                ancestry_files[ancestry].append(output_file)\n",
    "    \n",
    "    # Initialize empty lists for train and test files\n",
    "    train_files = []\n",
    "    test_files = []\n",
    "    \n",
    "    # Check and select files for each ancestry group\n",
    "    for group, files in ancestry_files.items():\n",
    "        if len(files) < (num_train_per_group + num_test_per_group):\n",
    "            raise ValueError(f\"Not enough individuals in {group} ancestry group. Required: {num_train_per_group + num_test_per_group}, Found: {len(files)}\")\n",
    "        \n",
    "        # Randomly select individuals for train and test\n",
    "        selected_files = random.sample(files, num_train_per_group + num_test_per_group)\n",
    "        train_files.extend(selected_files[:num_train_per_group])\n",
    "        test_files.extend(selected_files[num_train_per_group:num_train_per_group + num_test_per_group])\n",
    "    \n",
    "    # Combine data for the training set\n",
    "    train_df = pd.DataFrame()\n",
    "    for file_path in train_files:\n",
    "        df = pd.read_csv(file_path, sep='\\t')\n",
    "        train_df = pd.concat([train_df, df], ignore_index=True)\n",
    "    \n",
    "    # Combine data for the test set\n",
    "    test_df = pd.DataFrame()\n",
    "    for file_path in test_files:\n",
    "        df = pd.read_csv(file_path, sep='\\t')\n",
    "        test_df = pd.concat([test_df, df], ignore_index=True)\n",
    "    \n",
    "    return train_df, test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example usage\n",
    "train_df, test_df = select_and_split_samples(['GBR', 'CEU', 'TSI', 'YRI'], num_individuals_train=80, num_individuals_test=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>geneID</th>\n",
       "      <th>chrN</th>\n",
       "      <th>pos</th>\n",
       "      <th>refCount</th>\n",
       "      <th>altCount</th>\n",
       "      <th>MAF</th>\n",
       "      <th>min_MAF</th>\n",
       "      <th>diff_MAF</th>\n",
       "      <th>d</th>\n",
       "      <th>r2</th>\n",
       "      <th>log10_distance</th>\n",
       "      <th>individual</th>\n",
       "      <th>ancestry</th>\n",
       "      <th>qb_label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ENSG00000177757</td>\n",
       "      <td>1</td>\n",
       "      <td>754182</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0.1282</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>HG00234</td>\n",
       "      <td>GBR</td>\n",
       "      <td>0.475200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ENSG00000177757</td>\n",
       "      <td>1</td>\n",
       "      <td>754503</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.1610</td>\n",
       "      <td>0.1282</td>\n",
       "      <td>0.0328</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.868</td>\n",
       "      <td>2.506505</td>\n",
       "      <td>HG00234</td>\n",
       "      <td>GBR</td>\n",
       "      <td>0.475200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ENSG00000177757</td>\n",
       "      <td>1</td>\n",
       "      <td>754964</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.1630</td>\n",
       "      <td>0.1610</td>\n",
       "      <td>0.0020</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.000</td>\n",
       "      <td>2.663701</td>\n",
       "      <td>HG00234</td>\n",
       "      <td>GBR</td>\n",
       "      <td>0.475200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ENSG00000225880</td>\n",
       "      <td>1</td>\n",
       "      <td>762601</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>0.1282</td>\n",
       "      <td>0.1282</td>\n",
       "      <td>0.0348</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>HG00234</td>\n",
       "      <td>GBR</td>\n",
       "      <td>0.477750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ENSG00000187634</td>\n",
       "      <td>1</td>\n",
       "      <td>879317</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0398</td>\n",
       "      <td>0.0398</td>\n",
       "      <td>0.0884</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>HG00234</td>\n",
       "      <td>GBR</td>\n",
       "      <td>0.530147</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            geneID  chrN     pos  refCount  altCount     MAF  min_MAF  \\\n",
       "0  ENSG00000177757     1  754182         2         1  0.1282      NaN   \n",
       "1  ENSG00000177757     1  754503         1         0  0.1610   0.1282   \n",
       "2  ENSG00000177757     1  754964         0         1  0.1630   0.1610   \n",
       "3  ENSG00000225880     1  762601         4         3  0.1282   0.1282   \n",
       "4  ENSG00000187634     1  879317         0         1  0.0398   0.0398   \n",
       "\n",
       "   diff_MAF    d     r2  log10_distance individual ancestry  qb_label  \n",
       "0       NaN  NaN    NaN             NaN    HG00234      GBR  0.475200  \n",
       "1    0.0328  1.0  0.868        2.506505    HG00234      GBR  0.475200  \n",
       "2    0.0020  1.0  1.000        2.663701    HG00234      GBR  0.475200  \n",
       "3    0.0348  NaN    NaN             NaN    HG00234      GBR  0.477750  \n",
       "4    0.0884  NaN    NaN             NaN    HG00234      GBR  0.530147  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>geneID</th>\n",
       "      <th>chrN</th>\n",
       "      <th>pos</th>\n",
       "      <th>refCount</th>\n",
       "      <th>altCount</th>\n",
       "      <th>MAF</th>\n",
       "      <th>min_MAF</th>\n",
       "      <th>diff_MAF</th>\n",
       "      <th>d</th>\n",
       "      <th>r2</th>\n",
       "      <th>log10_distance</th>\n",
       "      <th>individual</th>\n",
       "      <th>ancestry</th>\n",
       "      <th>qb_label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ENSG00000188976</td>\n",
       "      <td>1</td>\n",
       "      <td>881627</td>\n",
       "      <td>60</td>\n",
       "      <td>45</td>\n",
       "      <td>0.3668</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>HG00232</td>\n",
       "      <td>GBR</td>\n",
       "      <td>0.438744</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ENSG00000188976</td>\n",
       "      <td>1</td>\n",
       "      <td>882033</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.2495</td>\n",
       "      <td>0.2495</td>\n",
       "      <td>0.1173</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.655</td>\n",
       "      <td>2.608526</td>\n",
       "      <td>HG00232</td>\n",
       "      <td>GBR</td>\n",
       "      <td>0.438744</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ENSG00000187961</td>\n",
       "      <td>1</td>\n",
       "      <td>900505</td>\n",
       "      <td>7</td>\n",
       "      <td>8</td>\n",
       "      <td>0.2903</td>\n",
       "      <td>0.2495</td>\n",
       "      <td>0.0408</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>HG00232</td>\n",
       "      <td>GBR</td>\n",
       "      <td>0.516317</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ENSG00000187583</td>\n",
       "      <td>1</td>\n",
       "      <td>909309</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0.1839</td>\n",
       "      <td>0.1839</td>\n",
       "      <td>0.1064</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>HG00232</td>\n",
       "      <td>GBR</td>\n",
       "      <td>0.591080</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ENSG00000187583</td>\n",
       "      <td>1</td>\n",
       "      <td>910394</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.3211</td>\n",
       "      <td>0.1839</td>\n",
       "      <td>0.1372</td>\n",
       "      <td>0.652</td>\n",
       "      <td>0.171</td>\n",
       "      <td>3.035430</td>\n",
       "      <td>HG00232</td>\n",
       "      <td>GBR</td>\n",
       "      <td>0.591080</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            geneID  chrN     pos  refCount  altCount     MAF  min_MAF  \\\n",
       "0  ENSG00000188976     1  881627        60        45  0.3668      NaN   \n",
       "1  ENSG00000188976     1  882033         1         1  0.2495   0.2495   \n",
       "2  ENSG00000187961     1  900505         7         8  0.2903   0.2495   \n",
       "3  ENSG00000187583     1  909309         0         3  0.1839   0.1839   \n",
       "4  ENSG00000187583     1  910394         1         1  0.3211   0.1839   \n",
       "\n",
       "   diff_MAF      d     r2  log10_distance individual ancestry  qb_label  \n",
       "0       NaN    NaN    NaN             NaN    HG00232      GBR  0.438744  \n",
       "1    0.1173  1.000  0.655        2.608526    HG00232      GBR  0.438744  \n",
       "2    0.0408    NaN    NaN             NaN    HG00232      GBR  0.516317  \n",
       "3    0.1064    NaN    NaN             NaN    HG00232      GBR  0.591080  \n",
       "4    0.1372  0.652  0.171        3.035430    HG00232      GBR  0.591080  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### modify the testset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_data_for_cnn_individual(data, feature_columns, label_col='qb_label'):\n",
    "    # Step 1: Determine the maximum number of hets per gene across individuals\n",
    "    max_hets_per_gene = data.groupby(['individual', 'geneID']).size().max()\n",
    "    \n",
    "    # Step 2: Calculate relative position for each gene by individual\n",
    "    # Sort by geneID and position for proper ordering\n",
    "    data = data.sort_values(by=['individual', 'geneID', 'pos'])\n",
    "    #data['relative_pos'] = data.groupby(['individual', 'geneID'])['pos'].diff().fillna(0)\n",
    "    data['log10_distance'] = data['log10_distance'].fillna(0)\n",
    "    # fill in d' values with 0.5 if nan\n",
    "    data['d'] = data['d'].fillna(0.5)\n",
    "    # fill in r2 values with 0.2-0.3 uniform distribution if nan\n",
    "    data['r2'] = data['r2'].fillna(np.random.uniform(0.2, 0.3))\n",
    "    # fill in NAn with 0 for min_MAF\n",
    "    data['min_MAF'] = data['min_MAF'].fillna(0)\n",
    "    # fill in NAn with 0 for diff_MAF\n",
    "    data['diff_MAF'] = data['diff_MAF'].fillna(0)\n",
    "    \n",
    "    # Step 3: Filter relevant columns - dropping geneID, chrN, and pos\n",
    "    X_data = []\n",
    "    y_data = []\n",
    "    \n",
    "    # Group by individual and geneID to form CNN inputs\n",
    "    grouped = data.groupby(['individual', 'geneID'])\n",
    "    \n",
    "    for (individual, geneID), group in grouped:\n",
    "        # Extract the feature columns for the current individual and gene\n",
    "        X = group[feature_columns].to_numpy()\n",
    "        \n",
    "        # Pad the feature array to have the same number of rows\n",
    "        padded_X = np.zeros((max_hets_per_gene, len(feature_columns)))\n",
    "        padded_X[:X.shape[0], :] = X  # Fill in the available data\n",
    "        \n",
    "        # Get the label (assumes it's the same across all rows in the group)\n",
    "        y = group[label_col].iloc[0]\n",
    "        \n",
    "        # Append to the lists\n",
    "        X_data.append(padded_X)\n",
    "        y_data.append(y)\n",
    "    \n",
    "    # Convert the lists to numpy arrays suitable for CNN input\n",
    "    X_data = np.array(X_data)\n",
    "    y_data = np.array(y_data)\n",
    "    \n",
    "    return X_data, y_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X shape: (589222, 62, 8)\n",
      "y shape: (589222,)\n"
     ]
    }
   ],
   "source": [
    "data_train = train_df\n",
    "feature_columns = ['refCount', 'altCount', 'MAF', 'min_MAF', 'diff_MAF', 'log10_distance', 'd', 'r2']\n",
    "X_train, y_train = prepare_data_for_cnn_individual(data_train, feature_columns)\n",
    "print(\"X shape:\", X_train.shape)\n",
    "print(\"y shape:\", y_train.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X test shape: (149173, 62, 8)\n",
      "y test shape: (149173,)\n"
     ]
    }
   ],
   "source": [
    "data_test = test_df\n",
    "X_test, y_test = prepare_data_for_cnn_individual(data_test, feature_columns)\n",
    "print(\"X test shape:\", X_test.shape)\n",
    "print(\"y test shape:\", y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>geneID</th>\n",
       "      <th>chrN</th>\n",
       "      <th>pos</th>\n",
       "      <th>refCount</th>\n",
       "      <th>altCount</th>\n",
       "      <th>MAF</th>\n",
       "      <th>min_MAF</th>\n",
       "      <th>diff_MAF</th>\n",
       "      <th>d</th>\n",
       "      <th>r2</th>\n",
       "      <th>log10_distance</th>\n",
       "      <th>individual</th>\n",
       "      <th>ancestry</th>\n",
       "      <th>qb_label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ENSG00000188976</td>\n",
       "      <td>1</td>\n",
       "      <td>881627</td>\n",
       "      <td>93</td>\n",
       "      <td>67</td>\n",
       "      <td>0.3668</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>HG00261</td>\n",
       "      <td>GBR</td>\n",
       "      <td>0.431798</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ENSG00000188976</td>\n",
       "      <td>1</td>\n",
       "      <td>889876</td>\n",
       "      <td>7</td>\n",
       "      <td>3</td>\n",
       "      <td>0.0159</td>\n",
       "      <td>0.0159</td>\n",
       "      <td>0.3509</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.027</td>\n",
       "      <td>3.916401</td>\n",
       "      <td>HG00261</td>\n",
       "      <td>GBR</td>\n",
       "      <td>0.431798</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ENSG00000187961</td>\n",
       "      <td>1</td>\n",
       "      <td>896064</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0378</td>\n",
       "      <td>0.0159</td>\n",
       "      <td>0.0219</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>HG00261</td>\n",
       "      <td>GBR</td>\n",
       "      <td>0.313768</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ENSG00000187961</td>\n",
       "      <td>1</td>\n",
       "      <td>897730</td>\n",
       "      <td>11</td>\n",
       "      <td>2</td>\n",
       "      <td>0.0388</td>\n",
       "      <td>0.0378</td>\n",
       "      <td>0.0010</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.000</td>\n",
       "      <td>3.221675</td>\n",
       "      <td>HG00261</td>\n",
       "      <td>GBR</td>\n",
       "      <td>0.313768</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ENSG00000187961</td>\n",
       "      <td>1</td>\n",
       "      <td>900741</td>\n",
       "      <td>9</td>\n",
       "      <td>3</td>\n",
       "      <td>0.0010</td>\n",
       "      <td>0.0010</td>\n",
       "      <td>0.0378</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>3.478711</td>\n",
       "      <td>HG00261</td>\n",
       "      <td>GBR</td>\n",
       "      <td>0.313768</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            geneID  chrN     pos  refCount  altCount     MAF  min_MAF  \\\n",
       "0  ENSG00000188976     1  881627        93        67  0.3668      NaN   \n",
       "1  ENSG00000188976     1  889876         7         3  0.0159   0.0159   \n",
       "2  ENSG00000187961     1  896064         0         1  0.0378   0.0159   \n",
       "3  ENSG00000187961     1  897730        11         2  0.0388   0.0378   \n",
       "4  ENSG00000187961     1  900741         9         3  0.0010   0.0010   \n",
       "\n",
       "   diff_MAF    d     r2  log10_distance individual ancestry  qb_label  \n",
       "0       NaN  NaN    NaN             NaN    HG00261      GBR  0.431798  \n",
       "1    0.3509  1.0  0.027        3.916401    HG00261      GBR  0.431798  \n",
       "2    0.0219  NaN    NaN             NaN    HG00261      GBR  0.313768  \n",
       "3    0.0010  1.0  1.000        3.221675    HG00261      GBR  0.313768  \n",
       "4    0.0378  1.0  0.000        3.478711    HG00261      GBR  0.313768  "
      ]
     },
     "execution_count": 162,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GeneCNN(nn.Module):\n",
    "    def __init__(self, input_features):\n",
    "        super(GeneCNN, self).__init__()\n",
    "        \n",
    "        # Convolutional and pooling layers\n",
    "        ## the number of filters in the 1st conv layer is 32 (common choice 16-64)\n",
    "        self.conv1 = nn.Conv1d(in_channels = input_features, out_channels=32, kernel_size=2)\n",
    "        self.pool1 = nn.MaxPool1d(kernel_size=2)\n",
    "        \n",
    "        ## the number of filters in the 2nd conv layer is 32 (common choice 16-64)\n",
    "        self.conv2 = nn.Conv1d(in_channels=32, out_channels=64, kernel_size=2)\n",
    "        self.pool2 = nn.MaxPool1d(kernel_size=2)\n",
    "        \n",
    "        ## the number of filters in the 3rd conv layer is 64 (common choice 16-64)\n",
    "        self.conv3 = nn.Conv1d(in_channels=64, out_channels=128, kernel_size=2)\n",
    "        \n",
    "        # Adaptive pooling layer to reduce size to 1\n",
    "        self.global_pool = nn.AdaptiveAvgPool1d(1)\n",
    "        \n",
    "        # Fully connected layers\n",
    "        self.fc1 = nn.Linear(128, 64)  # Adjusted input size to match the output channels from global_pool\n",
    "        self.fc2 = nn.Linear(64, 8)\n",
    "        self.fc3 = nn.Linear(8, 1)  # Single output neuron for regression\n",
    "        \n",
    "        # Dropout layer for regularization\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "\n",
    "        # # Add two batch normalization layers\n",
    "        # self.bn1 = nn.BatchNorm1d(128)\n",
    "        # self.bn2 = nn.BatchNorm1d(8)\n",
    "\n",
    "        # # Apply He initialization\n",
    "        # init.kaiming_uniform_(self.fc1.weight)\n",
    "        # init.kaiming_uniform_(self.fc2.weight)\n",
    "        # init.kaiming_uniform_(self.fc3.weight, nonlinearity=\"sigmoid\")\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Transpose to match Conv1d input format: (batch_size, features, length)\n",
    "        x = x.transpose(1, 2)\n",
    "        \n",
    "        # Convolution and pooling layers\n",
    "        x = self.pool1(F.relu(self.conv1(x)))  # Conv1 and pooling\n",
    "        #print(f\"Shape after conv1 and pool1: {x.shape}\")\n",
    "        \n",
    "        x = self.pool2(F.relu(self.conv2(x)))  # Conv2 and pooling\n",
    "        #print(f\"Shape after conv2 and pool2: {x.shape}\")\n",
    "        \n",
    "        x = F.relu(self.conv3(x))              # Conv3\n",
    "        #print(f\"Shape after conv3: {x.shape}\")\n",
    "        \n",
    "        x = self.global_pool(x)                # Adaptive pooling to size 1\n",
    "        #print(f\"Shape after global_pool: {x.shape}\")\n",
    "        \n",
    "        # Flatten and pass through fully connected layers\n",
    "        x = x.view(x.size(0), -1)              # Flatten\n",
    "        #print(f\"Shape after flattening: {x.shape}\")\n",
    "        \n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.dropout(x)\n",
    "        x = F.relu(self.fc2(x))\n",
    "        \n",
    "        # Output layer for regression (no activation or add sigmoid if bounded)\n",
    "        x = self.fc3(x)\n",
    "        \n",
    "        return x\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8"
      ]
     },
     "execution_count": 164,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GeneCNN(\n",
      "  (conv1): Conv1d(8, 32, kernel_size=(2,), stride=(1,))\n",
      "  (pool1): MaxPool1d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (conv2): Conv1d(32, 64, kernel_size=(2,), stride=(1,))\n",
      "  (pool2): MaxPool1d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (conv3): Conv1d(64, 128, kernel_size=(2,), stride=(1,))\n",
      "  (global_pool): AdaptiveAvgPool1d(output_size=1)\n",
      "  (fc1): Linear(in_features=128, out_features=64, bias=True)\n",
      "  (fc2): Linear(in_features=64, out_features=8, bias=True)\n",
      "  (fc3): Linear(in_features=8, out_features=1, bias=True)\n",
      "  (dropout): Dropout(p=0.5, inplace=False)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# Instantiate the model, loss function, and optimizer\n",
    "max_hets_per_gene = X_train.shape[2]  # Use the maximum number of hets per gene\n",
    "model = GeneCNN(input_features = max_hets_per_gene) # 6 features\n",
    "\n",
    "# Model summary\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming X_train and y_train are initially your full training data\n",
    "# Set aside a portion for validation\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.25, random_state=42, shuffle=True)\n",
    "\n",
    "# Convert them to PyTorch tensors\n",
    "X_train = torch.tensor(X_train, dtype=torch.float32)\n",
    "y_train = torch.tensor(y_train, dtype=torch.float32).unsqueeze(1)\n",
    "X_val = torch.tensor(X_val, dtype=torch.float32)\n",
    "y_val = torch.tensor(y_val, dtype=torch.float32).unsqueeze(1)\n",
    "\n",
    "X_test = torch.tensor(X_test, dtype=torch.float32)\n",
    "y_test = torch.tensor(y_test, dtype=torch.float32).unsqueeze(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a dataset and a DataLoader with a smaller batch size\n",
    "train_dataset = TensorDataset(X_train, y_train)\n",
    "val_dataset = TensorDataset(X_val, y_val)\n",
    "train_loader = DataLoader(train_dataset, batch_size=128, shuffle=True)  # Adjust batch size as needed\n",
    "val_loader = DataLoader(val_dataset, batch_size=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NaNs in X_train: tensor(False)\n",
      "NaNs in y_train: tensor(False)\n",
      "Infs in X_train: tensor(False)\n",
      "Infs in y_train: tensor(False)\n",
      "NaNs in X_val: tensor(False)\n",
      "NaNs in y_val: tensor(False)\n",
      "Infs in X_val: tensor(False)\n",
      "Infs in y_val: tensor(False)\n"
     ]
    }
   ],
   "source": [
    "# Check for NaNs or Infs in input and target data\n",
    "print(\"NaNs in X_train:\", torch.isnan(X_train).any())\n",
    "print(\"NaNs in y_train:\", torch.isnan(y_train).any())\n",
    "print(\"Infs in X_train:\", torch.isinf(X_train).any())\n",
    "print(\"Infs in y_train:\", torch.isinf(y_train).any())\n",
    "\n",
    "print(\"NaNs in X_val:\", torch.isnan(X_val).any())\n",
    "print(\"NaNs in y_val:\", torch.isnan(y_val).any())\n",
    "print(\"Infs in X_val:\", torch.isinf(X_val).any())\n",
    "print(\"Infs in y_val:\", torch.isinf(y_val).any())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/scarlett/.local/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:60: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/100], Train RMSE: 0.0872, Validation RMSE: 0.0494\n",
      "Epoch [2/100], Train RMSE: 0.0517, Validation RMSE: 0.0457\n",
      "Epoch [3/100], Train RMSE: 0.0494, Validation RMSE: 0.0442\n",
      "Epoch [4/100], Train RMSE: 0.0461, Validation RMSE: 0.0444\n",
      "Epoch [5/100], Train RMSE: 0.0453, Validation RMSE: 0.0425\n",
      "Epoch [6/100], Train RMSE: 0.0447, Validation RMSE: 0.0433\n",
      "Epoch [7/100], Train RMSE: 0.0438, Validation RMSE: 0.0417\n",
      "Epoch [8/100], Train RMSE: 0.0435, Validation RMSE: 0.0414\n",
      "Epoch [9/100], Train RMSE: 0.0431, Validation RMSE: 0.0416\n",
      "Epoch [10/100], Train RMSE: 0.0427, Validation RMSE: 0.0415\n",
      "Epoch [11/100], Train RMSE: 0.0423, Validation RMSE: 0.0426\n",
      "Epoch [12/100], Train RMSE: 0.0421, Validation RMSE: 0.0418\n",
      "Epoch [13/100], Train RMSE: 0.0413, Validation RMSE: 0.0416\n",
      "Epoch [14/100], Train RMSE: 0.0413, Validation RMSE: 0.0416\n",
      "Epoch [15/100], Train RMSE: 0.0412, Validation RMSE: 0.0410\n",
      "Epoch [16/100], Train RMSE: 0.0410, Validation RMSE: 0.0418\n",
      "Epoch [17/100], Train RMSE: 0.0410, Validation RMSE: 0.0408\n",
      "Epoch [18/100], Train RMSE: 0.0410, Validation RMSE: 0.0410\n",
      "Epoch [19/100], Train RMSE: 0.0409, Validation RMSE: 0.0412\n",
      "Epoch [20/100], Train RMSE: 0.0408, Validation RMSE: 0.0410\n",
      "Epoch [21/100], Train RMSE: 0.0407, Validation RMSE: 0.0410\n",
      "Epoch [22/100], Train RMSE: 0.0404, Validation RMSE: 0.0406\n",
      "Epoch [23/100], Train RMSE: 0.0403, Validation RMSE: 0.0409\n",
      "Epoch [24/100], Train RMSE: 0.0402, Validation RMSE: 0.0405\n",
      "Epoch [25/100], Train RMSE: 0.0401, Validation RMSE: 0.0412\n",
      "Epoch [26/100], Train RMSE: 0.0401, Validation RMSE: 0.0408\n",
      "Epoch [27/100], Train RMSE: 0.0401, Validation RMSE: 0.0408\n",
      "Epoch [28/100], Train RMSE: 0.0400, Validation RMSE: 0.0414\n",
      "Epoch [29/100], Train RMSE: 0.0399, Validation RMSE: 0.0406\n",
      "Epoch [30/100], Train RMSE: 0.0398, Validation RMSE: 0.0407\n",
      "Epoch [31/100], Train RMSE: 0.0398, Validation RMSE: 0.0408\n",
      "Epoch [32/100], Train RMSE: 0.0397, Validation RMSE: 0.0407\n",
      "Epoch [33/100], Train RMSE: 0.0396, Validation RMSE: 0.0405\n",
      "Epoch [34/100], Train RMSE: 0.0397, Validation RMSE: 0.0408\n",
      "Epoch [35/100], Train RMSE: 0.0396, Validation RMSE: 0.0406\n",
      "Epoch [36/100], Train RMSE: 0.0396, Validation RMSE: 0.0407\n",
      "Epoch [37/100], Train RMSE: 0.0395, Validation RMSE: 0.0406\n",
      "Epoch [38/100], Train RMSE: 0.0395, Validation RMSE: 0.0406\n",
      "Epoch [39/100], Train RMSE: 0.0396, Validation RMSE: 0.0406\n",
      "Epoch [40/100], Train RMSE: 0.0395, Validation RMSE: 0.0408\n",
      "Epoch [41/100], Train RMSE: 0.0395, Validation RMSE: 0.0407\n",
      "Epoch [42/100], Train RMSE: 0.0394, Validation RMSE: 0.0407\n",
      "Epoch [43/100], Train RMSE: 0.0394, Validation RMSE: 0.0406\n",
      "Epoch [44/100], Train RMSE: 0.0394, Validation RMSE: 0.0406\n",
      "Epoch [45/100], Train RMSE: 0.0394, Validation RMSE: 0.0405\n",
      "Epoch [46/100], Train RMSE: 0.0394, Validation RMSE: 0.0405\n",
      "Epoch [47/100], Train RMSE: 0.0394, Validation RMSE: 0.0406\n",
      "Epoch [48/100], Train RMSE: 0.0394, Validation RMSE: 0.0407\n",
      "Epoch [49/100], Train RMSE: 0.0394, Validation RMSE: 0.0406\n",
      "Epoch [50/100], Train RMSE: 0.0394, Validation RMSE: 0.0406\n",
      "Epoch [51/100], Train RMSE: 0.0394, Validation RMSE: 0.0406\n",
      "Epoch [52/100], Train RMSE: 0.0394, Validation RMSE: 0.0406\n",
      "Epoch [53/100], Train RMSE: 0.0394, Validation RMSE: 0.0406\n",
      "Epoch [54/100], Train RMSE: 0.0394, Validation RMSE: 0.0406\n",
      "Epoch [55/100], Train RMSE: 0.0394, Validation RMSE: 0.0406\n",
      "Epoch [56/100], Train RMSE: 0.0394, Validation RMSE: 0.0406\n",
      "Epoch [57/100], Train RMSE: 0.0394, Validation RMSE: 0.0406\n",
      "Epoch [58/100], Train RMSE: 0.0394, Validation RMSE: 0.0406\n",
      "Epoch [59/100], Train RMSE: 0.0394, Validation RMSE: 0.0406\n",
      "Epoch [60/100], Train RMSE: 0.0394, Validation RMSE: 0.0406\n",
      "Epoch [61/100], Train RMSE: 0.0394, Validation RMSE: 0.0406\n",
      "Epoch [62/100], Train RMSE: 0.0393, Validation RMSE: 0.0406\n",
      "Epoch [63/100], Train RMSE: 0.0394, Validation RMSE: 0.0406\n",
      "Epoch [64/100], Train RMSE: 0.0393, Validation RMSE: 0.0406\n",
      "Epoch [65/100], Train RMSE: 0.0394, Validation RMSE: 0.0406\n",
      "Epoch [66/100], Train RMSE: 0.0394, Validation RMSE: 0.0406\n",
      "Epoch [67/100], Train RMSE: 0.0393, Validation RMSE: 0.0406\n",
      "Epoch [68/100], Train RMSE: 0.0394, Validation RMSE: 0.0406\n",
      "Epoch [69/100], Train RMSE: 0.0394, Validation RMSE: 0.0406\n",
      "Epoch [70/100], Train RMSE: 0.0393, Validation RMSE: 0.0406\n",
      "Epoch [71/100], Train RMSE: 0.0393, Validation RMSE: 0.0406\n",
      "Epoch [72/100], Train RMSE: 0.0394, Validation RMSE: 0.0406\n",
      "Epoch [73/100], Train RMSE: 0.0393, Validation RMSE: 0.0406\n",
      "Epoch [74/100], Train RMSE: 0.0394, Validation RMSE: 0.0406\n",
      "Epoch [75/100], Train RMSE: 0.0394, Validation RMSE: 0.0406\n",
      "Epoch [76/100], Train RMSE: 0.0394, Validation RMSE: 0.0406\n",
      "Epoch [77/100], Train RMSE: 0.0393, Validation RMSE: 0.0406\n",
      "Epoch [78/100], Train RMSE: 0.0394, Validation RMSE: 0.0406\n",
      "Epoch [79/100], Train RMSE: 0.0394, Validation RMSE: 0.0406\n",
      "Epoch [80/100], Train RMSE: 0.0393, Validation RMSE: 0.0406\n",
      "Epoch [81/100], Train RMSE: 0.0394, Validation RMSE: 0.0406\n",
      "Epoch [82/100], Train RMSE: 0.0394, Validation RMSE: 0.0406\n",
      "Epoch [83/100], Train RMSE: 0.0393, Validation RMSE: 0.0406\n",
      "Epoch [84/100], Train RMSE: 0.0393, Validation RMSE: 0.0406\n",
      "Epoch [85/100], Train RMSE: 0.0393, Validation RMSE: 0.0406\n",
      "Epoch [86/100], Train RMSE: 0.0393, Validation RMSE: 0.0406\n",
      "Epoch [87/100], Train RMSE: 0.0394, Validation RMSE: 0.0406\n",
      "Epoch [88/100], Train RMSE: 0.0394, Validation RMSE: 0.0406\n",
      "Epoch [89/100], Train RMSE: 0.0394, Validation RMSE: 0.0406\n",
      "Epoch [90/100], Train RMSE: 0.0394, Validation RMSE: 0.0406\n",
      "Epoch [91/100], Train RMSE: 0.0394, Validation RMSE: 0.0406\n",
      "Epoch [92/100], Train RMSE: 0.0394, Validation RMSE: 0.0406\n",
      "Epoch [93/100], Train RMSE: 0.0394, Validation RMSE: 0.0406\n",
      "Epoch [94/100], Train RMSE: 0.0394, Validation RMSE: 0.0406\n",
      "Epoch [95/100], Train RMSE: 0.0394, Validation RMSE: 0.0406\n",
      "Epoch [96/100], Train RMSE: 0.0394, Validation RMSE: 0.0406\n",
      "Epoch [97/100], Train RMSE: 0.0394, Validation RMSE: 0.0406\n",
      "Epoch [98/100], Train RMSE: 0.0394, Validation RMSE: 0.0406\n",
      "Epoch [99/100], Train RMSE: 0.0393, Validation RMSE: 0.0406\n",
      "Epoch [100/100], Train RMSE: 0.0394, Validation RMSE: 0.0406\n"
     ]
    }
   ],
   "source": [
    "# Check if GPU is available\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Initialize the model\n",
    "model = GeneCNN(input_features=max_hets_per_gene).to(device)  # Change input_features to the correct feature count\n",
    "\n",
    "# Define loss function, optimizer, and optional scheduler\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.0005)\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', patience=3, factor=0.5, verbose=True)\n",
    "\n",
    "# Lists to store the training and validation losses\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "\n",
    "# Training loop\n",
    "epochs = 100\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    train_loss_epoch = 0.0\n",
    "    \n",
    "    for X_batch, y_batch in train_loader:\n",
    "        X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Forward pass\n",
    "        outputs = model(X_batch)\n",
    "        train_mse_loss = criterion(outputs, y_batch)  # MSE Loss\n",
    "        train_rmse_loss = torch.sqrt(train_mse_loss)  # RMSE\n",
    "        \n",
    "        # Backward pass\n",
    "        train_mse_loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "        optimizer.step()\n",
    "        \n",
    "        train_loss_epoch += train_mse_loss.item()  # Accumulate MSE loss\n",
    "    \n",
    "    # Validation\n",
    "    model.eval()\n",
    "    val_loss_epoch = 0.0\n",
    "    with torch.no_grad():\n",
    "        for X_val_batch, y_val_batch in val_loader:\n",
    "            X_val_batch, y_val_batch = X_val_batch.to(device), y_val_batch.to(device)\n",
    "            val_outputs = model(X_val_batch)\n",
    "            val_mse_loss = criterion(val_outputs, y_val_batch)\n",
    "            val_loss_epoch += val_mse_loss.item()  # Accumulate MSE loss\n",
    "\n",
    "    # Calculate average RMSE for the epoch\n",
    "    avg_train_rmse = torch.sqrt(torch.tensor(train_loss_epoch / len(train_loader))).item()\n",
    "    avg_val_rmse = torch.sqrt(torch.tensor(val_loss_epoch / len(val_loader))).item()\n",
    "\n",
    "    # Store the RMSE losses for each epoch\n",
    "    train_losses.append(avg_train_rmse)\n",
    "    val_losses.append(avg_val_rmse)\n",
    "    \n",
    "    # Step the scheduler based on validation loss\n",
    "    scheduler.step(avg_val_rmse)\n",
    "    \n",
    "    print(f\"Epoch [{epoch+1}/{epochs}], Train RMSE: {avg_train_rmse:.4f}, Validation RMSE: {avg_val_rmse:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### RELU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x500 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(range(1, epochs + 1), train_losses, label='Training RMSE')\n",
    "plt.plot(range(1, epochs + 1), val_losses, label='Validation RMSE')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('RMSE')\n",
    "plt.title('Training and Validation RMSE')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final Test RMSE: 0.0416\n"
     ]
    }
   ],
   "source": [
    "# Create a DataLoader for the test set with a smaller batch size\n",
    "test_dataset = TensorDataset(X_test, y_test)\n",
    "test_loader = DataLoader(test_dataset, batch_size=64)  # Adjust batch size as needed\n",
    "\n",
    "# Final evaluation on test set\n",
    "model.eval()\n",
    "test_loss_total = 0.0\n",
    "with torch.no_grad():\n",
    "    for X_batch, y_batch in test_loader:\n",
    "        X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
    "        test_outputs = model(X_batch)\n",
    "        test_loss = criterion(test_outputs, y_batch)  # MSE Loss\n",
    "        test_loss_total += test_loss.item() * X_batch.size(0)  # Sum up the batch loss (weighted by batch size)\n",
    "\n",
    "# Calculate the average loss and take the square root for RMSE\n",
    "average_test_mse = test_loss_total / len(test_loader.dataset)\n",
    "average_test_rmse = torch.sqrt(torch.tensor(average_test_mse)).item()\n",
    "print(f\"Final Test RMSE: {average_test_rmse:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simulator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [],
   "source": [
    "def simulate_reads_single_gene(p=0.5, read_coverage=100, phasing_error=0.04, num_hets=10, max_hets=64, data=None):\n",
    "    np.random.seed(42)  # For reproducibility\n",
    "\n",
    "    # Initialize lists to store simulated data\n",
    "    ref_counts = []\n",
    "    alt_counts = []\n",
    "    switches = []\n",
    "\n",
    "    # Sample MAF and log10_distance for each het site\n",
    "    if data is None:\n",
    "        raise ValueError(\"MAF and log10_distance data must be provided.\")\n",
    "    \n",
    "\n",
    "    # Sample MAF and log10_distance for each het site in the gene\n",
    "    samples = data.sample(n=num_hets, replace=True)\n",
    "    # fill in missing data\n",
    "    samples['log10_distance'] = samples['log10_distance'].fillna(0)\n",
    "    # fill in d' values with 0.5 if nan\n",
    "    samples['d'] = samples['d'].fillna(0.5)\n",
    "    # fill in r2 values with 0.2-0.3 uniform distribution if nan\n",
    "    samples['r2'] = samples['r2'].fillna(np.random.uniform(0.2, 0.3))\n",
    "    # fill in NAn with 0 for min_MAF\n",
    "    samples['min_MAF'] = samples['min_MAF'].fillna(0)\n",
    "    # fill in NAn with 0 for diff_MAF\n",
    "    samples['diff_MAF'] = samples['diff_MAF'].fillna(0)\n",
    "\n",
    "    maf_values = samples['MAF'].values\n",
    "    min_maf_values = samples['min_MAF'].values\n",
    "    diff_maf_values = samples['diff_MAF'].values\n",
    "    log10_distance_values = samples['log10_distance'].values\n",
    "    d_values = samples['d'].values\n",
    "    r2_values = samples['r2'].values\n",
    "\n",
    "    # Convert log10_distance to distance and handle NaN by replacing with mean distance\n",
    "    #distance_values = np.where(~np.isnan(log10_distance_values), 10**log10_distance_values, np.nan)\n",
    "    #mean_distance = np.nanmean(distance_values)\n",
    "    #distance_values = np.where(np.isnan(distance_values), mean_distance, distance_values)\n",
    "\n",
    "    current_phase = False  # Start without phase flipping\n",
    "\n",
    "    # Loop over each heterozygous site\n",
    "    for het in range(num_hets):\n",
    "        # Simulate binomial read counts based on the binomial probability\n",
    "        ref_count = np.random.binomial(n=read_coverage, p=p)  # Total ref count for this het\n",
    "        alt_count = read_coverage - ref_count  # Total alt count for this het\n",
    "\n",
    "        # Generate a random phasing error to determine if a phase switch occurs\n",
    "        phase_switch = np.random.rand() < phasing_error\n",
    "        \n",
    "        # If a phasing error occurred, toggle the current phase\n",
    "        if phase_switch:\n",
    "            current_phase = not current_phase\n",
    "\n",
    "        # If current phase is switched, swap the ref and alt counts\n",
    "        if current_phase:\n",
    "            ref_count, alt_count = alt_count, ref_count\n",
    "            switches.append(1)  # Indicate a switch\n",
    "        else:\n",
    "            switches.append(0)  # No switch\n",
    "\n",
    "        # Store accumulated values for this het site\n",
    "        ref_counts.append(ref_count)\n",
    "        alt_counts.append(alt_count)\n",
    "        # mafs.append(maf_values[het])\n",
    "\n",
    "    # feature_columns = ['refCount', 'altCount', 'MAF', 'min_MAF', 'diff_MAF', 'log10_distance', 'd', 'r2']\n",
    "    # Create DataFrame for model input with the four features\n",
    "    data = {\n",
    "        'refCount': ref_counts,\n",
    "        'altCount': alt_counts,\n",
    "        'MAF': maf_values,\n",
    "        'min_MAF': min_maf_values,\n",
    "        'diff_MAF': diff_maf_values,\n",
    "        'log10_distance': log10_distance_values,\n",
    "        'd': d_values,\n",
    "        'r2': r2_values,\n",
    "        'switch': switches\n",
    "    }\n",
    "    df_input = pd.DataFrame(data)\n",
    "    \n",
    "    # Pad the DataFrame to have max_hets rows\n",
    "    padded_data = np.zeros((max_hets, len(df_input.columns) - 1))  # Exclude the 'switch' column from padding\n",
    "    padded_data[:df_input.shape[0], :] = df_input.drop(columns=['switch']).values  # Fill in available data and zero-pad remaining rows\n",
    "    \n",
    "    # Convert padded data to numpy arrays similar to `prepare_data_for_cnn_individual` output\n",
    "    X_data = np.array([padded_data])  # Add batch dimension to match the expected CNN input format\n",
    "    y_data = np.array([p])  # Here, the `p` is used as a placeholder for the label\n",
    "    \n",
    "    return X_data, y_data, df_input  # Return DataFrame for visualization as well\n",
    "\n",
    "\n",
    "\n",
    "def evaluate_simulation(X_sim, y_sim, model, device='cpu'):\n",
    "    \"\"\"\n",
    "    Runs the model on simulated data, calculates the predicted binomial p values, \n",
    "    and returns the RMSE compared to the true p values.\n",
    "\n",
    "    Parameters:\n",
    "    - X_sim: np.array, simulated input features\n",
    "    - y_sim: np.array, true binomial p values\n",
    "    - model: PyTorch model, the trained model to use for predictions\n",
    "    - device: str, device to run the model on ('cpu' or 'cuda')\n",
    "\n",
    "    Returns:\n",
    "    - predicted_p_values: np.array, the predicted binomial p values\n",
    "    - rmse: float, root mean squared error between the predicted and true p values\n",
    "    \"\"\"\n",
    "\n",
    "    # Convert X_sim and y_sim to PyTorch tensors\n",
    "    X_sim_tensor = torch.tensor(X_sim, dtype=torch.float32).to(device)\n",
    "    y_sim_tensor = torch.tensor(y_sim, dtype=torch.float32).unsqueeze(1).to(device)\n",
    "\n",
    "    # Set the model to evaluation mode\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        # Run the model to get the predicted binomial p values\n",
    "        predicted_p = model(X_sim_tensor)\n",
    "        \n",
    "        # Print the predicted binomial p values for inspection\n",
    "        print(\"Predicted binomial p values:\")\n",
    "        print(predicted_p.cpu().numpy())\n",
    "        \n",
    "        # Calculate the Mean Squared Error (MSE)\n",
    "        mse_loss = nn.MSELoss()(predicted_p, y_sim_tensor)\n",
    "        \n",
    "        # Calculate the RMSE\n",
    "        rmse = torch.sqrt(mse_loss).item()\n",
    "        print(f\"Root Mean Squared Error (RMSE): {rmse:.4f}\")\n",
    "    \n",
    "    # Return the predicted p values and RMSE\n",
    "    return predicted_p.cpu().numpy(), rmse\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted binomial p values:\n",
      "[[0.5018013]]\n",
      "Root Mean Squared Error (RMSE): 0.0018\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>refCount</th>\n",
       "      <th>altCount</th>\n",
       "      <th>MAF</th>\n",
       "      <th>min_MAF</th>\n",
       "      <th>diff_MAF</th>\n",
       "      <th>log10_distance</th>\n",
       "      <th>d</th>\n",
       "      <th>r2</th>\n",
       "      <th>switch</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>7</td>\n",
       "      <td>13</td>\n",
       "      <td>0.2893</td>\n",
       "      <td>0.2893</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>1.995635</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>11</td>\n",
       "      <td>9</td>\n",
       "      <td>0.1203</td>\n",
       "      <td>0.1143</td>\n",
       "      <td>0.0060</td>\n",
       "      <td>2.107210</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.912000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5</td>\n",
       "      <td>15</td>\n",
       "      <td>0.0636</td>\n",
       "      <td>0.0636</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>4.511456</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>12</td>\n",
       "      <td>8</td>\n",
       "      <td>0.4652</td>\n",
       "      <td>0.1876</td>\n",
       "      <td>0.2776</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.500</td>\n",
       "      <td>0.215599</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>8</td>\n",
       "      <td>12</td>\n",
       "      <td>0.4911</td>\n",
       "      <td>0.4583</td>\n",
       "      <td>0.0328</td>\n",
       "      <td>3.943890</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.876000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>9</td>\n",
       "      <td>11</td>\n",
       "      <td>0.1256</td>\n",
       "      <td>0.1256</td>\n",
       "      <td>0.3388</td>\n",
       "      <td>3.283979</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.146000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>0.4851</td>\n",
       "      <td>0.0895</td>\n",
       "      <td>0.3956</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.500</td>\n",
       "      <td>0.215599</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>11</td>\n",
       "      <td>9</td>\n",
       "      <td>0.4205</td>\n",
       "      <td>0.0954</td>\n",
       "      <td>0.3251</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.500</td>\n",
       "      <td>0.215599</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>9</td>\n",
       "      <td>11</td>\n",
       "      <td>0.3141</td>\n",
       "      <td>0.3131</td>\n",
       "      <td>0.0010</td>\n",
       "      <td>5.762318</td>\n",
       "      <td>0.705</td>\n",
       "      <td>0.433000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>0.3062</td>\n",
       "      <td>0.2028</td>\n",
       "      <td>0.1034</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.500</td>\n",
       "      <td>0.215599</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   refCount  altCount     MAF  min_MAF  diff_MAF  log10_distance      d  \\\n",
       "0         7        13  0.2893   0.2893    0.0000        1.995635  1.000   \n",
       "1        11         9  0.1203   0.1143    0.0060        2.107210  1.000   \n",
       "2         5        15  0.0636   0.0636    0.0000        4.511456  1.000   \n",
       "3        12         8  0.4652   0.1876    0.2776        0.000000  0.500   \n",
       "4         8        12  0.4911   0.4583    0.0328        3.943890  1.000   \n",
       "5         9        11  0.1256   0.1256    0.3388        3.283979  1.000   \n",
       "6        10        10  0.4851   0.0895    0.3956        0.000000  0.500   \n",
       "7        11         9  0.4205   0.0954    0.3251        0.000000  0.500   \n",
       "8         9        11  0.3141   0.3131    0.0010        5.762318  0.705   \n",
       "9        10        10  0.3062   0.2028    0.1034        0.000000  0.500   \n",
       "\n",
       "         r2  switch  \n",
       "0  1.000000       0  \n",
       "1  0.912000       0  \n",
       "2  1.000000       0  \n",
       "3  0.215599       0  \n",
       "4  0.876000       0  \n",
       "5  0.146000       0  \n",
       "6  0.215599       0  \n",
       "7  0.215599       0  \n",
       "8  0.433000       0  \n",
       "9  0.215599       0  "
      ]
     },
     "execution_count": 180,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Simulate data using the function\n",
    "X_sim, y_sim, df_vis = simulate_reads_single_gene(\n",
    "    p=0.5, read_coverage=20, phasing_error=0.04, num_hets=10, max_hets=64, data=train_df\n",
    ")\n",
    "\n",
    "# Call the evaluation function\n",
    "predicted_p_values, rmse = evaluate_simulation(X_sim, y_sim, model, device)\n",
    "\n",
    "# Visualize the simulated data if needed\n",
    "df_vis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted binomial p values:\n",
      "[[0.49537563]]\n",
      "Root Mean Squared Error (RMSE): 0.0046\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>refCount</th>\n",
       "      <th>altCount</th>\n",
       "      <th>MAF</th>\n",
       "      <th>min_MAF</th>\n",
       "      <th>diff_MAF</th>\n",
       "      <th>log10_distance</th>\n",
       "      <th>d</th>\n",
       "      <th>r2</th>\n",
       "      <th>switch</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>44</td>\n",
       "      <td>56</td>\n",
       "      <td>0.2893</td>\n",
       "      <td>0.2893</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>1.995635</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>52</td>\n",
       "      <td>48</td>\n",
       "      <td>0.1203</td>\n",
       "      <td>0.1143</td>\n",
       "      <td>0.0060</td>\n",
       "      <td>2.107210</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.912000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>58</td>\n",
       "      <td>42</td>\n",
       "      <td>0.0636</td>\n",
       "      <td>0.0636</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>4.511456</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>51</td>\n",
       "      <td>49</td>\n",
       "      <td>0.4652</td>\n",
       "      <td>0.1876</td>\n",
       "      <td>0.2776</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.500</td>\n",
       "      <td>0.215599</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>56</td>\n",
       "      <td>44</td>\n",
       "      <td>0.4911</td>\n",
       "      <td>0.4583</td>\n",
       "      <td>0.0328</td>\n",
       "      <td>3.943890</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.876000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>50</td>\n",
       "      <td>50</td>\n",
       "      <td>0.1256</td>\n",
       "      <td>0.1256</td>\n",
       "      <td>0.3388</td>\n",
       "      <td>3.283979</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.146000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>49</td>\n",
       "      <td>51</td>\n",
       "      <td>0.4851</td>\n",
       "      <td>0.0895</td>\n",
       "      <td>0.3956</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.500</td>\n",
       "      <td>0.215599</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>46</td>\n",
       "      <td>54</td>\n",
       "      <td>0.4205</td>\n",
       "      <td>0.0954</td>\n",
       "      <td>0.3251</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.500</td>\n",
       "      <td>0.215599</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>43</td>\n",
       "      <td>57</td>\n",
       "      <td>0.3141</td>\n",
       "      <td>0.3131</td>\n",
       "      <td>0.0010</td>\n",
       "      <td>5.762318</td>\n",
       "      <td>0.705</td>\n",
       "      <td>0.433000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>57</td>\n",
       "      <td>43</td>\n",
       "      <td>0.3062</td>\n",
       "      <td>0.2028</td>\n",
       "      <td>0.1034</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.500</td>\n",
       "      <td>0.215599</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   refCount  altCount     MAF  min_MAF  diff_MAF  log10_distance      d  \\\n",
       "0        44        56  0.2893   0.2893    0.0000        1.995635  1.000   \n",
       "1        52        48  0.1203   0.1143    0.0060        2.107210  1.000   \n",
       "2        58        42  0.0636   0.0636    0.0000        4.511456  1.000   \n",
       "3        51        49  0.4652   0.1876    0.2776        0.000000  0.500   \n",
       "4        56        44  0.4911   0.4583    0.0328        3.943890  1.000   \n",
       "5        50        50  0.1256   0.1256    0.3388        3.283979  1.000   \n",
       "6        49        51  0.4851   0.0895    0.3956        0.000000  0.500   \n",
       "7        46        54  0.4205   0.0954    0.3251        0.000000  0.500   \n",
       "8        43        57  0.3141   0.3131    0.0010        5.762318  0.705   \n",
       "9        57        43  0.3062   0.2028    0.1034        0.000000  0.500   \n",
       "\n",
       "         r2  switch  \n",
       "0  1.000000       0  \n",
       "1  0.912000       0  \n",
       "2  1.000000       0  \n",
       "3  0.215599       0  \n",
       "4  0.876000       0  \n",
       "5  0.146000       0  \n",
       "6  0.215599       0  \n",
       "7  0.215599       0  \n",
       "8  0.433000       0  \n",
       "9  0.215599       0  "
      ]
     },
     "execution_count": 181,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Simulate data using the function\n",
    "X_sim, y_sim, df_vis = simulate_reads_single_gene(\n",
    "    p=0.5, read_coverage=100, phasing_error=0.04, num_hets=10, max_hets=64, data=train_df\n",
    ")\n",
    "\n",
    "# Call the evaluation function\n",
    "predicted_p_values, rmse = evaluate_simulation(X_sim, y_sim, model, device)\n",
    "\n",
    "# Visualize the simulated data if needed\n",
    "df_vis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted binomial p values:\n",
      "[[0.9043281]]\n",
      "Root Mean Squared Error (RMSE): 0.8043\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>refCount</th>\n",
       "      <th>altCount</th>\n",
       "      <th>MAF</th>\n",
       "      <th>min_MAF</th>\n",
       "      <th>diff_MAF</th>\n",
       "      <th>log10_distance</th>\n",
       "      <th>d</th>\n",
       "      <th>r2</th>\n",
       "      <th>switch</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>6</td>\n",
       "      <td>94</td>\n",
       "      <td>0.2893</td>\n",
       "      <td>0.2893</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>1.995635</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>11</td>\n",
       "      <td>89</td>\n",
       "      <td>0.1203</td>\n",
       "      <td>0.1143</td>\n",
       "      <td>0.0060</td>\n",
       "      <td>2.107210</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.912000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4</td>\n",
       "      <td>96</td>\n",
       "      <td>0.0636</td>\n",
       "      <td>0.0636</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>4.511456</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>13</td>\n",
       "      <td>87</td>\n",
       "      <td>0.4652</td>\n",
       "      <td>0.1876</td>\n",
       "      <td>0.2776</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.500</td>\n",
       "      <td>0.215599</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7</td>\n",
       "      <td>93</td>\n",
       "      <td>0.4911</td>\n",
       "      <td>0.4583</td>\n",
       "      <td>0.0328</td>\n",
       "      <td>3.943890</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.876000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>8</td>\n",
       "      <td>92</td>\n",
       "      <td>0.1256</td>\n",
       "      <td>0.1256</td>\n",
       "      <td>0.3388</td>\n",
       "      <td>3.283979</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.146000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>9</td>\n",
       "      <td>91</td>\n",
       "      <td>0.4851</td>\n",
       "      <td>0.0895</td>\n",
       "      <td>0.3956</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.500</td>\n",
       "      <td>0.215599</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>11</td>\n",
       "      <td>89</td>\n",
       "      <td>0.4205</td>\n",
       "      <td>0.0954</td>\n",
       "      <td>0.3251</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.500</td>\n",
       "      <td>0.215599</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>8</td>\n",
       "      <td>92</td>\n",
       "      <td>0.3141</td>\n",
       "      <td>0.3131</td>\n",
       "      <td>0.0010</td>\n",
       "      <td>5.762318</td>\n",
       "      <td>0.705</td>\n",
       "      <td>0.433000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>10</td>\n",
       "      <td>90</td>\n",
       "      <td>0.3062</td>\n",
       "      <td>0.2028</td>\n",
       "      <td>0.1034</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.500</td>\n",
       "      <td>0.215599</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   refCount  altCount     MAF  min_MAF  diff_MAF  log10_distance      d  \\\n",
       "0         6        94  0.2893   0.2893    0.0000        1.995635  1.000   \n",
       "1        11        89  0.1203   0.1143    0.0060        2.107210  1.000   \n",
       "2         4        96  0.0636   0.0636    0.0000        4.511456  1.000   \n",
       "3        13        87  0.4652   0.1876    0.2776        0.000000  0.500   \n",
       "4         7        93  0.4911   0.4583    0.0328        3.943890  1.000   \n",
       "5         8        92  0.1256   0.1256    0.3388        3.283979  1.000   \n",
       "6         9        91  0.4851   0.0895    0.3956        0.000000  0.500   \n",
       "7        11        89  0.4205   0.0954    0.3251        0.000000  0.500   \n",
       "8         8        92  0.3141   0.3131    0.0010        5.762318  0.705   \n",
       "9        10        90  0.3062   0.2028    0.1034        0.000000  0.500   \n",
       "\n",
       "         r2  switch  \n",
       "0  1.000000       0  \n",
       "1  0.912000       0  \n",
       "2  1.000000       0  \n",
       "3  0.215599       0  \n",
       "4  0.876000       0  \n",
       "5  0.146000       0  \n",
       "6  0.215599       0  \n",
       "7  0.215599       0  \n",
       "8  0.433000       0  \n",
       "9  0.215599       0  "
      ]
     },
     "execution_count": 182,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Simulate data using the function\n",
    "X_sim, y_sim, df_vis = simulate_reads_single_gene(\n",
    "    p=0.1, read_coverage=100, phasing_error=0.04, num_hets=10, max_hets=64, data=train_df\n",
    ")\n",
    "\n",
    "# Call the evaluation function\n",
    "predicted_p_values, rmse = evaluate_simulation(X_sim, y_sim, model, device)\n",
    "\n",
    "# Visualize the simulated data if needed\n",
    "df_vis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'simulate_reads_single_gene' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Simulate data using the function\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m X_sim, y_sim, df_vis \u001b[38;5;241m=\u001b[39m \u001b[43msimulate_reads_single_gene\u001b[49m(\n\u001b[1;32m      3\u001b[0m     p\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.1\u001b[39m, read_coverage\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m100\u001b[39m, phasing_error\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.5\u001b[39m, num_hets\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10\u001b[39m, max_hets\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m64\u001b[39m, data\u001b[38;5;241m=\u001b[39mtrain_df\n\u001b[1;32m      4\u001b[0m )\n\u001b[1;32m      6\u001b[0m \u001b[38;5;66;03m# Call the evaluation function\u001b[39;00m\n\u001b[1;32m      7\u001b[0m predicted_p_values, rmse \u001b[38;5;241m=\u001b[39m evaluate_simulation(X_sim, y_sim, model, device)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'simulate_reads_single_gene' is not defined"
     ]
    }
   ],
   "source": [
    "# Simulate data using the function\n",
    "X_sim, y_sim, df_vis = simulate_reads_single_gene(\n",
    "    p=0.1, read_coverage=100, phasing_error=0.5, num_hets=10, max_hets=64, data=train_df\n",
    ")\n",
    "\n",
    "# Call the evaluation function\n",
    "predicted_p_values, rmse = evaluate_simulation(X_sim, y_sim, model, device)\n",
    "\n",
    "# Visualize the simulated data if needed\n",
    "df_vis\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare data\n",
    "\n",
    "- Group SNPs by Gene: Arrange them in sequential order within each gene based on their position.\n",
    "- Create Fixed-Length Sequences: Like with CNN input, you'll need a consistent sequence length. For each gene, you can either:\n",
    "- Pad sequences with zeros to the maximum length of SNPs in any gene.\n",
    "- Use a shorter maximum length and truncate longer sequences.\n",
    "- Feature Encoding: Use the SNP feature values (such as feature1, feature2, etc.) as input for each time step. Each SNPs features will form a vector that acts as the input at each time step in the RNN sequence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>geneID</th>\n",
       "      <th>chrN</th>\n",
       "      <th>pos</th>\n",
       "      <th>refCount</th>\n",
       "      <th>altCount</th>\n",
       "      <th>MAF</th>\n",
       "      <th>min_MAF</th>\n",
       "      <th>diff_MAF</th>\n",
       "      <th>d</th>\n",
       "      <th>r2</th>\n",
       "      <th>log10_distance</th>\n",
       "      <th>individual</th>\n",
       "      <th>ancestry</th>\n",
       "      <th>qb_label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ENSG00000177757</td>\n",
       "      <td>1</td>\n",
       "      <td>754182</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0.1282</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>HG00234</td>\n",
       "      <td>GBR</td>\n",
       "      <td>0.475200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ENSG00000177757</td>\n",
       "      <td>1</td>\n",
       "      <td>754503</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.1610</td>\n",
       "      <td>0.1282</td>\n",
       "      <td>0.0328</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.868</td>\n",
       "      <td>2.506505</td>\n",
       "      <td>HG00234</td>\n",
       "      <td>GBR</td>\n",
       "      <td>0.475200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ENSG00000177757</td>\n",
       "      <td>1</td>\n",
       "      <td>754964</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.1630</td>\n",
       "      <td>0.1610</td>\n",
       "      <td>0.0020</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>2.663701</td>\n",
       "      <td>HG00234</td>\n",
       "      <td>GBR</td>\n",
       "      <td>0.475200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ENSG00000225880</td>\n",
       "      <td>1</td>\n",
       "      <td>762601</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>0.1282</td>\n",
       "      <td>0.1282</td>\n",
       "      <td>0.0348</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>HG00234</td>\n",
       "      <td>GBR</td>\n",
       "      <td>0.477750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ENSG00000187634</td>\n",
       "      <td>1</td>\n",
       "      <td>879317</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0398</td>\n",
       "      <td>0.0398</td>\n",
       "      <td>0.0884</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>HG00234</td>\n",
       "      <td>GBR</td>\n",
       "      <td>0.530147</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1751402</th>\n",
       "      <td>ENSG00000181090</td>\n",
       "      <td>9</td>\n",
       "      <td>140707590</td>\n",
       "      <td>36</td>\n",
       "      <td>38</td>\n",
       "      <td>0.4017</td>\n",
       "      <td>0.2307</td>\n",
       "      <td>0.1710</td>\n",
       "      <td>0.926</td>\n",
       "      <td>0.305</td>\n",
       "      <td>4.460251</td>\n",
       "      <td>NA19149</td>\n",
       "      <td>YRI</td>\n",
       "      <td>0.550008</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1751403</th>\n",
       "      <td>ENSG00000181090</td>\n",
       "      <td>9</td>\n",
       "      <td>140709103</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>0.4009</td>\n",
       "      <td>0.4009</td>\n",
       "      <td>0.0008</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>3.179839</td>\n",
       "      <td>NA19149</td>\n",
       "      <td>YRI</td>\n",
       "      <td>0.550008</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1751404</th>\n",
       "      <td>ENSG00000181090</td>\n",
       "      <td>9</td>\n",
       "      <td>140709880</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>0.3669</td>\n",
       "      <td>0.3669</td>\n",
       "      <td>0.0340</td>\n",
       "      <td>0.968</td>\n",
       "      <td>0.372</td>\n",
       "      <td>2.890421</td>\n",
       "      <td>NA19149</td>\n",
       "      <td>YRI</td>\n",
       "      <td>0.550008</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1751405</th>\n",
       "      <td>ENSG00000181090</td>\n",
       "      <td>9</td>\n",
       "      <td>140711882</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.3933</td>\n",
       "      <td>0.3669</td>\n",
       "      <td>0.0264</td>\n",
       "      <td>0.826</td>\n",
       "      <td>0.641</td>\n",
       "      <td>3.301464</td>\n",
       "      <td>NA19149</td>\n",
       "      <td>YRI</td>\n",
       "      <td>0.550008</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1751406</th>\n",
       "      <td>ENSG00000181090</td>\n",
       "      <td>9</td>\n",
       "      <td>140727327</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0930</td>\n",
       "      <td>0.0930</td>\n",
       "      <td>0.3003</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.074</td>\n",
       "      <td>4.188788</td>\n",
       "      <td>NA19149</td>\n",
       "      <td>YRI</td>\n",
       "      <td>0.550008</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1751407 rows  14 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                  geneID  chrN        pos  refCount  altCount     MAF  \\\n",
       "0        ENSG00000177757     1     754182         2         1  0.1282   \n",
       "1        ENSG00000177757     1     754503         1         0  0.1610   \n",
       "2        ENSG00000177757     1     754964         0         1  0.1630   \n",
       "3        ENSG00000225880     1     762601         4         3  0.1282   \n",
       "4        ENSG00000187634     1     879317         0         1  0.0398   \n",
       "...                  ...   ...        ...       ...       ...     ...   \n",
       "1751402  ENSG00000181090     9  140707590        36        38  0.4017   \n",
       "1751403  ENSG00000181090     9  140709103         3         2  0.4009   \n",
       "1751404  ENSG00000181090     9  140709880         2         5  0.3669   \n",
       "1751405  ENSG00000181090     9  140711882         0         1  0.3933   \n",
       "1751406  ENSG00000181090     9  140727327         1         1  0.0930   \n",
       "\n",
       "         min_MAF  diff_MAF      d     r2  log10_distance individual ancestry  \\\n",
       "0            NaN       NaN    NaN    NaN             NaN    HG00234      GBR   \n",
       "1         0.1282    0.0328  1.000  0.868        2.506505    HG00234      GBR   \n",
       "2         0.1610    0.0020  1.000  1.000        2.663701    HG00234      GBR   \n",
       "3         0.1282    0.0348    NaN    NaN             NaN    HG00234      GBR   \n",
       "4         0.0398    0.0884    NaN    NaN             NaN    HG00234      GBR   \n",
       "...          ...       ...    ...    ...             ...        ...      ...   \n",
       "1751402   0.2307    0.1710  0.926  0.305        4.460251    NA19149      YRI   \n",
       "1751403   0.4009    0.0008  1.000  1.000        3.179839    NA19149      YRI   \n",
       "1751404   0.3669    0.0340  0.968  0.372        2.890421    NA19149      YRI   \n",
       "1751405   0.3669    0.0264  0.826  0.641        3.301464    NA19149      YRI   \n",
       "1751406   0.0930    0.3003  1.000  0.074        4.188788    NA19149      YRI   \n",
       "\n",
       "         qb_label  \n",
       "0        0.475200  \n",
       "1        0.475200  \n",
       "2        0.475200  \n",
       "3        0.477750  \n",
       "4        0.530147  \n",
       "...           ...  \n",
       "1751402  0.550008  \n",
       "1751403  0.550008  \n",
       "1751404  0.550008  \n",
       "1751405  0.550008  \n",
       "1751406  0.550008  \n",
       "\n",
       "[1751407 rows x 14 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_data_for_rnn_individual(data, feature_columns, label_col='qb_label', max_snp_per_gene=64):\n",
    "    # Sort by individual, geneID, and SNP position to ensure sequence order\n",
    "    data = data.sort_values(by=['individual', 'geneID', 'pos'])\n",
    "    \n",
    "    # Fill missing values in the same way as the CNN data preparation function\n",
    "    data['log10_distance'] = data['log10_distance'].fillna(0)\n",
    "    data['d'] = data['d'].fillna(0.5)\n",
    "    data['r2'] = data['r2'].fillna(np.random.uniform(0.2, 0.3))\n",
    "    data['min_MAF'] = data['min_MAF'].fillna(0)\n",
    "    data['diff_MAF'] = data['diff_MAF'].fillna(0)\n",
    "\n",
    "    X_data = []\n",
    "    y_data = []\n",
    "\n",
    "    # Group by individual and geneID to form RNN sequences\n",
    "    grouped = data.groupby(['individual', 'geneID'])\n",
    "    for (individual, geneID), group in grouped:\n",
    "        # Extract feature columns for the SNPs in this gene\n",
    "        X = group[feature_columns].to_numpy()\n",
    "\n",
    "        # Pad or truncate SNP sequence to fixed length (max_snp_per_gene)\n",
    "        if X.shape[0] < max_snp_per_gene:\n",
    "            padded_X = np.zeros((max_snp_per_gene, len(feature_columns)))\n",
    "            padded_X[:X.shape[0], :] = X\n",
    "        else:\n",
    "            padded_X = X[:max_snp_per_gene, :]\n",
    "        \n",
    "        # Get the label (assumes it's the same across all rows in the group)\n",
    "        y = group[label_col].iloc[0]\n",
    "        \n",
    "        # Append to the lists\n",
    "        X_data.append(padded_X)\n",
    "        y_data.append(y)\n",
    "\n",
    "    # Convert to numpy arrays\n",
    "    X_data = np.array(X_data)\n",
    "    y_data = np.array(y_data)\n",
    "\n",
    "    return X_data, y_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X shape: (597797, 63, 8)\n",
      "y shape: (597797,)\n"
     ]
    }
   ],
   "source": [
    "data_train = train_df\n",
    "feature_columns = ['refCount', 'altCount', 'MAF', 'min_MAF', 'diff_MAF', 'log10_distance', 'd', 'r2']\n",
    "X_train, y_train = prepare_data_for_rnn_individual(data_train, feature_columns)\n",
    "print(\"X shape:\", X_train.shape)\n",
    "print(\"y shape:\", y_train.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X shape: (145906, 64, 8)\n",
      "y shape: (145906,)\n"
     ]
    }
   ],
   "source": [
    "data_test = test_df\n",
    "feature_columns = ['refCount', 'altCount', 'MAF', 'min_MAF', 'diff_MAF', 'log10_distance', 'd', 'r2']\n",
    "X_test, y_test = prepare_data_for_rnn_individual(data_test, feature_columns)\n",
    "print(\"X shape:\", X_test.shape)\n",
    "print(\"y shape:\", y_test.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GeneLSTM(nn.Module):\n",
    "    def __init__(self, input_features):\n",
    "        super(GeneLSTM, self).__init__()\n",
    "\n",
    "        # LSTM layer with increased hidden size and additional layers\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=input_features, \n",
    "            hidden_size=128,  # Increased hidden size\n",
    "            num_layers=4,     # Increased number of layers\n",
    "            batch_first=True, \n",
    "            dropout=0.2, \n",
    "            bidirectional=True\n",
    "        )\n",
    "\n",
    "        # Fully connected layers with batch normalization\n",
    "        self.fc1 = nn.Linear(128 * 2, 128)  # Adjusted for bidirectional output\n",
    "        self.bn1 = nn.BatchNorm1d(128)\n",
    "        self.fc2 = nn.Linear(128, 64)\n",
    "        self.bn2 = nn.BatchNorm1d(64)\n",
    "        self.fc3 = nn.Linear(64, 1)\n",
    "\n",
    "        # Dropout layer\n",
    "        self.dropout = nn.Dropout(0.3)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Initialize hidden and cell states for bidirectional LSTM\n",
    "        h0 = torch.zeros(4 * 2, x.size(0), 128).to(x.device)  # 4 layers * 2 directions\n",
    "        c0 = torch.zeros(4 * 2, x.size(0), 128).to(x.device)\n",
    "\n",
    "        # Forward propagate LSTM\n",
    "        out, _ = self.lstm(x, (h0, c0))\n",
    "\n",
    "        # Take the output from the last time step\n",
    "        out = out[:, -1, :]  # Shape: [batch_size, 128 * 2]\n",
    "        \n",
    "        # Pass through fully connected layers with batch normalization and dropout\n",
    "        out = torch.relu(self.bn1(self.fc1(out)))\n",
    "        out = self.dropout(out)\n",
    "        out = torch.relu(self.bn2(self.fc2(out)))\n",
    "        out = self.fc3(out)\n",
    "        \n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GeneGRU(nn.Module):\n",
    "    def __init__(self, input_features):\n",
    "        super(GeneGRU, self).__init__()\n",
    "\n",
    "        # GRU layer with increased hidden size and more layers\n",
    "        self.gru = nn.GRU(\n",
    "            input_size=input_features, \n",
    "            hidden_size=128, \n",
    "            num_layers=4, \n",
    "            batch_first=True, \n",
    "            dropout=0.2, \n",
    "            bidirectional=True\n",
    "        )\n",
    "\n",
    "        # Fully connected layers with batch normalization\n",
    "        self.fc1 = nn.Linear(128 * 2, 128)\n",
    "        self.bn1 = nn.BatchNorm1d(128)\n",
    "        self.fc2 = nn.Linear(128, 64)\n",
    "        self.bn2 = nn.BatchNorm1d(64)\n",
    "        self.fc3 = nn.Linear(64, 1)\n",
    "\n",
    "        # Dropout layer\n",
    "        self.dropout = nn.Dropout(0.3)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Initialize hidden state for bidirectional GRU\n",
    "        h0 = torch.zeros(4 * 2, x.size(0), 128).to(x.device)  # 4 layers * 2 directions\n",
    "\n",
    "        # Forward propagate GRU\n",
    "        out, _ = self.gru(x, h0)\n",
    "\n",
    "        # Take the output from the last time step\n",
    "        out = out[:, -1, :]  # Shape: [batch_size, 128 * 2]\n",
    "        \n",
    "        # Pass through fully connected layers with batch normalization and dropout\n",
    "        out = torch.relu(self.bn1(self.fc1(out)))\n",
    "        out = self.dropout(out)\n",
    "        out = torch.relu(self.bn2(self.fc2(out)))\n",
    "        out = self.fc3(out)\n",
    "        \n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GeneGRU(\n",
      "  (gru): GRU(8, 128, num_layers=4, batch_first=True, dropout=0.2, bidirectional=True)\n",
      "  (fc1): Linear(in_features=256, out_features=128, bias=True)\n",
      "  (bn1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (fc2): Linear(in_features=128, out_features=64, bias=True)\n",
      "  (bn2): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (fc3): Linear(in_features=64, out_features=1, bias=True)\n",
      "  (dropout): Dropout(p=0.3, inplace=False)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# Instantiate the model, loss function, and optimizer\n",
    "max_hets_per_gene = X_train.shape[2]  # Use the maximum number of hets per gene\n",
    "rnn_model = GeneGRU(input_features = max_hets_per_gene) # 6 features\n",
    "\n",
    "# Model summary\n",
    "print(rnn_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2381489/2152187196.py:6: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  X_train = torch.tensor(X_train, dtype=torch.float32)\n",
      "/tmp/ipykernel_2381489/2152187196.py:7: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  y_train = torch.tensor(y_train, dtype=torch.float32).unsqueeze(1)\n",
      "/tmp/ipykernel_2381489/2152187196.py:8: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  X_val = torch.tensor(X_val, dtype=torch.float32)\n",
      "/tmp/ipykernel_2381489/2152187196.py:9: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  y_val = torch.tensor(y_val, dtype=torch.float32).unsqueeze(1)\n",
      "/tmp/ipykernel_2381489/2152187196.py:11: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  X_test = torch.tensor(X_test, dtype=torch.float32)\n",
      "/tmp/ipykernel_2381489/2152187196.py:12: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  y_test = torch.tensor(y_test, dtype=torch.float32).unsqueeze(1)\n"
     ]
    }
   ],
   "source": [
    "# Assuming X_train and y_train are initially your full training data\n",
    "# Set aside a portion for validation\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.25, random_state=42, shuffle=True)\n",
    "\n",
    "# Convert them to PyTorch tensors\n",
    "X_train = torch.tensor(X_train, dtype=torch.float32)\n",
    "y_train = torch.tensor(y_train, dtype=torch.float32).unsqueeze(1)\n",
    "X_val = torch.tensor(X_val, dtype=torch.float32)\n",
    "y_val = torch.tensor(y_val, dtype=torch.float32).unsqueeze(1)\n",
    "\n",
    "X_test = torch.tensor(X_test, dtype=torch.float32)\n",
    "y_test = torch.tensor(y_test, dtype=torch.float32).unsqueeze(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a dataset and a DataLoader with a smaller batch size\n",
    "train_dataset = TensorDataset(X_train, y_train)\n",
    "val_dataset = TensorDataset(X_val, y_val)\n",
    "train_loader = DataLoader(train_dataset, batch_size=128, shuffle=True)  # Adjust batch size as needed\n",
    "val_loader = DataLoader(val_dataset, batch_size=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/scarlett/.local/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:60: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\n",
      "/home/scarlett/.local/lib/python3.10/site-packages/torch/nn/modules/loss.py:538: UserWarning: Using a target size (torch.Size([128, 1, 1, 1, 1, 1, 1])) that is different to the input size (torch.Size([128, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/home/scarlett/.local/lib/python3.10/site-packages/torch/nn/modules/loss.py:538: UserWarning: Using a target size (torch.Size([111, 1, 1, 1, 1, 1, 1])) that is different to the input size (torch.Size([111, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/home/scarlett/.local/lib/python3.10/site-packages/torch/nn/modules/loss.py:538: UserWarning: Using a target size (torch.Size([80, 1, 1, 1, 1, 1, 1])) that is different to the input size (torch.Size([80, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/50], Train RMSE: 0.1406, Validation RMSE: 0.0782\n",
      "Epoch [2/50], Train RMSE: 0.0876, Validation RMSE: 0.1931\n",
      "Epoch [3/50], Train RMSE: 0.0795, Validation RMSE: 0.1788\n",
      "Epoch [4/50], Train RMSE: 0.0768, Validation RMSE: 0.1530\n",
      "Epoch [5/50], Train RMSE: 0.0754, Validation RMSE: 0.1556\n",
      "Epoch [6/50], Train RMSE: 0.0747, Validation RMSE: 0.1440\n",
      "Epoch [7/50], Train RMSE: 0.0744, Validation RMSE: 0.1487\n",
      "Epoch [8/50], Train RMSE: 0.0742, Validation RMSE: 0.1355\n",
      "Epoch [9/50], Train RMSE: 0.0741, Validation RMSE: 0.1339\n",
      "Epoch [10/50], Train RMSE: 0.0738, Validation RMSE: 0.1397\n",
      "Epoch [11/50], Train RMSE: 0.0737, Validation RMSE: 0.1388\n",
      "Epoch [12/50], Train RMSE: 0.0737, Validation RMSE: 0.1343\n",
      "Epoch [13/50], Train RMSE: 0.0736, Validation RMSE: 0.1215\n",
      "Epoch [14/50], Train RMSE: 0.0735, Validation RMSE: 0.1388\n",
      "Epoch [15/50], Train RMSE: 0.0734, Validation RMSE: 0.1306\n",
      "Epoch [16/50], Train RMSE: 0.0734, Validation RMSE: 0.1256\n",
      "Epoch [17/50], Train RMSE: 0.0735, Validation RMSE: 0.1041\n",
      "Epoch [18/50], Train RMSE: 0.0733, Validation RMSE: 0.1285\n",
      "Epoch [19/50], Train RMSE: 0.0733, Validation RMSE: 0.1157\n",
      "Epoch [20/50], Train RMSE: 0.0733, Validation RMSE: 0.1269\n",
      "Epoch [21/50], Train RMSE: 0.0732, Validation RMSE: 0.1182\n",
      "Epoch [22/50], Train RMSE: 0.0732, Validation RMSE: 0.1240\n",
      "Epoch [23/50], Train RMSE: 0.0732, Validation RMSE: 0.1166\n",
      "Epoch [24/50], Train RMSE: 0.0732, Validation RMSE: 0.1197\n",
      "Epoch [25/50], Train RMSE: 0.0732, Validation RMSE: 0.1207\n",
      "Epoch [26/50], Train RMSE: 0.0732, Validation RMSE: 0.1206\n",
      "Epoch [27/50], Train RMSE: 0.0732, Validation RMSE: 0.1234\n",
      "Epoch [28/50], Train RMSE: 0.0732, Validation RMSE: 0.1222\n",
      "Epoch [29/50], Train RMSE: 0.0731, Validation RMSE: 0.1069\n",
      "Epoch [30/50], Train RMSE: 0.0731, Validation RMSE: 0.1168\n",
      "Epoch [31/50], Train RMSE: 0.0731, Validation RMSE: 0.1153\n",
      "Epoch [32/50], Train RMSE: 0.0732, Validation RMSE: 0.1196\n",
      "Epoch [33/50], Train RMSE: 0.0732, Validation RMSE: 0.1126\n",
      "Epoch [34/50], Train RMSE: 0.0732, Validation RMSE: 0.1171\n",
      "Epoch [35/50], Train RMSE: 0.0731, Validation RMSE: 0.1244\n",
      "Epoch [36/50], Train RMSE: 0.0732, Validation RMSE: 0.1196\n",
      "Epoch [37/50], Train RMSE: 0.0732, Validation RMSE: 0.1185\n",
      "Epoch [38/50], Train RMSE: 0.0731, Validation RMSE: 0.1196\n",
      "Epoch [39/50], Train RMSE: 0.0731, Validation RMSE: 0.1217\n",
      "Epoch [40/50], Train RMSE: 0.0732, Validation RMSE: 0.1158\n",
      "Epoch [41/50], Train RMSE: 0.0731, Validation RMSE: 0.1096\n",
      "Epoch [42/50], Train RMSE: 0.0731, Validation RMSE: 0.1154\n",
      "Epoch [43/50], Train RMSE: 0.0732, Validation RMSE: 0.1217\n",
      "Epoch [44/50], Train RMSE: 0.0731, Validation RMSE: 0.1170\n",
      "Epoch [45/50], Train RMSE: 0.0731, Validation RMSE: 0.1213\n",
      "Epoch [46/50], Train RMSE: 0.0732, Validation RMSE: 0.1166\n",
      "Epoch [47/50], Train RMSE: 0.0731, Validation RMSE: 0.1220\n",
      "Epoch [48/50], Train RMSE: 0.0731, Validation RMSE: 0.1169\n",
      "Epoch [49/50], Train RMSE: 0.0731, Validation RMSE: 0.1177\n",
      "Epoch [50/50], Train RMSE: 0.0732, Validation RMSE: 0.1167\n"
     ]
    }
   ],
   "source": [
    "# Set device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Initialize the model\n",
    "GRU_model = GeneGRU(input_features=len(feature_columns)).to(device)\n",
    "\n",
    "# Define loss function, optimizer, and scheduler\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(rnn_model.parameters(), lr=0.001)\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', patience=3, factor=0.5, verbose=True)\n",
    "\n",
    "# Lists to store the training and validation losses\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "\n",
    "# Training loop\n",
    "epochs = 50\n",
    "for epoch in range(epochs):\n",
    "    GRU_model.train()\n",
    "    train_loss_epoch = 0.0\n",
    "    \n",
    "    for X_batch, y_batch in train_loader:\n",
    "        X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Forward pass\n",
    "        outputs = rnn_model(X_batch)\n",
    "        train_mse_loss = criterion(outputs, y_batch)  # MSE Loss\n",
    "        train_rmse_loss = torch.sqrt(train_mse_loss)  # RMSE\n",
    "        \n",
    "        # Backward pass\n",
    "        train_mse_loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(GRU_model.parameters(), max_norm=1.0)  # Corrected model reference\n",
    "        optimizer.step()\n",
    "        \n",
    "        train_loss_epoch += train_mse_loss.item()  # Accumulate MSE loss\n",
    "    \n",
    "    # Validation\n",
    "    GRU_model.eval()\n",
    "    val_loss_epoch = 0.0\n",
    "    with torch.no_grad():\n",
    "        for X_val_batch, y_val_batch in val_loader:\n",
    "            X_val_batch, y_val_batch = X_val_batch.to(device), y_val_batch.to(device)\n",
    "            val_outputs = GRU_model(X_val_batch)  # Corrected model reference\n",
    "            val_mse_loss = criterion(val_outputs, y_val_batch)\n",
    "            val_loss_epoch += val_mse_loss.item()  # Accumulate MSE loss\n",
    "\n",
    "    # Calculate average RMSE for the epoch\n",
    "    avg_train_rmse = torch.sqrt(torch.tensor(train_loss_epoch / len(train_loader))).item()\n",
    "    avg_val_rmse = torch.sqrt(torch.tensor(val_loss_epoch / len(val_loader))).item()\n",
    "\n",
    "    # Store the RMSE losses for each epoch\n",
    "    train_losses.append(avg_train_rmse)\n",
    "    val_losses.append(avg_val_rmse)\n",
    "    \n",
    "    # Step the scheduler based on validation loss\n",
    "    scheduler.step(avg_val_rmse)\n",
    "    \n",
    "    print(f\"Epoch [{epoch+1}/{epochs}], Train RMSE: {avg_train_rmse:.4f}, Validation RMSE: {avg_val_rmse:.4f}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x500 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(range(1, epochs + 1), train_losses, label='Training RMSE')\n",
    "plt.plot(range(1, epochs + 1), val_losses, label='Validation RMSE')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('RMSE')\n",
    "plt.title('Training and Validation RMSE')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'GRU_model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[54], line 6\u001b[0m\n\u001b[1;32m      3\u001b[0m test_loader \u001b[38;5;241m=\u001b[39m DataLoader(test_dataset, batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m64\u001b[39m)  \u001b[38;5;66;03m# Adjust batch size as needed\u001b[39;00m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;66;03m# Final evaluation on test set\u001b[39;00m\n\u001b[0;32m----> 6\u001b[0m \u001b[43mGRU_model\u001b[49m\u001b[38;5;241m.\u001b[39meval()\n\u001b[1;32m      7\u001b[0m test_loss_total \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.0\u001b[39m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n",
      "\u001b[0;31mNameError\u001b[0m: name 'GRU_model' is not defined"
     ]
    }
   ],
   "source": [
    "# Create a DataLoader for the test set with a smaller batch size\n",
    "test_dataset = TensorDataset(X_test, y_test)\n",
    "test_loader = DataLoader(test_dataset, batch_size=64)  # Adjust batch size as needed\n",
    "\n",
    "# Final evaluation on test set\n",
    "GRU_model.eval()\n",
    "test_loss_total = 0.0\n",
    "with torch.no_grad():\n",
    "    for X_batch, y_batch in test_loader:\n",
    "        X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
    "        test_outputs = model(X_batch)\n",
    "        test_loss = criterion(test_outputs, y_batch)  # MSE Loss\n",
    "        test_loss_total += test_loss.item() * X_batch.size(0)  # Sum up the batch loss (weighted by batch size)\n",
    "\n",
    "# Calculate the average loss and take the square root for RMSE\n",
    "average_test_mse = test_loss_total / len(test_loader.dataset)\n",
    "average_test_rmse = torch.sqrt(torch.tensor(average_test_mse)).item()\n",
    "print(f\"Final Test RMSE: {average_test_rmse:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/scarlett/.local/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:60: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\n",
      "/home/scarlett/.local/lib/python3.10/site-packages/torch/nn/modules/loss.py:538: UserWarning: Using a target size (torch.Size([128, 1, 1, 1, 1, 1, 1])) that is different to the input size (torch.Size([128, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/home/scarlett/.local/lib/python3.10/site-packages/torch/nn/modules/loss.py:538: UserWarning: Using a target size (torch.Size([111, 1, 1, 1, 1, 1, 1])) that is different to the input size (torch.Size([111, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/home/scarlett/.local/lib/python3.10/site-packages/torch/nn/modules/loss.py:538: UserWarning: Using a target size (torch.Size([80, 1, 1, 1, 1, 1, 1])) that is different to the input size (torch.Size([80, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/50], Train RMSE: 0.5578, Validation RMSE: 0.1167\n",
      "Epoch [2/50], Train RMSE: 0.5592, Validation RMSE: 0.1167\n",
      "Epoch [3/50], Train RMSE: 0.5587, Validation RMSE: 0.1167\n",
      "Epoch [4/50], Train RMSE: 0.5576, Validation RMSE: 0.1167\n",
      "Epoch [5/50], Train RMSE: 0.5585, Validation RMSE: 0.1167\n"
     ]
    }
   ],
   "source": [
    "# Set device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Initialize the model\n",
    "LSTM_model = GeneLSTM(input_features=len(feature_columns)).to(device)\n",
    "\n",
    "# Define loss function, optimizer, and scheduler\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(rnn_model.parameters(), lr=0.001)\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', patience=3, factor=0.5, verbose=True)\n",
    "\n",
    "# Lists to store the training and validation losses\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "\n",
    "# Training loop\n",
    "epochs = 50\n",
    "for epoch in range(epochs):\n",
    "    LSTM_model.train()\n",
    "    train_loss_epoch = 0.0\n",
    "    \n",
    "    for X_batch, y_batch in train_loader:\n",
    "        X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Forward pass\n",
    "        outputs = LSTM_model(X_batch)\n",
    "        train_mse_loss = criterion(outputs, y_batch)  # MSE Loss\n",
    "        train_rmse_loss = torch.sqrt(train_mse_loss)  # RMSE\n",
    "        \n",
    "        # Backward pass\n",
    "        train_mse_loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(rnn_model.parameters(), max_norm=1.0)  # Corrected model reference\n",
    "        optimizer.step()\n",
    "        \n",
    "        train_loss_epoch += train_mse_loss.item()  # Accumulate MSE loss\n",
    "    \n",
    "    # Validation\n",
    "    rnn_model.eval()\n",
    "    val_loss_epoch = 0.0\n",
    "    with torch.no_grad():\n",
    "        for X_val_batch, y_val_batch in val_loader:\n",
    "            X_val_batch, y_val_batch = X_val_batch.to(device), y_val_batch.to(device)\n",
    "            val_outputs = rnn_model(X_val_batch)  # Corrected model reference\n",
    "            val_mse_loss = criterion(val_outputs, y_val_batch)\n",
    "            val_loss_epoch += val_mse_loss.item()  # Accumulate MSE loss\n",
    "\n",
    "    # Calculate average RMSE for the epoch\n",
    "    avg_train_rmse = torch.sqrt(torch.tensor(train_loss_epoch / len(train_loader))).item()\n",
    "    avg_val_rmse = torch.sqrt(torch.tensor(val_loss_epoch / len(val_loader))).item()\n",
    "\n",
    "    # Store the RMSE losses for each epoch\n",
    "    train_losses.append(avg_train_rmse)\n",
    "    val_losses.append(avg_val_rmse)\n",
    "    \n",
    "    # Step the scheduler based on validation loss\n",
    "    scheduler.step(avg_val_rmse)\n",
    "    \n",
    "    print(f\"Epoch [{epoch+1}/{epochs}], Train RMSE: {avg_train_rmse:.4f}, Validation RMSE: {avg_val_rmse:.4f}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(range(1, epochs + 1), train_losses, label='Training RMSE')\n",
    "plt.plot(range(1, epochs + 1), val_losses, label='Validation RMSE')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('RMSE')\n",
    "plt.title('Training and Validation RMSE')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a DataLoader for the test set with a smaller batch size\n",
    "test_dataset = TensorDataset(X_test, y_test)\n",
    "test_loader = DataLoader(test_dataset, batch_size=64)  # Adjust batch size as needed\n",
    "\n",
    "# Final evaluation on test set\n",
    "LSTM_model.eval()\n",
    "test_loss_total = 0.0\n",
    "with torch.no_grad():\n",
    "    for X_batch, y_batch in test_loader:\n",
    "        X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
    "        test_outputs = model(X_batch)\n",
    "        test_loss = criterion(test_outputs, y_batch)  # MSE Loss\n",
    "        test_loss_total += test_loss.item() * X_batch.size(0)  # Sum up the batch loss (weighted by batch size)\n",
    "\n",
    "# Calculate the average loss and take the square root for RMSE\n",
    "average_test_mse = test_loss_total / len(test_loader.dataset)\n",
    "average_test_rmse = torch.sqrt(torch.tensor(average_test_mse)).item()\n",
    "print(f\"Final Test RMSE: {average_test_rmse:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
