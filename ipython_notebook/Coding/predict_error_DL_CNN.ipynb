{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.1.1\n"
     ]
    }
   ],
   "source": [
    "# Data handling and processing\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "import os \n",
    "# Data visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Model evaluation and utilities\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import log_loss\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
    "warnings.filterwarnings('ignore', message=\"is_sparse is deprecated\")\n",
    "from sklearn.model_selection import ParameterSampler\n",
    "from sklearn.metrics import roc_auc_score, brier_score_loss\n",
    "\n",
    "# XGBoost\n",
    "import xgboost as xgb\n",
    "print(xgb.__version__)\n",
    "\n",
    "# Statsmodels\n",
    "import statsmodels.api as sm\n",
    "\n",
    "# PyTorch\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset, random_split\n",
    "import torchmetrics\n",
    "from torch.nn import functional as F\n",
    "from torch.nn import CrossEntropyLoss\n",
    "\n",
    "# Optuna for hyperparameter optimization\n",
    "import optuna\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.metrics import log_loss\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import torch.nn.init as init"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare NN input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_sample(sample):\n",
    "    # Define the paths based on the sample name\n",
    "    qb_path = f'/data2/1000Genome/{sample}/qb/{sample}'\n",
    "    readcount_path = f'/data2/1000Genome/{sample}/beastie/runModel_phased_even100/chr1-22_alignBiasp0.05_s0.7_a0.05_sinCov0_totCov1_W1000K1000/tmp/{sample}'\n",
    "    \n",
    "    # Check if the 'beastie' folder exists (we already filtered for this, so this is optional now)\n",
    "    if not os.path.isdir(f'/data2/1000Genome/{sample}/beastie'):\n",
    "        return\n",
    "\n",
    "    # Read the qb file and extract qb_label\n",
    "    qb_data = pd.read_csv(f\"{qb_path}_qb_highestsite.tsv\", sep='\\t')\n",
    "    qb_data[\"qb_label\"] = qb_data[\"qb_mode\"]\n",
    "    qb_label = qb_data[[\"geneID\", \"qb_label\"]]\n",
    "\n",
    "    # Read the input readcount file\n",
    "    input_readcount = pd.read_csv(f\"{readcount_path}_real_alignBiasafterFilter.phasedByshapeit2.cleaned.tsv\", sep='\\t')\n",
    "    input_readcount = input_readcount[[\"geneID\", \"chrN\", \"pos\", \"refCount\", \"altCount\"]]\n",
    "\n",
    "    # Read the genetic features file\n",
    "    genetic_features = pd.read_csv(f\"{readcount_path}.meta.w_error.tsv\", sep='\\t')\n",
    "    genetic_features = genetic_features[[\"geneID\", \"pos\", \"MAF\", \"min_MAF\", \"diff_MAF\", \"d\", \"r2\", \"log10_distance\"]]\n",
    "\n",
    "    # Perform inner join of the genetic features and the input readcount\n",
    "    input_df = pd.merge(input_readcount, genetic_features, how=\"inner\", on=[\"geneID\", \"pos\"])\n",
    "    input_df[\"individual\"] = sample\n",
    "\n",
    "    # Read the ancestry file and get the first word\n",
    "    ancestry_file = f'/data2/1000Genome/{sample}/ancestry'\n",
    "    with open(ancestry_file, 'r') as file:\n",
    "        ancestry = file.readline().strip()\n",
    "\n",
    "    input_df[\"ancestry\"] = ancestry\n",
    "\n",
    "    # Format the geneID column\n",
    "    input_df[\"geneID\"] = input_df[\"geneID\"].str.split(\".\").str[0]\n",
    "\n",
    "    # Merge input_df with qb_label\n",
    "    input_df = pd.merge(input_df, qb_label, how=\"inner\", on=[\"geneID\"])\n",
    "\n",
    "    # Save the final DataFrame as a TSV file\n",
    "    output_file = f\"{qb_path}_NN_input.tsv\"\n",
    "    input_df.to_csv(output_file, sep='\\t', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Find only the samples with the 'beastie' directory\n",
    "# base_dir = '/data2/1000Genome/'\n",
    "# samples = [\n",
    "#     sample for sample in os.listdir(base_dir)\n",
    "#     if os.path.isdir(os.path.join(base_dir, sample)) and\n",
    "#     os.path.isdir(f'/data2/1000Genome/{sample}/beastie')\n",
    "# ]\n",
    "\n",
    "# # Process each sample with a progress bar\n",
    "# for sample in tqdm(samples, desc=\"Processing samples\"):\n",
    "#     process_sample(sample)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Randomly select some individuals for a training set and testing set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_and_split_samples(ancestry_group, num_individuals_train=10, num_individuals_test=10, base_dir='/data2/1000Genome/'):\n",
    "    # Collect output file paths for all individuals in the specified ancestry group\n",
    "    ancestry_files = []\n",
    "    for sample in os.listdir(base_dir):\n",
    "        sample_dir = os.path.join(base_dir, sample)\n",
    "        ancestry_file = os.path.join(sample_dir, 'ancestry')\n",
    "        output_file = os.path.join(sample_dir, 'qb', f\"{sample}_NN_input.tsv\")\n",
    "        \n",
    "        # Check if both ancestry file and output file exist\n",
    "        if os.path.isfile(ancestry_file) and os.path.isfile(output_file):\n",
    "            with open(ancestry_file, 'r') as file:\n",
    "                ancestry = file.readline().strip()\n",
    "            # Collect the file path if it matches the specified ancestry group\n",
    "            if ancestry == ancestry_group:\n",
    "                ancestry_files.append(output_file)\n",
    "    \n",
    "    # Ensure we have enough individuals to split into train and test sets\n",
    "    total_required = num_individuals_train + num_individuals_test\n",
    "    if len(ancestry_files) < total_required:\n",
    "        raise ValueError(f\"Not enough individuals in {ancestry_group} ancestry group. Required: {total_required}, Found: {len(ancestry_files)}\")\n",
    "    \n",
    "    # Randomly select the specified number of unique individuals for training and testing\n",
    "    selected_files = random.sample(ancestry_files, total_required)\n",
    "    train_files = selected_files[:num_individuals_train]\n",
    "    test_files = selected_files[num_individuals_train:num_individuals_train + num_individuals_test]\n",
    "    \n",
    "    # Combine data for the training set\n",
    "    train_df = pd.DataFrame()\n",
    "    for file_path in train_files:\n",
    "        df = pd.read_csv(file_path, sep='\\t')\n",
    "        train_df = pd.concat([train_df, df], ignore_index=True)\n",
    "    \n",
    "    # Combine data for the test set\n",
    "    test_df = pd.DataFrame()\n",
    "    for file_path in test_files:\n",
    "        df = pd.read_csv(file_path, sep='\\t')\n",
    "        test_df = pd.concat([test_df, df], ignore_index=True)\n",
    "    \n",
    "    return train_df, test_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example usage\n",
    "train_df, test_df = select_and_split_samples('GBR', num_individuals_train=40, num_individuals_test=40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>geneID</th>\n",
       "      <th>chrN</th>\n",
       "      <th>pos</th>\n",
       "      <th>refCount</th>\n",
       "      <th>altCount</th>\n",
       "      <th>MAF</th>\n",
       "      <th>min_MAF</th>\n",
       "      <th>diff_MAF</th>\n",
       "      <th>d</th>\n",
       "      <th>r2</th>\n",
       "      <th>log10_distance</th>\n",
       "      <th>individual</th>\n",
       "      <th>ancestry</th>\n",
       "      <th>qb_label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ENSG00000188976</td>\n",
       "      <td>1</td>\n",
       "      <td>881627</td>\n",
       "      <td>125</td>\n",
       "      <td>112</td>\n",
       "      <td>0.3668</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>HG00122</td>\n",
       "      <td>GBR</td>\n",
       "      <td>0.476214</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ENSG00000188976</td>\n",
       "      <td>1</td>\n",
       "      <td>882033</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0.2495</td>\n",
       "      <td>0.2495</td>\n",
       "      <td>0.1173</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.655</td>\n",
       "      <td>2.608526</td>\n",
       "      <td>HG00122</td>\n",
       "      <td>GBR</td>\n",
       "      <td>0.476214</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ENSG00000188976</td>\n",
       "      <td>1</td>\n",
       "      <td>882250</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0.0606</td>\n",
       "      <td>0.0606</td>\n",
       "      <td>0.1889</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.021</td>\n",
       "      <td>2.336460</td>\n",
       "      <td>HG00122</td>\n",
       "      <td>GBR</td>\n",
       "      <td>0.476214</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ENSG00000187961</td>\n",
       "      <td>1</td>\n",
       "      <td>900505</td>\n",
       "      <td>16</td>\n",
       "      <td>12</td>\n",
       "      <td>0.2903</td>\n",
       "      <td>0.0606</td>\n",
       "      <td>0.2297</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>HG00122</td>\n",
       "      <td>GBR</td>\n",
       "      <td>0.454083</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ENSG00000131591</td>\n",
       "      <td>1</td>\n",
       "      <td>1027070</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.1183</td>\n",
       "      <td>0.1183</td>\n",
       "      <td>0.1720</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>HG00122</td>\n",
       "      <td>GBR</td>\n",
       "      <td>0.469853</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            geneID  chrN      pos  refCount  altCount     MAF  min_MAF  \\\n",
       "0  ENSG00000188976     1   881627       125       112  0.3668      NaN   \n",
       "1  ENSG00000188976     1   882033         1         2  0.2495   0.2495   \n",
       "2  ENSG00000188976     1   882250         0         2  0.0606   0.0606   \n",
       "3  ENSG00000187961     1   900505        16        12  0.2903   0.0606   \n",
       "4  ENSG00000131591     1  1027070         1         0  0.1183   0.1183   \n",
       "\n",
       "   diff_MAF    d     r2  log10_distance individual ancestry  qb_label  \n",
       "0       NaN  NaN    NaN             NaN    HG00122      GBR  0.476214  \n",
       "1    0.1173  1.0  0.655        2.608526    HG00122      GBR  0.476214  \n",
       "2    0.1889  1.0  0.021        2.336460    HG00122      GBR  0.476214  \n",
       "3    0.2297  NaN    NaN             NaN    HG00122      GBR  0.454083  \n",
       "4    0.1720  NaN    NaN             NaN    HG00122      GBR  0.469853  "
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### modify the testset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_data_for_cnn_individual(data, feature_columns, label_col='qb_label'):\n",
    "    # Step 1: Determine the maximum number of hets per gene across individuals\n",
    "    max_hets_per_gene = data.groupby(['individual', 'geneID']).size().max()\n",
    "    \n",
    "    # Step 2: Calculate relative position for each gene by individual\n",
    "    # Sort by geneID and position for proper ordering\n",
    "    data = data.sort_values(by=['individual', 'geneID', 'pos'])\n",
    "    #data['relative_pos'] = data.groupby(['individual', 'geneID'])['pos'].diff().fillna(0)\n",
    "    data['log10_distance'] = data['log10_distance'].fillna(0)\n",
    "    # fill in d' values with 0.5 if nan\n",
    "    data['d'] = data['d'].fillna(0.5)\n",
    "    # fill in r2 values with 0.2-0.3 uniform distribution if nan\n",
    "    data['r2'] = data['r2'].fillna(np.random.uniform(0.2, 0.3))\n",
    "    # fill in NAn with 0 for min_MAF\n",
    "    data['min_MAF'] = data['min_MAF'].fillna(0)\n",
    "    # fill in NAn with 0 for diff_MAF\n",
    "    data['diff_MAF'] = data['diff_MAF'].fillna(0)\n",
    "    \n",
    "    # Step 3: Filter relevant columns - dropping geneID, chrN, and pos\n",
    "    X_data = []\n",
    "    y_data = []\n",
    "    \n",
    "    # Group by individual and geneID to form CNN inputs\n",
    "    grouped = data.groupby(['individual', 'geneID'])\n",
    "    \n",
    "    for (individual, geneID), group in grouped:\n",
    "        # Extract the feature columns for the current individual and gene\n",
    "        X = group[feature_columns].to_numpy()\n",
    "        \n",
    "        # Pad the feature array to have the same number of rows\n",
    "        padded_X = np.zeros((max_hets_per_gene, len(feature_columns)))\n",
    "        padded_X[:X.shape[0], :] = X  # Fill in the available data\n",
    "        \n",
    "        # Get the label (assumes it's the same across all rows in the group)\n",
    "        y = group[label_col].iloc[0]\n",
    "        \n",
    "        # Append to the lists\n",
    "        X_data.append(padded_X)\n",
    "        y_data.append(y)\n",
    "    \n",
    "    # Convert the lists to numpy arrays suitable for CNN input\n",
    "    X_data = np.array(X_data)\n",
    "    y_data = np.array(y_data)\n",
    "    \n",
    "    return X_data, y_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X shape: (279073, 62, 8)\n",
      "y shape: (279073,)\n"
     ]
    }
   ],
   "source": [
    "data_train = train_df\n",
    "feature_columns = ['refCount', 'altCount', 'MAF', 'min_MAF', 'diff_MAF', 'log10_distance', 'd', 'r2']\n",
    "X_train, y_train = prepare_data_for_cnn_individual(data_train, feature_columns)\n",
    "print(\"X shape:\", X_train.shape)\n",
    "print(\"y shape:\", y_train.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X test shape: (279877, 64, 8)\n",
      "y test shape: (279877,)\n"
     ]
    }
   ],
   "source": [
    "data_test = test_df\n",
    "X_test, y_test = prepare_data_for_cnn_individual(data_test, feature_columns)\n",
    "print(\"X test shape:\", X_test.shape)\n",
    "print(\"y test shape:\", y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>geneID</th>\n",
       "      <th>chrN</th>\n",
       "      <th>pos</th>\n",
       "      <th>refCount</th>\n",
       "      <th>altCount</th>\n",
       "      <th>MAF</th>\n",
       "      <th>min_MAF</th>\n",
       "      <th>diff_MAF</th>\n",
       "      <th>d</th>\n",
       "      <th>r2</th>\n",
       "      <th>log10_distance</th>\n",
       "      <th>individual</th>\n",
       "      <th>ancestry</th>\n",
       "      <th>qb_label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ENSG00000188976</td>\n",
       "      <td>1</td>\n",
       "      <td>881627</td>\n",
       "      <td>125</td>\n",
       "      <td>112</td>\n",
       "      <td>0.3668</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>HG00122</td>\n",
       "      <td>GBR</td>\n",
       "      <td>0.476214</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ENSG00000188976</td>\n",
       "      <td>1</td>\n",
       "      <td>882033</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0.2495</td>\n",
       "      <td>0.2495</td>\n",
       "      <td>0.1173</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.655</td>\n",
       "      <td>2.608526</td>\n",
       "      <td>HG00122</td>\n",
       "      <td>GBR</td>\n",
       "      <td>0.476214</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ENSG00000188976</td>\n",
       "      <td>1</td>\n",
       "      <td>882250</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0.0606</td>\n",
       "      <td>0.0606</td>\n",
       "      <td>0.1889</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.021</td>\n",
       "      <td>2.336460</td>\n",
       "      <td>HG00122</td>\n",
       "      <td>GBR</td>\n",
       "      <td>0.476214</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ENSG00000187961</td>\n",
       "      <td>1</td>\n",
       "      <td>900505</td>\n",
       "      <td>16</td>\n",
       "      <td>12</td>\n",
       "      <td>0.2903</td>\n",
       "      <td>0.0606</td>\n",
       "      <td>0.2297</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>HG00122</td>\n",
       "      <td>GBR</td>\n",
       "      <td>0.454083</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ENSG00000131591</td>\n",
       "      <td>1</td>\n",
       "      <td>1027070</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.1183</td>\n",
       "      <td>0.1183</td>\n",
       "      <td>0.1720</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>HG00122</td>\n",
       "      <td>GBR</td>\n",
       "      <td>0.469853</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            geneID  chrN      pos  refCount  altCount     MAF  min_MAF  \\\n",
       "0  ENSG00000188976     1   881627       125       112  0.3668      NaN   \n",
       "1  ENSG00000188976     1   882033         1         2  0.2495   0.2495   \n",
       "2  ENSG00000188976     1   882250         0         2  0.0606   0.0606   \n",
       "3  ENSG00000187961     1   900505        16        12  0.2903   0.0606   \n",
       "4  ENSG00000131591     1  1027070         1         0  0.1183   0.1183   \n",
       "\n",
       "   diff_MAF    d     r2  log10_distance individual ancestry  qb_label  \n",
       "0       NaN  NaN    NaN             NaN    HG00122      GBR  0.476214  \n",
       "1    0.1173  1.0  0.655        2.608526    HG00122      GBR  0.476214  \n",
       "2    0.1889  1.0  0.021        2.336460    HG00122      GBR  0.476214  \n",
       "3    0.2297  NaN    NaN             NaN    HG00122      GBR  0.454083  \n",
       "4    0.1720  NaN    NaN             NaN    HG00122      GBR  0.469853  "
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GeneCNN(nn.Module):\n",
    "    def __init__(self, input_features):\n",
    "        super(GeneCNN, self).__init__()\n",
    "        \n",
    "        # Convolutional and pooling layers\n",
    "        ## the number of filters in the 1st conv layer is 32 (common choice 16-64)\n",
    "        self.conv1 = nn.Conv1d(in_channels = input_features, out_channels=32, kernel_size=2)\n",
    "        self.pool1 = nn.MaxPool1d(kernel_size=2)\n",
    "        \n",
    "        ## the number of filters in the 2nd conv layer is 32 (common choice 16-64)\n",
    "        self.conv2 = nn.Conv1d(in_channels=32, out_channels=64, kernel_size=2)\n",
    "        self.pool2 = nn.MaxPool1d(kernel_size=2)\n",
    "        \n",
    "        ## the number of filters in the 3rd conv layer is 64 (common choice 16-64)\n",
    "        self.conv3 = nn.Conv1d(in_channels=64, out_channels=128, kernel_size=2)\n",
    "        \n",
    "        # Adaptive pooling layer to reduce size to 1\n",
    "        self.global_pool = nn.AdaptiveAvgPool1d(1)\n",
    "        \n",
    "        # Fully connected layers\n",
    "        self.fc1 = nn.Linear(128, 64)  # Adjusted input size to match the output channels from global_pool\n",
    "        self.fc2 = nn.Linear(64, 8)\n",
    "        self.fc3 = nn.Linear(8, 1)  # Single output neuron for regression\n",
    "        \n",
    "        # Dropout layer for regularization\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "\n",
    "        # # Add two batch normalization layers\n",
    "        # self.bn1 = nn.BatchNorm1d(128)\n",
    "        # self.bn2 = nn.BatchNorm1d(8)\n",
    "\n",
    "        # # Apply He initialization\n",
    "        # init.kaiming_uniform_(self.fc1.weight)\n",
    "        # init.kaiming_uniform_(self.fc2.weight)\n",
    "        # init.kaiming_uniform_(self.fc3.weight, nonlinearity=\"sigmoid\")\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Transpose to match Conv1d input format: (batch_size, features, length)\n",
    "        x = x.transpose(1, 2)\n",
    "        \n",
    "        # Convolution and pooling layers\n",
    "        x = self.pool1(F.relu(self.conv1(x)))  # Conv1 and pooling\n",
    "        #print(f\"Shape after conv1 and pool1: {x.shape}\")\n",
    "        \n",
    "        x = self.pool2(F.relu(self.conv2(x)))  # Conv2 and pooling\n",
    "        #print(f\"Shape after conv2 and pool2: {x.shape}\")\n",
    "        \n",
    "        x = F.relu(self.conv3(x))              # Conv3\n",
    "        #print(f\"Shape after conv3: {x.shape}\")\n",
    "        \n",
    "        x = self.global_pool(x)                # Adaptive pooling to size 1\n",
    "        #print(f\"Shape after global_pool: {x.shape}\")\n",
    "        \n",
    "        # Flatten and pass through fully connected layers\n",
    "        x = x.view(x.size(0), -1)              # Flatten\n",
    "        #print(f\"Shape after flattening: {x.shape}\")\n",
    "        \n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.dropout(x)\n",
    "        x = F.relu(self.fc2(x))\n",
    "        \n",
    "        # Output layer for regression (no activation or add sigmoid if bounded)\n",
    "        x = self.fc3(x)\n",
    "        \n",
    "        return x\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GeneCNN(\n",
      "  (conv1): Conv1d(8, 32, kernel_size=(2,), stride=(1,))\n",
      "  (pool1): MaxPool1d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (conv2): Conv1d(32, 64, kernel_size=(2,), stride=(1,))\n",
      "  (pool2): MaxPool1d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (conv3): Conv1d(64, 128, kernel_size=(2,), stride=(1,))\n",
      "  (global_pool): AdaptiveAvgPool1d(output_size=1)\n",
      "  (fc1): Linear(in_features=128, out_features=64, bias=True)\n",
      "  (fc2): Linear(in_features=64, out_features=8, bias=True)\n",
      "  (fc3): Linear(in_features=8, out_features=1, bias=True)\n",
      "  (dropout): Dropout(p=0.5, inplace=False)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# Instantiate the model, loss function, and optimizer\n",
    "max_hets_per_gene = X_train.shape[2]  # Use the maximum number of hets per gene\n",
    "model = GeneCNN(input_features = max_hets_per_gene) # 6 features\n",
    "\n",
    "# Model summary\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming X_train and y_train are initially your full training data\n",
    "# Set aside a portion for validation\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.25, random_state=42, shuffle=True)\n",
    "\n",
    "# Convert them to PyTorch tensors\n",
    "X_train = torch.tensor(X_train, dtype=torch.float32)\n",
    "y_train = torch.tensor(y_train, dtype=torch.float32).unsqueeze(1)\n",
    "X_val = torch.tensor(X_val, dtype=torch.float32)\n",
    "y_val = torch.tensor(y_val, dtype=torch.float32).unsqueeze(1)\n",
    "\n",
    "X_test = torch.tensor(X_test, dtype=torch.float32)\n",
    "y_test = torch.tensor(y_test, dtype=torch.float32).unsqueeze(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a dataset and a DataLoader with a smaller batch size\n",
    "train_dataset = TensorDataset(X_train, y_train)\n",
    "val_dataset = TensorDataset(X_val, y_val)\n",
    "train_loader = DataLoader(train_dataset, batch_size=128, shuffle=True)  # Adjust batch size as needed\n",
    "val_loader = DataLoader(val_dataset, batch_size=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NaNs in X_train: tensor(False)\n",
      "NaNs in y_train: tensor(False)\n",
      "Infs in X_train: tensor(False)\n",
      "Infs in y_train: tensor(False)\n",
      "NaNs in X_val: tensor(False)\n",
      "NaNs in y_val: tensor(False)\n",
      "Infs in X_val: tensor(False)\n",
      "Infs in y_val: tensor(False)\n"
     ]
    }
   ],
   "source": [
    "# Check for NaNs or Infs in input and target data\n",
    "print(\"NaNs in X_train:\", torch.isnan(X_train).any())\n",
    "print(\"NaNs in y_train:\", torch.isnan(y_train).any())\n",
    "print(\"Infs in X_train:\", torch.isinf(X_train).any())\n",
    "print(\"Infs in y_train:\", torch.isinf(y_train).any())\n",
    "\n",
    "print(\"NaNs in X_val:\", torch.isnan(X_val).any())\n",
    "print(\"NaNs in y_val:\", torch.isnan(y_val).any())\n",
    "print(\"Infs in X_val:\", torch.isinf(X_val).any())\n",
    "print(\"Infs in y_val:\", torch.isinf(y_val).any())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/100], Train RMSE: 0.1115, Validation RMSE: 0.1025\n",
      "Epoch [2/100], Train RMSE: 0.0528, Validation RMSE: 0.0838\n",
      "Epoch [3/100], Train RMSE: 0.0483, Validation RMSE: 0.0863\n",
      "Epoch [4/100], Train RMSE: 0.0459, Validation RMSE: 0.0871\n",
      "Epoch [5/100], Train RMSE: 0.0453, Validation RMSE: 0.0727\n",
      "Epoch [6/100], Train RMSE: 0.0450, Validation RMSE: 0.0741\n",
      "Epoch [7/100], Train RMSE: 0.0434, Validation RMSE: 0.0730\n",
      "Epoch [8/100], Train RMSE: 0.0436, Validation RMSE: 0.0640\n",
      "Epoch [9/100], Train RMSE: 0.0435, Validation RMSE: 0.0659\n",
      "Epoch [10/100], Train RMSE: 0.0424, Validation RMSE: 0.0574\n",
      "Epoch [11/100], Train RMSE: 0.0424, Validation RMSE: 0.0650\n",
      "Epoch [12/100], Train RMSE: 0.0425, Validation RMSE: 0.0544\n",
      "Epoch [13/100], Train RMSE: 0.0418, Validation RMSE: 0.0570\n",
      "Epoch [14/100], Train RMSE: 0.0415, Validation RMSE: 0.0539\n",
      "Epoch [15/100], Train RMSE: 0.0413, Validation RMSE: 0.0587\n",
      "Epoch [16/100], Train RMSE: 0.0412, Validation RMSE: 0.0561\n",
      "Epoch [17/100], Train RMSE: 0.0409, Validation RMSE: 0.0583\n",
      "Epoch [18/100], Train RMSE: 0.0411, Validation RMSE: 0.0600\n",
      "Epoch [19/100], Train RMSE: 0.0400, Validation RMSE: 0.0560\n",
      "Epoch [20/100], Train RMSE: 0.0398, Validation RMSE: 0.0604\n",
      "Epoch [21/100], Train RMSE: 0.0398, Validation RMSE: 0.0575\n",
      "Epoch [22/100], Train RMSE: 0.0396, Validation RMSE: 0.0607\n",
      "Epoch [23/100], Train RMSE: 0.0393, Validation RMSE: 0.0589\n",
      "Epoch [24/100], Train RMSE: 0.0391, Validation RMSE: 0.0577\n",
      "Epoch [25/100], Train RMSE: 0.0390, Validation RMSE: 0.0605\n",
      "Epoch [26/100], Train RMSE: 0.0390, Validation RMSE: 0.0608\n",
      "Epoch [27/100], Train RMSE: 0.0388, Validation RMSE: 0.0606\n",
      "Epoch [28/100], Train RMSE: 0.0387, Validation RMSE: 0.0607\n",
      "Epoch [29/100], Train RMSE: 0.0387, Validation RMSE: 0.0597\n",
      "Epoch [30/100], Train RMSE: 0.0386, Validation RMSE: 0.0590\n",
      "Epoch [31/100], Train RMSE: 0.0385, Validation RMSE: 0.0597\n",
      "Epoch [32/100], Train RMSE: 0.0385, Validation RMSE: 0.0599\n",
      "Epoch [33/100], Train RMSE: 0.0384, Validation RMSE: 0.0601\n",
      "Epoch [34/100], Train RMSE: 0.0385, Validation RMSE: 0.0610\n",
      "Epoch [35/100], Train RMSE: 0.0384, Validation RMSE: 0.0609\n",
      "Epoch [36/100], Train RMSE: 0.0383, Validation RMSE: 0.0613\n",
      "Epoch [37/100], Train RMSE: 0.0383, Validation RMSE: 0.0605\n",
      "Epoch [38/100], Train RMSE: 0.0383, Validation RMSE: 0.0611\n",
      "Epoch [39/100], Train RMSE: 0.0383, Validation RMSE: 0.0604\n",
      "Epoch [40/100], Train RMSE: 0.0383, Validation RMSE: 0.0603\n",
      "Epoch [41/100], Train RMSE: 0.0383, Validation RMSE: 0.0603\n",
      "Epoch [42/100], Train RMSE: 0.0383, Validation RMSE: 0.0605\n",
      "Epoch [43/100], Train RMSE: 0.0382, Validation RMSE: 0.0608\n",
      "Epoch [44/100], Train RMSE: 0.0383, Validation RMSE: 0.0604\n",
      "Epoch [45/100], Train RMSE: 0.0382, Validation RMSE: 0.0606\n",
      "Epoch [46/100], Train RMSE: 0.0382, Validation RMSE: 0.0606\n",
      "Epoch [47/100], Train RMSE: 0.0383, Validation RMSE: 0.0610\n",
      "Epoch [48/100], Train RMSE: 0.0382, Validation RMSE: 0.0608\n",
      "Epoch [49/100], Train RMSE: 0.0383, Validation RMSE: 0.0605\n",
      "Epoch [50/100], Train RMSE: 0.0382, Validation RMSE: 0.0608\n",
      "Epoch [51/100], Train RMSE: 0.0382, Validation RMSE: 0.0607\n",
      "Epoch [52/100], Train RMSE: 0.0383, Validation RMSE: 0.0607\n",
      "Epoch [53/100], Train RMSE: 0.0382, Validation RMSE: 0.0606\n",
      "Epoch [54/100], Train RMSE: 0.0382, Validation RMSE: 0.0606\n",
      "Epoch [55/100], Train RMSE: 0.0382, Validation RMSE: 0.0607\n",
      "Epoch [56/100], Train RMSE: 0.0382, Validation RMSE: 0.0606\n",
      "Epoch [57/100], Train RMSE: 0.0382, Validation RMSE: 0.0607\n",
      "Epoch [58/100], Train RMSE: 0.0383, Validation RMSE: 0.0606\n",
      "Epoch [59/100], Train RMSE: 0.0382, Validation RMSE: 0.0607\n",
      "Epoch [60/100], Train RMSE: 0.0382, Validation RMSE: 0.0607\n",
      "Epoch [61/100], Train RMSE: 0.0382, Validation RMSE: 0.0607\n",
      "Epoch [62/100], Train RMSE: 0.0381, Validation RMSE: 0.0607\n",
      "Epoch [63/100], Train RMSE: 0.0382, Validation RMSE: 0.0607\n",
      "Epoch [64/100], Train RMSE: 0.0382, Validation RMSE: 0.0607\n",
      "Epoch [65/100], Train RMSE: 0.0382, Validation RMSE: 0.0607\n",
      "Epoch [66/100], Train RMSE: 0.0382, Validation RMSE: 0.0607\n",
      "Epoch [67/100], Train RMSE: 0.0382, Validation RMSE: 0.0607\n",
      "Epoch [68/100], Train RMSE: 0.0381, Validation RMSE: 0.0607\n",
      "Epoch [69/100], Train RMSE: 0.0382, Validation RMSE: 0.0607\n",
      "Epoch [70/100], Train RMSE: 0.0382, Validation RMSE: 0.0607\n",
      "Epoch [71/100], Train RMSE: 0.0383, Validation RMSE: 0.0607\n",
      "Epoch [72/100], Train RMSE: 0.0382, Validation RMSE: 0.0607\n",
      "Epoch [73/100], Train RMSE: 0.0383, Validation RMSE: 0.0607\n",
      "Epoch [74/100], Train RMSE: 0.0382, Validation RMSE: 0.0607\n",
      "Epoch [75/100], Train RMSE: 0.0382, Validation RMSE: 0.0607\n",
      "Epoch [76/100], Train RMSE: 0.0382, Validation RMSE: 0.0607\n",
      "Epoch [77/100], Train RMSE: 0.0381, Validation RMSE: 0.0607\n",
      "Epoch [78/100], Train RMSE: 0.0382, Validation RMSE: 0.0607\n",
      "Epoch [79/100], Train RMSE: 0.0382, Validation RMSE: 0.0607\n",
      "Epoch [80/100], Train RMSE: 0.0382, Validation RMSE: 0.0607\n",
      "Epoch [81/100], Train RMSE: 0.0382, Validation RMSE: 0.0607\n",
      "Epoch [82/100], Train RMSE: 0.0383, Validation RMSE: 0.0607\n",
      "Epoch [83/100], Train RMSE: 0.0382, Validation RMSE: 0.0607\n",
      "Epoch [84/100], Train RMSE: 0.0382, Validation RMSE: 0.0607\n",
      "Epoch [85/100], Train RMSE: 0.0382, Validation RMSE: 0.0607\n",
      "Epoch [86/100], Train RMSE: 0.0382, Validation RMSE: 0.0607\n",
      "Epoch [87/100], Train RMSE: 0.0382, Validation RMSE: 0.0607\n",
      "Epoch [88/100], Train RMSE: 0.0381, Validation RMSE: 0.0607\n",
      "Epoch [89/100], Train RMSE: 0.0382, Validation RMSE: 0.0607\n",
      "Epoch [90/100], Train RMSE: 0.0382, Validation RMSE: 0.0607\n",
      "Epoch [91/100], Train RMSE: 0.0381, Validation RMSE: 0.0607\n",
      "Epoch [92/100], Train RMSE: 0.0381, Validation RMSE: 0.0607\n",
      "Epoch [93/100], Train RMSE: 0.0382, Validation RMSE: 0.0607\n",
      "Epoch [94/100], Train RMSE: 0.0382, Validation RMSE: 0.0607\n",
      "Epoch [95/100], Train RMSE: 0.0382, Validation RMSE: 0.0607\n",
      "Epoch [96/100], Train RMSE: 0.0383, Validation RMSE: 0.0607\n",
      "Epoch [97/100], Train RMSE: 0.0382, Validation RMSE: 0.0607\n",
      "Epoch [98/100], Train RMSE: 0.0382, Validation RMSE: 0.0607\n",
      "Epoch [99/100], Train RMSE: 0.0383, Validation RMSE: 0.0607\n",
      "Epoch [100/100], Train RMSE: 0.0382, Validation RMSE: 0.0607\n"
     ]
    }
   ],
   "source": [
    "# Check if GPU is available\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Initialize the model\n",
    "model = GeneCNN(input_features=max_hets_per_gene).to(device)  # Change input_features to the correct feature count\n",
    "\n",
    "# Define loss function, optimizer, and optional scheduler\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.0005)\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', patience=3, factor=0.5, verbose=True)\n",
    "\n",
    "# Lists to store the training and validation losses\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "\n",
    "# Training loop\n",
    "epochs = 100\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    train_loss_epoch = 0.0\n",
    "    \n",
    "    for X_batch, y_batch in train_loader:\n",
    "        X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Forward pass\n",
    "        outputs = model(X_batch)\n",
    "        train_mse_loss = criterion(outputs, y_batch)  # MSE Loss\n",
    "        train_rmse_loss = torch.sqrt(train_mse_loss)  # RMSE\n",
    "        \n",
    "        # Backward pass\n",
    "        train_mse_loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "        optimizer.step()\n",
    "        \n",
    "        train_loss_epoch += train_mse_loss.item()  # Accumulate MSE loss\n",
    "    \n",
    "    # Validation\n",
    "    model.eval()\n",
    "    val_loss_epoch = 0.0\n",
    "    with torch.no_grad():\n",
    "        for X_val_batch, y_val_batch in val_loader:\n",
    "            X_val_batch, y_val_batch = X_val_batch.to(device), y_val_batch.to(device)\n",
    "            val_outputs = model(X_val_batch)\n",
    "            val_mse_loss = criterion(val_outputs, y_val_batch)\n",
    "            val_loss_epoch += val_mse_loss.item()  # Accumulate MSE loss\n",
    "\n",
    "    # Calculate average RMSE for the epoch\n",
    "    avg_train_rmse = torch.sqrt(torch.tensor(train_loss_epoch / len(train_loader))).item()\n",
    "    avg_val_rmse = torch.sqrt(torch.tensor(val_loss_epoch / len(val_loader))).item()\n",
    "\n",
    "    # Store the RMSE losses for each epoch\n",
    "    train_losses.append(avg_train_rmse)\n",
    "    val_losses.append(avg_val_rmse)\n",
    "    \n",
    "    # Step the scheduler based on validation loss\n",
    "    scheduler.step(avg_val_rmse)\n",
    "    \n",
    "    print(f\"Epoch [{epoch+1}/{epochs}], Train RMSE: {avg_train_rmse:.4f}, Validation RMSE: {avg_val_rmse:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### RELU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x500 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(range(1, epochs + 1), train_losses, label='Training RMSE')\n",
    "plt.plot(range(1, epochs + 1), val_losses, label='Validation RMSE')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('RMSE')\n",
    "plt.title('Training and Validation RMSE')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final Test RMSE: 0.0597\n"
     ]
    }
   ],
   "source": [
    "# Create a DataLoader for the test set with a smaller batch size\n",
    "test_dataset = TensorDataset(X_test, y_test)\n",
    "test_loader = DataLoader(test_dataset, batch_size=64)  # Adjust batch size as needed\n",
    "\n",
    "# Final evaluation on test set\n",
    "model.eval()\n",
    "test_loss_total = 0.0\n",
    "with torch.no_grad():\n",
    "    for X_batch, y_batch in test_loader:\n",
    "        X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
    "        test_outputs = model(X_batch)\n",
    "        test_loss = criterion(test_outputs, y_batch)  # MSE Loss\n",
    "        test_loss_total += test_loss.item() * X_batch.size(0)  # Sum up the batch loss (weighted by batch size)\n",
    "\n",
    "# Calculate the average loss and take the square root for RMSE\n",
    "average_test_mse = test_loss_total / len(test_loader.dataset)\n",
    "average_test_rmse = torch.sqrt(torch.tensor(average_test_mse)).item()\n",
    "print(f\"Final Test RMSE: {average_test_rmse:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simulator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "def simulate_reads_single_gene(p=0.5, read_coverage=100, phasing_error=0.04, num_hets=10, max_hets=64, data=None):\n",
    "    np.random.seed(42)  # For reproducibility\n",
    "\n",
    "    # Initialize lists to store simulated data\n",
    "    ref_counts = []\n",
    "    alt_counts = []\n",
    "    switches = []\n",
    "\n",
    "    # Sample MAF and log10_distance for each het site\n",
    "    if data is None:\n",
    "        raise ValueError(\"MAF and log10_distance data must be provided.\")\n",
    "    \n",
    "\n",
    "    # Sample MAF and log10_distance for each het site in the gene\n",
    "    samples = data.sample(n=num_hets, replace=True)\n",
    "    # fill in missing data\n",
    "    samples['log10_distance'] = samples['log10_distance'].fillna(0)\n",
    "    # fill in d' values with 0.5 if nan\n",
    "    samples['d'] = samples['d'].fillna(0.5)\n",
    "    # fill in r2 values with 0.2-0.3 uniform distribution if nan\n",
    "    samples['r2'] = samples['r2'].fillna(np.random.uniform(0.2, 0.3))\n",
    "    # fill in NAn with 0 for min_MAF\n",
    "    samples['min_MAF'] = samples['min_MAF'].fillna(0)\n",
    "    # fill in NAn with 0 for diff_MAF\n",
    "    samples['diff_MAF'] = samples['diff_MAF'].fillna(0)\n",
    "\n",
    "    maf_values = samples['MAF'].values\n",
    "    min_maf_values = samples['min_MAF'].values\n",
    "    diff_maf_values = samples['diff_MAF'].values\n",
    "    log10_distance_values = samples['log10_distance'].values\n",
    "    d_values = samples['d'].values\n",
    "    r2_values = samples['r2'].values\n",
    "\n",
    "    # Convert log10_distance to distance and handle NaN by replacing with mean distance\n",
    "    #distance_values = np.where(~np.isnan(log10_distance_values), 10**log10_distance_values, np.nan)\n",
    "    #mean_distance = np.nanmean(distance_values)\n",
    "    #distance_values = np.where(np.isnan(distance_values), mean_distance, distance_values)\n",
    "\n",
    "    current_phase = False  # Start without phase flipping\n",
    "\n",
    "    # Loop over each heterozygous site\n",
    "    for het in range(num_hets):\n",
    "        # Simulate binomial read counts based on the binomial probability\n",
    "        ref_count = np.random.binomial(n=read_coverage, p=p)  # Total ref count for this het\n",
    "        alt_count = read_coverage - ref_count  # Total alt count for this het\n",
    "\n",
    "        # Generate a random phasing error to determine if a phase switch occurs\n",
    "        phase_switch = np.random.rand() < phasing_error\n",
    "        \n",
    "        # If a phasing error occurred, toggle the current phase\n",
    "        if phase_switch:\n",
    "            current_phase = not current_phase\n",
    "\n",
    "        # If current phase is switched, swap the ref and alt counts\n",
    "        if current_phase:\n",
    "            ref_count, alt_count = alt_count, ref_count\n",
    "            switches.append(1)  # Indicate a switch\n",
    "        else:\n",
    "            switches.append(0)  # No switch\n",
    "\n",
    "        # Store accumulated values for this het site\n",
    "        ref_counts.append(ref_count)\n",
    "        alt_counts.append(alt_count)\n",
    "        # mafs.append(maf_values[het])\n",
    "\n",
    "    # feature_columns = ['refCount', 'altCount', 'MAF', 'min_MAF', 'diff_MAF', 'log10_distance', 'd', 'r2']\n",
    "    # Create DataFrame for model input with the four features\n",
    "    data = {\n",
    "        'refCount': ref_counts,\n",
    "        'altCount': alt_counts,\n",
    "        'MAF': maf_values,\n",
    "        'min_MAF': min_maf_values,\n",
    "        'diff_MAF': diff_maf_values,\n",
    "        'log10_distance': log10_distance_values,\n",
    "        'd': d_values,\n",
    "        'r2': r2_values,\n",
    "        'switch': switches\n",
    "    }\n",
    "    df_input = pd.DataFrame(data)\n",
    "    \n",
    "    # Pad the DataFrame to have max_hets rows\n",
    "    padded_data = np.zeros((max_hets, len(df_input.columns) - 1))  # Exclude the 'switch' column from padding\n",
    "    padded_data[:df_input.shape[0], :] = df_input.drop(columns=['switch']).values  # Fill in available data and zero-pad remaining rows\n",
    "    \n",
    "    # Convert padded data to numpy arrays similar to `prepare_data_for_cnn_individual` output\n",
    "    X_data = np.array([padded_data])  # Add batch dimension to match the expected CNN input format\n",
    "    y_data = np.array([p])  # Here, the `p` is used as a placeholder for the label\n",
    "    \n",
    "    return X_data, y_data, df_input  # Return DataFrame for visualization as well\n",
    "\n",
    "\n",
    "\n",
    "def evaluate_simulation(X_sim, y_sim, model, device='cpu'):\n",
    "    \"\"\"\n",
    "    Runs the model on simulated data, calculates the predicted binomial p values, \n",
    "    and returns the RMSE compared to the true p values.\n",
    "\n",
    "    Parameters:\n",
    "    - X_sim: np.array, simulated input features\n",
    "    - y_sim: np.array, true binomial p values\n",
    "    - model: PyTorch model, the trained model to use for predictions\n",
    "    - device: str, device to run the model on ('cpu' or 'cuda')\n",
    "\n",
    "    Returns:\n",
    "    - predicted_p_values: np.array, the predicted binomial p values\n",
    "    - rmse: float, root mean squared error between the predicted and true p values\n",
    "    \"\"\"\n",
    "\n",
    "    # Convert X_sim and y_sim to PyTorch tensors\n",
    "    X_sim_tensor = torch.tensor(X_sim, dtype=torch.float32).to(device)\n",
    "    y_sim_tensor = torch.tensor(y_sim, dtype=torch.float32).unsqueeze(1).to(device)\n",
    "\n",
    "    # Set the model to evaluation mode\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        # Run the model to get the predicted binomial p values\n",
    "        predicted_p = model(X_sim_tensor)\n",
    "        \n",
    "        # Print the predicted binomial p values for inspection\n",
    "        print(\"Predicted binomial p values:\")\n",
    "        print(predicted_p.cpu().numpy())\n",
    "        \n",
    "        # Calculate the Mean Squared Error (MSE)\n",
    "        mse_loss = nn.MSELoss()(predicted_p, y_sim_tensor)\n",
    "        \n",
    "        # Calculate the RMSE\n",
    "        rmse = torch.sqrt(mse_loss).item()\n",
    "        print(f\"Root Mean Squared Error (RMSE): {rmse:.4f}\")\n",
    "    \n",
    "    # Return the predicted p values and RMSE\n",
    "    return predicted_p.cpu().numpy(), rmse\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted binomial p values:\n",
      "[[0.47522193]]\n",
      "Root Mean Squared Error (RMSE): 0.0248\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>refCount</th>\n",
       "      <th>altCount</th>\n",
       "      <th>MAF</th>\n",
       "      <th>min_MAF</th>\n",
       "      <th>diff_MAF</th>\n",
       "      <th>log10_distance</th>\n",
       "      <th>d</th>\n",
       "      <th>r2</th>\n",
       "      <th>switch</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>7</td>\n",
       "      <td>13</td>\n",
       "      <td>0.4553</td>\n",
       "      <td>0.3231</td>\n",
       "      <td>0.1322</td>\n",
       "      <td>4.156943</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.547000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>11</td>\n",
       "      <td>9</td>\n",
       "      <td>0.3519</td>\n",
       "      <td>0.3072</td>\n",
       "      <td>0.0447</td>\n",
       "      <td>3.202761</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.928000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5</td>\n",
       "      <td>15</td>\n",
       "      <td>0.0875</td>\n",
       "      <td>0.0875</td>\n",
       "      <td>0.0517</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.500</td>\n",
       "      <td>0.215599</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>12</td>\n",
       "      <td>8</td>\n",
       "      <td>0.0656</td>\n",
       "      <td>0.0656</td>\n",
       "      <td>0.0169</td>\n",
       "      <td>2.495544</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.756000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>8</td>\n",
       "      <td>12</td>\n",
       "      <td>0.3718</td>\n",
       "      <td>0.3718</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>3.145818</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>9</td>\n",
       "      <td>11</td>\n",
       "      <td>0.4414</td>\n",
       "      <td>0.4414</td>\n",
       "      <td>0.0019</td>\n",
       "      <td>2.350248</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.977000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>0.2922</td>\n",
       "      <td>0.2922</td>\n",
       "      <td>0.1482</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.500</td>\n",
       "      <td>0.215599</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>11</td>\n",
       "      <td>9</td>\n",
       "      <td>0.0209</td>\n",
       "      <td>0.0149</td>\n",
       "      <td>0.0060</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.500</td>\n",
       "      <td>0.215599</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>9</td>\n",
       "      <td>11</td>\n",
       "      <td>0.4851</td>\n",
       "      <td>0.2972</td>\n",
       "      <td>0.1879</td>\n",
       "      <td>2.742725</td>\n",
       "      <td>0.962</td>\n",
       "      <td>0.414000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>0.2952</td>\n",
       "      <td>0.2753</td>\n",
       "      <td>0.0199</td>\n",
       "      <td>2.190332</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.920000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   refCount  altCount     MAF  min_MAF  diff_MAF  log10_distance      d  \\\n",
       "0         7        13  0.4553   0.3231    0.1322        4.156943  1.000   \n",
       "1        11         9  0.3519   0.3072    0.0447        3.202761  1.000   \n",
       "2         5        15  0.0875   0.0875    0.0517        0.000000  0.500   \n",
       "3        12         8  0.0656   0.0656    0.0169        2.495544  1.000   \n",
       "4         8        12  0.3718   0.3718    0.0000        3.145818  1.000   \n",
       "5         9        11  0.4414   0.4414    0.0019        2.350248  1.000   \n",
       "6        10        10  0.2922   0.2922    0.1482        0.000000  0.500   \n",
       "7        11         9  0.0209   0.0149    0.0060        0.000000  0.500   \n",
       "8         9        11  0.4851   0.2972    0.1879        2.742725  0.962   \n",
       "9        10        10  0.2952   0.2753    0.0199        2.190332  1.000   \n",
       "\n",
       "         r2  switch  \n",
       "0  0.547000       0  \n",
       "1  0.928000       0  \n",
       "2  0.215599       0  \n",
       "3  0.756000       0  \n",
       "4  1.000000       0  \n",
       "5  0.977000       0  \n",
       "6  0.215599       0  \n",
       "7  0.215599       0  \n",
       "8  0.414000       0  \n",
       "9  0.920000       0  "
      ]
     },
     "execution_count": 151,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Simulate data using the function\n",
    "X_sim, y_sim, df_vis = simulate_reads_single_gene(\n",
    "    p=0.5, read_coverage=20, phasing_error=0.04, num_hets=10, max_hets=64, data=train_df\n",
    ")\n",
    "\n",
    "# Call the evaluation function\n",
    "predicted_p_values, rmse = evaluate_simulation(X_sim, y_sim, model, device)\n",
    "\n",
    "# Visualize the simulated data if needed\n",
    "df_vis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted binomial p values:\n",
      "[[0.45800358]]\n",
      "Root Mean Squared Error (RMSE): 0.0420\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>refCount</th>\n",
       "      <th>altCount</th>\n",
       "      <th>MAF</th>\n",
       "      <th>min_MAF</th>\n",
       "      <th>diff_MAF</th>\n",
       "      <th>log10_distance</th>\n",
       "      <th>d</th>\n",
       "      <th>r2</th>\n",
       "      <th>switch</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>44</td>\n",
       "      <td>56</td>\n",
       "      <td>0.4553</td>\n",
       "      <td>0.3231</td>\n",
       "      <td>0.1322</td>\n",
       "      <td>4.156943</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.547000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>52</td>\n",
       "      <td>48</td>\n",
       "      <td>0.3519</td>\n",
       "      <td>0.3072</td>\n",
       "      <td>0.0447</td>\n",
       "      <td>3.202761</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.928000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>58</td>\n",
       "      <td>42</td>\n",
       "      <td>0.0875</td>\n",
       "      <td>0.0875</td>\n",
       "      <td>0.0517</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.500</td>\n",
       "      <td>0.215599</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>51</td>\n",
       "      <td>49</td>\n",
       "      <td>0.0656</td>\n",
       "      <td>0.0656</td>\n",
       "      <td>0.0169</td>\n",
       "      <td>2.495544</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.756000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>56</td>\n",
       "      <td>44</td>\n",
       "      <td>0.3718</td>\n",
       "      <td>0.3718</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>3.145818</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>50</td>\n",
       "      <td>50</td>\n",
       "      <td>0.4414</td>\n",
       "      <td>0.4414</td>\n",
       "      <td>0.0019</td>\n",
       "      <td>2.350248</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.977000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>49</td>\n",
       "      <td>51</td>\n",
       "      <td>0.2922</td>\n",
       "      <td>0.2922</td>\n",
       "      <td>0.1482</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.500</td>\n",
       "      <td>0.215599</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>46</td>\n",
       "      <td>54</td>\n",
       "      <td>0.0209</td>\n",
       "      <td>0.0149</td>\n",
       "      <td>0.0060</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.500</td>\n",
       "      <td>0.215599</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>43</td>\n",
       "      <td>57</td>\n",
       "      <td>0.4851</td>\n",
       "      <td>0.2972</td>\n",
       "      <td>0.1879</td>\n",
       "      <td>2.742725</td>\n",
       "      <td>0.962</td>\n",
       "      <td>0.414000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>57</td>\n",
       "      <td>43</td>\n",
       "      <td>0.2952</td>\n",
       "      <td>0.2753</td>\n",
       "      <td>0.0199</td>\n",
       "      <td>2.190332</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.920000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   refCount  altCount     MAF  min_MAF  diff_MAF  log10_distance      d  \\\n",
       "0        44        56  0.4553   0.3231    0.1322        4.156943  1.000   \n",
       "1        52        48  0.3519   0.3072    0.0447        3.202761  1.000   \n",
       "2        58        42  0.0875   0.0875    0.0517        0.000000  0.500   \n",
       "3        51        49  0.0656   0.0656    0.0169        2.495544  1.000   \n",
       "4        56        44  0.3718   0.3718    0.0000        3.145818  1.000   \n",
       "5        50        50  0.4414   0.4414    0.0019        2.350248  1.000   \n",
       "6        49        51  0.2922   0.2922    0.1482        0.000000  0.500   \n",
       "7        46        54  0.0209   0.0149    0.0060        0.000000  0.500   \n",
       "8        43        57  0.4851   0.2972    0.1879        2.742725  0.962   \n",
       "9        57        43  0.2952   0.2753    0.0199        2.190332  1.000   \n",
       "\n",
       "         r2  switch  \n",
       "0  0.547000       0  \n",
       "1  0.928000       0  \n",
       "2  0.215599       0  \n",
       "3  0.756000       0  \n",
       "4  1.000000       0  \n",
       "5  0.977000       0  \n",
       "6  0.215599       0  \n",
       "7  0.215599       0  \n",
       "8  0.414000       0  \n",
       "9  0.920000       0  "
      ]
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Simulate data using the function\n",
    "X_sim, y_sim, df_vis = simulate_reads_single_gene(\n",
    "    p=0.5, read_coverage=100, phasing_error=0.04, num_hets=10, max_hets=64, data=train_df\n",
    ")\n",
    "\n",
    "# Call the evaluation function\n",
    "predicted_p_values, rmse = evaluate_simulation(X_sim, y_sim, model, device)\n",
    "\n",
    "# Visualize the simulated data if needed\n",
    "df_vis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted binomial p values:\n",
      "[[0.6588241]]\n",
      "Root Mean Squared Error (RMSE): 0.5588\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>refCount</th>\n",
       "      <th>altCount</th>\n",
       "      <th>MAF</th>\n",
       "      <th>min_MAF</th>\n",
       "      <th>diff_MAF</th>\n",
       "      <th>log10_distance</th>\n",
       "      <th>d</th>\n",
       "      <th>r2</th>\n",
       "      <th>switch</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>6</td>\n",
       "      <td>94</td>\n",
       "      <td>0.4553</td>\n",
       "      <td>0.3231</td>\n",
       "      <td>0.1322</td>\n",
       "      <td>4.156943</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.547000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>11</td>\n",
       "      <td>89</td>\n",
       "      <td>0.3519</td>\n",
       "      <td>0.3072</td>\n",
       "      <td>0.0447</td>\n",
       "      <td>3.202761</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.928000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4</td>\n",
       "      <td>96</td>\n",
       "      <td>0.0875</td>\n",
       "      <td>0.0875</td>\n",
       "      <td>0.0517</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.500</td>\n",
       "      <td>0.215599</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>13</td>\n",
       "      <td>87</td>\n",
       "      <td>0.0656</td>\n",
       "      <td>0.0656</td>\n",
       "      <td>0.0169</td>\n",
       "      <td>2.495544</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.756000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7</td>\n",
       "      <td>93</td>\n",
       "      <td>0.3718</td>\n",
       "      <td>0.3718</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>3.145818</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>8</td>\n",
       "      <td>92</td>\n",
       "      <td>0.4414</td>\n",
       "      <td>0.4414</td>\n",
       "      <td>0.0019</td>\n",
       "      <td>2.350248</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.977000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>9</td>\n",
       "      <td>91</td>\n",
       "      <td>0.2922</td>\n",
       "      <td>0.2922</td>\n",
       "      <td>0.1482</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.500</td>\n",
       "      <td>0.215599</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>11</td>\n",
       "      <td>89</td>\n",
       "      <td>0.0209</td>\n",
       "      <td>0.0149</td>\n",
       "      <td>0.0060</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.500</td>\n",
       "      <td>0.215599</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>8</td>\n",
       "      <td>92</td>\n",
       "      <td>0.4851</td>\n",
       "      <td>0.2972</td>\n",
       "      <td>0.1879</td>\n",
       "      <td>2.742725</td>\n",
       "      <td>0.962</td>\n",
       "      <td>0.414000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>10</td>\n",
       "      <td>90</td>\n",
       "      <td>0.2952</td>\n",
       "      <td>0.2753</td>\n",
       "      <td>0.0199</td>\n",
       "      <td>2.190332</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.920000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   refCount  altCount     MAF  min_MAF  diff_MAF  log10_distance      d  \\\n",
       "0         6        94  0.4553   0.3231    0.1322        4.156943  1.000   \n",
       "1        11        89  0.3519   0.3072    0.0447        3.202761  1.000   \n",
       "2         4        96  0.0875   0.0875    0.0517        0.000000  0.500   \n",
       "3        13        87  0.0656   0.0656    0.0169        2.495544  1.000   \n",
       "4         7        93  0.3718   0.3718    0.0000        3.145818  1.000   \n",
       "5         8        92  0.4414   0.4414    0.0019        2.350248  1.000   \n",
       "6         9        91  0.2922   0.2922    0.1482        0.000000  0.500   \n",
       "7        11        89  0.0209   0.0149    0.0060        0.000000  0.500   \n",
       "8         8        92  0.4851   0.2972    0.1879        2.742725  0.962   \n",
       "9        10        90  0.2952   0.2753    0.0199        2.190332  1.000   \n",
       "\n",
       "         r2  switch  \n",
       "0  0.547000       0  \n",
       "1  0.928000       0  \n",
       "2  0.215599       0  \n",
       "3  0.756000       0  \n",
       "4  1.000000       0  \n",
       "5  0.977000       0  \n",
       "6  0.215599       0  \n",
       "7  0.215599       0  \n",
       "8  0.414000       0  \n",
       "9  0.920000       0  "
      ]
     },
     "execution_count": 149,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Simulate data using the function\n",
    "X_sim, y_sim, df_vis = simulate_reads_single_gene(\n",
    "    p=0.1, read_coverage=100, phasing_error=0.04, num_hets=10, max_hets=64, data=train_df\n",
    ")\n",
    "\n",
    "# Call the evaluation function\n",
    "predicted_p_values, rmse = evaluate_simulation(X_sim, y_sim, model, device)\n",
    "\n",
    "# Visualize the simulated data if needed\n",
    "df_vis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted binomial p values:\n",
      "[[0.26917696]]\n",
      "Root Mean Squared Error (RMSE): 0.1692\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>refCount</th>\n",
       "      <th>altCount</th>\n",
       "      <th>MAF</th>\n",
       "      <th>min_MAF</th>\n",
       "      <th>diff_MAF</th>\n",
       "      <th>log10_distance</th>\n",
       "      <th>d</th>\n",
       "      <th>r2</th>\n",
       "      <th>switch</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>94</td>\n",
       "      <td>6</td>\n",
       "      <td>0.4553</td>\n",
       "      <td>0.3231</td>\n",
       "      <td>0.1322</td>\n",
       "      <td>4.156943</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.547000</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>11</td>\n",
       "      <td>89</td>\n",
       "      <td>0.3519</td>\n",
       "      <td>0.3072</td>\n",
       "      <td>0.0447</td>\n",
       "      <td>3.202761</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.928000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>96</td>\n",
       "      <td>4</td>\n",
       "      <td>0.0875</td>\n",
       "      <td>0.0875</td>\n",
       "      <td>0.0517</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.500</td>\n",
       "      <td>0.215599</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>13</td>\n",
       "      <td>87</td>\n",
       "      <td>0.0656</td>\n",
       "      <td>0.0656</td>\n",
       "      <td>0.0169</td>\n",
       "      <td>2.495544</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.756000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>93</td>\n",
       "      <td>7</td>\n",
       "      <td>0.3718</td>\n",
       "      <td>0.3718</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>3.145818</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>8</td>\n",
       "      <td>92</td>\n",
       "      <td>0.4414</td>\n",
       "      <td>0.4414</td>\n",
       "      <td>0.0019</td>\n",
       "      <td>2.350248</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.977000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>91</td>\n",
       "      <td>9</td>\n",
       "      <td>0.2922</td>\n",
       "      <td>0.2922</td>\n",
       "      <td>0.1482</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.500</td>\n",
       "      <td>0.215599</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>11</td>\n",
       "      <td>89</td>\n",
       "      <td>0.0209</td>\n",
       "      <td>0.0149</td>\n",
       "      <td>0.0060</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.500</td>\n",
       "      <td>0.215599</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>92</td>\n",
       "      <td>8</td>\n",
       "      <td>0.4851</td>\n",
       "      <td>0.2972</td>\n",
       "      <td>0.1879</td>\n",
       "      <td>2.742725</td>\n",
       "      <td>0.962</td>\n",
       "      <td>0.414000</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>10</td>\n",
       "      <td>90</td>\n",
       "      <td>0.2952</td>\n",
       "      <td>0.2753</td>\n",
       "      <td>0.0199</td>\n",
       "      <td>2.190332</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.920000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   refCount  altCount     MAF  min_MAF  diff_MAF  log10_distance      d  \\\n",
       "0        94         6  0.4553   0.3231    0.1322        4.156943  1.000   \n",
       "1        11        89  0.3519   0.3072    0.0447        3.202761  1.000   \n",
       "2        96         4  0.0875   0.0875    0.0517        0.000000  0.500   \n",
       "3        13        87  0.0656   0.0656    0.0169        2.495544  1.000   \n",
       "4        93         7  0.3718   0.3718    0.0000        3.145818  1.000   \n",
       "5         8        92  0.4414   0.4414    0.0019        2.350248  1.000   \n",
       "6        91         9  0.2922   0.2922    0.1482        0.000000  0.500   \n",
       "7        11        89  0.0209   0.0149    0.0060        0.000000  0.500   \n",
       "8        92         8  0.4851   0.2972    0.1879        2.742725  0.962   \n",
       "9        10        90  0.2952   0.2753    0.0199        2.190332  1.000   \n",
       "\n",
       "         r2  switch  \n",
       "0  0.547000       1  \n",
       "1  0.928000       0  \n",
       "2  0.215599       1  \n",
       "3  0.756000       0  \n",
       "4  1.000000       1  \n",
       "5  0.977000       0  \n",
       "6  0.215599       1  \n",
       "7  0.215599       0  \n",
       "8  0.414000       1  \n",
       "9  0.920000       0  "
      ]
     },
     "execution_count": 150,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Simulate data using the function\n",
    "X_sim, y_sim, df_vis = simulate_reads_single_gene(\n",
    "    p=0.1, read_coverage=100, phasing_error=0.99, num_hets=10, max_hets=64, data=train_df\n",
    ")\n",
    "\n",
    "# Call the evaluation function\n",
    "predicted_p_values, rmse = evaluate_simulation(X_sim, y_sim, model, device)\n",
    "\n",
    "# Visualize the simulated data if needed\n",
    "df_vis\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
