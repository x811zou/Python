{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/bin/python\n"
     ]
    }
   ],
   "source": [
    "#source /home/scarlett/github/ipython_notebook/Coding_pythons/myenv/bin/activate\n",
    "import sys\n",
    "print(sys.executable)\n",
    "\n",
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "from torch.nn import CrossEntropyLoss\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To install new packages:\n",
    "- source /home/scarlett/github/Coding_pythons/newenv/bin/activate\n",
    "- pip install <package_name>\n",
    "- pip list\n",
    "- import <package_name>\n",
    "- deactivate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deep learning\n",
    "- deep learning is everywhere:\n",
    "    - language translation\n",
    "    - self-driving cars\n",
    "    - medical diagnostics\n",
    "    - chatbots\n",
    "\n",
    "- used on multiple data types: images, text and audio\n",
    "- traditional machine learning: relies on hand-crafted feature engineering\n",
    "- deep learning: enables feature learning from raw data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is deep learning?\n",
    "- An architecture of network consists of input, hidden layers and output\n",
    "- A network can have one or many hidden layers\n",
    "- Require 100K data\n",
    "- pytorch supports tabular data, also unstructral data like image (torchvision), audio data (torchaudio), text data (torchtext)\n",
    "- deep learning often requires a GPU, which, compard to a CPU can offer: parallel computing capabilities, faster training times and better performance\n",
    "- tensors are multidimensional representations of their elements"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### make tensor: load from list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1, 2, 3],\n",
       "        [4, 5, 6],\n",
       "        [7, 8, 9]])"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lst = [[1,2,3],[4,5,6],[7,8,9]]\n",
    "tensor = torch.tensor(lst)\n",
    "tensor "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 3])"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tensor.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.int64"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tensor.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cpu')"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tensor.device"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### load from NumPy array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "array = [[1,2,3],[4,5,6],[7,8,9]]\n",
    "np_array = np.array(array)\n",
    "np_tensor = torch.from_numpy(np_array)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### tensor operations - most numpy array operations can be performedon PyTorch tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 2,  4,  6],\n",
       "        [ 8, 10, 12],\n",
       "        [14, 16, 18]])"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tensor + np_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 1,  4,  9],\n",
       "        [16, 25, 36],\n",
       "        [49, 64, 81]])"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tensor * np_tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## neural net\n",
    "### basic two layer NN without hidden layer\n",
    "- first layer: input, second layer: linear layer\n",
    "- each linear layer has a .weight and .bias associated\n",
    "- nn.linear performs\n",
    "    - when input_tensor is passed to linear_layer, the linear operation performed is matrix multiplication of input_tensor and the weights, followed by adding in the bias\n",
    "    - input X, weights W0, bias b0 --> y0 = W0 * X +b0; in pytorch: output = W0 * input + b0\n",
    "    - initially, when we call nn.Linear(), weights and biases are initialized randomly, so they are not yet useful\n",
    "    - we have to tune these weights and biases\n",
    "- two-layer network summary:\n",
    "    - took 1 by 3 input as the first layer (1 linear layer with specific arguments as the second layer, and retured a 1 by 2 output)\n",
    "    - linear layers have connections (or arrows) between each input and output neuron, making them fully connected\n",
    "        - networks with only linear layers are called: \"fully connected networks\"\n",
    "        - each neuron in one layer is connected to each neuron in the next layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.4958, -0.3643]], grad_fn=<AddmmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "## create input_tensor with 3 features\n",
    "input_tensor = torch.tensor([[0.3471, 0.4547, -0.2356]], dtype=torch.float32)\n",
    "\n",
    "## define first linear layer ( a linear layer takes an input, applies a linear fxn, and returns output)\n",
    "linear_layer = nn.Linear(in_features=3, out_features=2)\n",
    "\n",
    "## pass input through linear layer\n",
    "output = linear_layer(input_tensor)\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "linear_layer.weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "linear_layer.bias"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multi-layer model: Stacking layers with nn.Sequential()\n",
    "- this model takes input, passes it to each linear layer in sequence, and returns output\n",
    "    - the first layer takes input with 10 features, and output with a tensor with 18 features\n",
    "    - the second layer takes input with 18 features, and output with a tensor with 20 features\n",
    "    - the third layer takess input with 20 featuresl and output with a tensor with 5 features\n",
    "- output is not linear until each layer has tuned weights and biases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = nn.Sequential(\n",
    "    nn.Linear(10,18), \n",
    "    nn.Linear(18,20), \n",
    "    nn.Linear(20,5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make a input_tensor with ten features (neurons)\n",
    "input_tensor = torch.rand(1,10)\n",
    "input_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pass input_tensor through model\n",
    "output = model(input_tensor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### In this exercise, you will implement a small neural network containing two linear layers. The first layer takes an eight-dimensional input, and the last layer outputs a one-dimensional tensor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_tensor = torch.Tensor([[2, 3, 6, 7, 9, 3, 2, 1]])\n",
    "\n",
    "# Implement a small neural network with exactly two linear layers\n",
    "model = nn.Sequential(nn.Linear(8,5),\n",
    "                      nn.Linear(5,1)\n",
    "                     )\n",
    "\n",
    "output = model(input_tensor)\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Activation functions\n",
    "-  used in the last layer of NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chapter 2: Training First NN with PyTorch\n",
    "### Section 2.1 Running a forward pass\n",
    "- input data is **passed forward** or **propagated** through a network\n",
    "- computations performed at each layer\n",
    "- outputs of each layer passed to each subsequent layer\n",
    "- output of final layer: \"prediction\"\n",
    "- used for both **training** and prediction\n",
    "- some possible outputs:\n",
    "    - binary classification (sum up to 1)\n",
    "    - multiclass classification (sum up to 1, value with highest probability is assigned predicted label in each row)\n",
    "    - regression values\n",
    "- results would not be meaningful untill we use backpropagation to update weights and biases\n",
    "- there is also a backward pass\n",
    "    - used to update weights and baises during training\n",
    "    - in the training loop: \n",
    "        1. propagate data forward\n",
    "        2. compare outputs to the true values (ground-truth)\n",
    "        3. backpropagate to update model weights and baises\n",
    "        4. repeaat weights and biases are tuned to produce useful outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### binary classificationl forward pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.3303,  0.2862, -1.0250,  0.2587, -0.0196, -0.1723],\n",
       "        [-0.8667,  1.0218, -2.2905, -0.2409, -0.4707,  1.5993],\n",
       "        [ 0.0264,  0.5633,  1.2191, -0.4299,  0.9548, -0.7300],\n",
       "        [ 0.1807,  0.9939,  1.2226, -0.6087,  0.9974, -1.3976],\n",
       "        [ 1.4309, -0.6085,  0.1766, -0.1891,  0.3819,  0.0889]])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create input data of shape 5*6\n",
    "data = torch.randn(5, 6)\n",
    "input_data = data\n",
    "input_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.4012],\n",
      "        [0.3571],\n",
      "        [0.3702],\n",
      "        [0.3924],\n",
      "        [0.4166]], grad_fn=<SigmoidBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# create binary classification model\n",
    "model = nn.Sequential(\n",
    "    nn.Linear(6, 3),\n",
    "    nn.Linear(3, 1),\n",
    "    nn.Sigmoid()\n",
    ")\n",
    "\n",
    "# pass input data through model\n",
    "output = model(input_data)\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### multi-class classificationl forward pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([5, 3])\n"
     ]
    }
   ],
   "source": [
    "# specifcy model has 3 clases\n",
    "n_classes = 3\n",
    "\n",
    "# create 2-layer multi-class classification model\n",
    "model = nn.Sequential(\n",
    "    nn.Linear(6, 4),\n",
    "    nn.Linear(4, n_classes),\n",
    "    nn.Softmax(dim=-1)\n",
    ")\n",
    "\n",
    "# pass input data through model\n",
    "output = model(input_data)\n",
    "print(output.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### regression forward pass\n",
    "- no activation layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.2892],\n",
      "        [-0.1950],\n",
      "        [-0.1918],\n",
      "        [-0.2454],\n",
      "        [-0.2936]], grad_fn=<AddmmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# create regression model\n",
    "model = nn.Sequential(\n",
    "    nn.Linear(6, 4),\n",
    "    nn.Linear(4, 1)\n",
    ")\n",
    "\n",
    "# pass input data through model\n",
    "output = model(input_data)\n",
    "\n",
    "# return output\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Section 2.2 Using loss function to assess model predictions\n",
    "#### Loss function\n",
    "- gives feedback to model during training\n",
    "- takes in model prediction y_hat and ground truth y\n",
    "- outputs a float\n",
    "- example: predicted class = 0\n",
    "    - if correct = low loss\n",
    "    - if wrong = high loss\n",
    "- Goal: minimize loss\n",
    "- one-hot encoding concepts: loss = F(y, y_hat)\n",
    "    - **y is a single integer** (class label) e.g. y = 0 when y is mammal\n",
    "    - **y_hat is a tensor** (output of softmax)\n",
    "        - if N is the number of classes, e.g. N = 3\n",
    "        - y_hat is a tensor with N dimensions, e.g. y_hat = [0.5,0.8,0.1]\n",
    "    - transform true label to tensor of zeros and ones\n",
    "        - ground truth y = 0\n",
    "        - number of classes N = 3\n",
    "        - class: 0  1  2 \n",
    "        - one-hot encoding: 1  0  0 / \n",
    "                            0  1  0 /\n",
    "                            0  1  1 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1, 0, 0],\n",
       "        [0, 1, 0],\n",
       "        [0, 0, 1]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "F.one_hot(torch.tensor([0, 1, 2]), num_classes=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1, 0, 0])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "F.one_hot(torch.tensor(0), num_classes=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### loss function takes\n",
    "- scores\n",
    "    - model predictions before the final softmax function\n",
    "- one_hot_target\n",
    "    - one hot encoded ground truth label\n",
    "#### loss function output\n",
    "- loss\n",
    "    - a single float"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.81307452917099\n"
     ]
    }
   ],
   "source": [
    "# Correct way to create tensors\n",
    "scores = torch.tensor([[-0.1211, 0.1059]])  # Raw scores (logits), not probabilities\n",
    "# Target should be the index of the correct class, not one-hot encoded\n",
    "class_target = torch.tensor([0])  # Class index, e.g., class 0\n",
    "\n",
    "# Instantiate the CrossEntropyLoss\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Compute the loss\n",
    "loss = criterion(scores, class_target)\n",
    "\n",
    "print(\"Loss:\", loss.item())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Creating one-hot encoded labels**\n",
    "- Manually create a one-hot encoded vector of the ground truth label y by filling in the NumPy array provided.\n",
    "- Create a one-hot encoded vector of the ground truth label y using PyTorch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = 1\n",
    "num_classes = 3\n",
    "\n",
    "# Create the one-hot encoded vector using NumPy\n",
    "one_hot_numpy = np.array([0, 1, 0])\n",
    "\n",
    "# Create the one-hot encoded vector using PyTorch\n",
    "one_hot_pytorch = F.one_hot(torch.tensor(y), num_classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Calculating cross entropy loss**\n",
    "- Create the one-hot encoded vector of the ground truth label y and assign it to one_hot_label.\n",
    "- Create the cross entropy loss function and store it as criterion.\n",
    "- Calculate the cross entropy loss using the one_hot_label vector and the scores vector, by calling the loss_function you created."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(8.0619, dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "y = [2]\n",
    "scores = torch.tensor([[0.1, 6.0, -2.0, 3.2]])\n",
    "\n",
    "# Create a one-hot encoded vector of the label y\n",
    "one_hot_label = F.one_hot(torch.tensor(y), scores.shape[1])\n",
    "\n",
    "# Create the cross entropy loss function\n",
    "criterion = CrossEntropyLoss()\n",
    "\n",
    "# Calculate the cross entropy loss\n",
    "loss = criterion(scores.double(), one_hot_label.double())\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Section 2.3 Using derivatives to update model parameters\n",
    "- goal: minimize the loss\n",
    "    - high loss: wrong prediction\n",
    "    - low loss: correct prediction\n",
    "- hiking down a mountain to the valley floor\n",
    "    - steep slopes: a step makes us lose a lot of elevation = derivative is high (red arrows)\n",
    "    - gentle slopes: a step makes us lose a little bit of elevation = detivative is low (green arrows)\n",
    "    - valley floor: not losing elevation by taking a step = derivative is null\n",
    "- connecting derivatives and model training\n",
    "    - model training: updating a model's parameters to minimize the loss\n",
    "        - we take a dataset with features X, and groud truth y. We run a forward pass using X and calculate loss by comparing model output, y_hat, with y. we compute gradients of the loss function and use them to update the model parameters with backpropagation so that weights are no longer random and biases are useful. we repeat until the layers are tuned.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Backpropagation concepts\n",
    "- consider a network made of 3 layers. L0, L1, L2\n",
    "    - calculate local gradients for L0, L1 and L2 using backpropagation\n",
    "    - calculate loss gradients with respect to L2, then use L2 gradients to calcualte L1 gradients, and so on"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the model and run a forward pass\n",
    "model = nn.Sequential(\n",
    "    nn.Linear(6, 4),\n",
    "    nn.Linear(4, 3),\n",
    "    nn.Linear(3,2))\n",
    "prediction = model(input_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "Target 2 is out of bounds.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[39], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# calculate the loss and compute the gradients\u001b[39;00m\n\u001b[1;32m      2\u001b[0m criterion \u001b[38;5;241m=\u001b[39m CrossEntropyLoss()\n\u001b[0;32m----> 3\u001b[0m loss \u001b[38;5;241m=\u001b[39m \u001b[43mcriterion\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprediction\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      4\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/loss.py:1188\u001b[0m, in \u001b[0;36mCrossEntropyLoss.forward\u001b[0;34m(self, input, target)\u001b[0m\n\u001b[1;32m   1187\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor, target: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m-> 1188\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcross_entropy\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1189\u001b[0m \u001b[43m                           \u001b[49m\u001b[43mignore_index\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mignore_index\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreduction\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreduction\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1190\u001b[0m \u001b[43m                           \u001b[49m\u001b[43mlabel_smoothing\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlabel_smoothing\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/functional.py:3104\u001b[0m, in \u001b[0;36mcross_entropy\u001b[0;34m(input, target, weight, size_average, ignore_index, reduce, reduction, label_smoothing)\u001b[0m\n\u001b[1;32m   3102\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m size_average \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m reduce \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   3103\u001b[0m     reduction \u001b[38;5;241m=\u001b[39m _Reduction\u001b[38;5;241m.\u001b[39mlegacy_get_string(size_average, reduce)\n\u001b[0;32m-> 3104\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_C\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_nn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcross_entropy_loss\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_Reduction\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_enum\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreduction\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mignore_index\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabel_smoothing\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mIndexError\u001b[0m: Target 2 is out of bounds."
     ]
    }
   ],
   "source": [
    "# calculate the loss and compute the gradients\n",
    "criterion = CrossEntropyLoss()\n",
    "loss = criterion(prediction, target)\n",
    "loss.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(None, None)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# access each layer's gradidents\n",
    "model[0].weight.grad, model[0].bias.grad\n",
    "model[1].weight.grad, model[1].bias.grad\n",
    "model[2].weight.grad, model[2].bias.grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### updating model parametes\n",
    "- update the weights by subtracting local gradients scaled by the learning rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "unsupported operand type(s) for *: 'float' and 'NoneType'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[41], line 7\u001b[0m\n\u001b[1;32m      5\u001b[0m weight \u001b[38;5;241m=\u001b[39m model[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mweight\n\u001b[1;32m      6\u001b[0m weight_grad \u001b[38;5;241m=\u001b[39m model[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mweight\u001b[38;5;241m.\u001b[39mgrad\n\u001b[0;32m----> 7\u001b[0m weight \u001b[38;5;241m=\u001b[39m weight \u001b[38;5;241m-\u001b[39m \u001b[43mlr\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mweight_grad\u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;66;03m# update the bias\u001b[39;00m\n\u001b[1;32m     10\u001b[0m bias \u001b[38;5;241m=\u001b[39m model[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mbias\n",
      "\u001b[0;31mTypeError\u001b[0m: unsupported operand type(s) for *: 'float' and 'NoneType'"
     ]
    }
   ],
   "source": [
    "# learning rate is typically small\n",
    "lr = 0.001\n",
    "\n",
    "# update the weights\n",
    "weight = model[0].weight\n",
    "weight_grad = model[0].weight.grad\n",
    "weight = weight - lr * weight_grad\n",
    "\n",
    "# update the bias\n",
    "bias = model[0].bias\n",
    "bias_grad = model[0].bias.grad\n",
    "bias = bias - lr * bias_grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### convex and non-convex funtions\n",
    "- convex function has one global minimum\n",
    "- non-convex function has more than 1 global minimum (loss function in DL)\n",
    "\n",
    "#### gradient descent\n",
    "- for non-convex functins, the iterative process used is GD\n",
    "- in PyTorch, an optimizer takes care of weight updates\n",
    "- the most common optimizer is stochastic gradient descent (SGD)\n",
    "- optimizer handles updating model parameters (or weights) after calculation of local gradients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the optimizer\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.0001)\n",
    "optimizer.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Question:\n",
    "- The following tensors are provided:\n",
    "    - weight: a 2 by 9 element tensor\n",
    "    - bias: a 2 element tensor\n",
    "    - preds: a 1 by 2 element tensor containing the model predictions\n",
    "    - target: a 1 by 2 element one-hot encoded tensor containing the ground-truth label\n",
    "\n",
    "- Use the criterion you have defined to calculate the loss value with respect to the predictions and target values.\n",
    "- Compute the gradients of the cross entropy loss.\n",
    "- Display the gradients of the weight and bias tensors, in that order."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Calculate the loss\n",
    "loss = criterion(preds, target)\n",
    "\n",
    "# Compute the gradients of the loss\n",
    "loss.backward()\n",
    "\n",
    "# Display gradients of the weight and bias tensors in order\n",
    "print(weight.grad)\n",
    "print(bias.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Question: A PyTorch model created with the nn.Sequential() is a module that contains the different layers of your network. Recall that each layer parameter can be accessed by indexing the created model directly. In this exercise, you will practice accessing the parameters of different linear layers of a neural network. You won't be accessing the sigmoid.\n",
    "- Access the weight parameter of the first linear layer.\n",
    "- Access the bias parameter of the second linear layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = nn.Sequential(nn.Linear(16, 8),\n",
    "                      nn.Sigmoid(),\n",
    "                      nn.Linear(8, 2))\n",
    "\n",
    "# Access the weight of the first linear layer\n",
    "weight_0 = model[0].weight\n",
    "\n",
    "# Access the bias of the second linear layer\n",
    "bias_1 = model[2].bias"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Question: Updating the weights manually\n",
    "- Create the gradient variables by accessing the local gradients of each weight tensor.\n",
    "- Update the weights using the gradients scaled by the learning rate.\n",
    "- Use optim to create an SGD optimizer with a learning rate of your choice (must be less than one) for the model provided.\n",
    "- Update the model's parameters using the optimizer.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'Sigmoid' object has no attribute 'weight'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[47], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m weight0 \u001b[38;5;241m=\u001b[39m model[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mweight\n\u001b[0;32m----> 2\u001b[0m weight1 \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\n\u001b[1;32m      3\u001b[0m weight2 \u001b[38;5;241m=\u001b[39m model[\u001b[38;5;241m2\u001b[39m]\u001b[38;5;241m.\u001b[39mweight\n\u001b[1;32m      5\u001b[0m \u001b[38;5;66;03m# Access the gradients of the weight of each linear layer\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1729\u001b[0m, in \u001b[0;36mModule.__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   1727\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m modules:\n\u001b[1;32m   1728\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m modules[name]\n\u001b[0;32m-> 1729\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(\u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m object has no attribute \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'Sigmoid' object has no attribute 'weight'"
     ]
    }
   ],
   "source": [
    "weight0 = model[0].weight\n",
    "weight1 = model[1].weight\n",
    "weight2 = model[2].weight\n",
    "\n",
    "# Access the gradients of the weight of each linear layer\n",
    "grads0 = weight0.grad\n",
    "grads1 = weight1.grad\n",
    "grads2 = weight2.grad\n",
    "\n",
    "# Update the weights using the learning rate and the gradients\n",
    "weight0 = weight0 - lr*grads0\n",
    "weight1 = weight1 - lr*grads1\n",
    "weight2 = weight2 - lr*grads2\n",
    "\n",
    "# Create the optimizer\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.001)\n",
    "\n",
    "loss = criterion(pred, target)\n",
    "loss.backward()\n",
    "\n",
    "# Update the model's parameters using the optimizer\n",
    "optimizer.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Section 2.4 Writing our first training loop\n",
    "- create a model\n",
    "- choose a loss function\n",
    "- create a dataset (normalize, missing, imbalance)\n",
    "- define an optimizer\n",
    "- run a training loop, where for each sample of the dataset, we repeat:\n",
    "    - calculating loss(forward pass)\n",
    "    - calculating local gradients\n",
    "    - updating model parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example: data science salary dataset (regression problem)\n",
    "- no softmax or sigmoid as last activation function\n",
    "- last layer is linear layer\n",
    "- **loss function: MSE loss** --> mean (sum(prediction - target)^2)\n",
    "    - prediction and targets must be a float tensor\n",
    "- regression problem does not need one-hot encoding and the final linear layer outputs a single float"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the dataset and the dataloader\n",
    "dataset = TensorDataset(torch.tensor(features).float(), torch.tensor(target).float())\n",
    "dataloader = DataLoader(dataset, batch_size=4, shuffle=True)\n",
    "\n",
    "# create the model\n",
    "model = nn.Sequential(nn.Linear(6, 4),\n",
    "                      nn.Linear(2,1))\n",
    "\n",
    "# create the loss and optimizer\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loop through the dataset multiple times - loop through data once is called an epoch\n",
    "# each iteration of the dataoader provides a batch of samples\n",
    "for epoch in range(num_epochs):\n",
    "    for data in dataLoader:\n",
    "        # set the gradients to zero\n",
    "        optimizer.zero_grad()\n",
    "        # get feature and target from the data loader\n",
    "        features, target = data\n",
    "        # run a forward pass\n",
    "        pred = model(features)\n",
    "        # calculate the loss and gradients\n",
    "        loss = criterion(pred, target)\n",
    "        loss.backward()\n",
    "        # update the weights\n",
    "        optimizer.step()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Practice: using the MSELoss\n",
    "- Calculate the MSELoss using NumPy.\n",
    "- Create a MSELoss function using PyTorch.\n",
    "- Convert y_hat and y to tensors and then float data types, and then use them to calculate MSELoss using PyTorch as mse_pytorch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_hat = np.array(10)\n",
    "y = np.array(1)\n",
    "\n",
    "# Calculate the MSELoss using NumPy\n",
    "mse_numpy = np.mean((y_hat - y)**2)\n",
    "\n",
    "# Create the MSELoss function\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "# Calculate the MSELoss using the created loss function\n",
    "mse_pytorch = criterion(torch.tensor(y_hat).float(), torch.tensor(y).float())\n",
    "print(mse_pytorch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Practice: writing a training loop\n",
    "- Write a for loop that iterates over the dataloader; this should be nested within a for loop that iterates over a range equal to the number of epochs.\n",
    "- Set the gradients of the optimizer to zero.\n",
    "- Write the forward pass.\n",
    "- Compute the MSE loss value using the criterion() function provided.\n",
    "- Compute the gradients.\n",
    "- Update the model's parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loop over the number of epochs and the dataloader\n",
    "for i in range(num_epochs):\n",
    "  for data in dataloader:\n",
    "    # Set the gradients to zero\n",
    "    optimizer.zero_grad()\n",
    "    # Run a forward pass\n",
    "    feature, target = data\n",
    "    prediction = model(feature)    \n",
    "    # Calculate the loss\n",
    "    loss = criterion(prediction, target)    \n",
    "    # Compute the gradients\n",
    "    loss.backward()\n",
    "    # Update the model's parameters\n",
    "    optimizer.step()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chapter 3: NN architecture and hyperparameters\n",
    "### Section 3.1 discover activation functions between layers\n",
    "- sigmoid: output will be bounded between 0 and 1\n",
    "    - gradient vanishing problem that gradient is close to zero for low or high values\n",
    "- softmax can only be used in the last layer\n",
    "- ReLU: rectified linear unit\n",
    "    - fx = max(x, 0)\n",
    "    - overcome vanishing gradients problem\n",
    "- leaky ReLU: for negative inputs, it multiplies the input by a small coefficient (default to 0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "relu = nn.ReLU()\n",
    "leaky_relu = nn.LeakyReLU(negative_slope = 0.05)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Question: In this exercise, you'll begin with a ReLU implementation in PyTorch. Next, you'll calculate the gradients of the function.\n",
    "- Calculate the gradient of the ReLU function for x using the relu_pytorch() function you defined, then running a backward pass.\n",
    "- Find the gradient at x."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a ReLU function with PyTorch\n",
    "relu_pytorch = nn.ReLU()\n",
    "\n",
    "# Apply your ReLU function on x, and calculate gradients\n",
    "x = torch.tensor(-1.0, requires_grad=True)\n",
    "y = relu_pytorch(x)\n",
    "y.backward()\n",
    "\n",
    "# Print the gradient of the ReLU function for x\n",
    "gradient = x.grad\n",
    "print(gradient)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Question: In this exercise, you will implement the leaky ReLU function in NumPy and PyTorch and practice using it. The numpy as np package, the torch package as well as the torch.nn as nn have already been imported.\n",
    "- Create a leaky ReLU function in PyTorch with a negative slope of 0.05.\n",
    "- Call the function on the tensor x, which has already been defined for you."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a leaky relu function in PyTorch\n",
    "leaky_relu_pytorch = nn.LeakyReLU(negative_slope = 0.05)\n",
    "\n",
    "x = torch.tensor(-2.0)\n",
    "# Call the above function on the tensor x\n",
    "output = leaky_relu_pytorch(x)\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Section 3.2 A deeper dive into NN architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Section 3.3 Learning rate and momentum"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Section 3.4 Layer initialization and transfer learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chapter 4: Evaluating and improving models\n",
    "### Section 4.1 A deeper dive into loading data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Section 4.2 Evaluating model performance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Section 4.3 Fighting overfitting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Section 4.4 Improvinf model performance"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
