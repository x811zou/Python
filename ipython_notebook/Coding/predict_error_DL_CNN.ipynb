{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.1.1\n"
     ]
    }
   ],
   "source": [
    "# Data handling and processing\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "import os \n",
    "# Data visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Model evaluation and utilities\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import log_loss\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
    "warnings.filterwarnings('ignore', message=\"is_sparse is deprecated\")\n",
    "from sklearn.model_selection import ParameterSampler\n",
    "from sklearn.metrics import roc_auc_score, brier_score_loss\n",
    "\n",
    "# XGBoost\n",
    "import xgboost as xgb\n",
    "print(xgb.__version__)\n",
    "\n",
    "# Statsmodels\n",
    "import statsmodels.api as sm\n",
    "\n",
    "# PyTorch\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset, random_split\n",
    "import torchmetrics\n",
    "from torch.nn import functional as F\n",
    "from torch.nn import CrossEntropyLoss\n",
    "\n",
    "# Optuna for hyperparameter optimization\n",
    "import optuna\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.metrics import log_loss\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import torch.nn.init as init"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare NN input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_sample(sample):\n",
    "    # Define the paths based on the sample name\n",
    "    qb_path = f'/data2/1000Genome/{sample}/qb/{sample}'\n",
    "    readcount_path = f'/data2/1000Genome/{sample}/beastie/runModel_phased_even100/chr1-22_alignBiasp0.05_s0.7_a0.05_sinCov0_totCov1_W1000K1000/tmp/{sample}'\n",
    "    \n",
    "    # Check if the 'beastie' folder exists (we already filtered for this, so this is optional now)\n",
    "    if not os.path.isdir(f'/data2/1000Genome/{sample}/beastie'):\n",
    "        return\n",
    "\n",
    "    # Read the qb file and extract qb_label\n",
    "    qb_data = pd.read_csv(f\"{qb_path}_qb_highestsite.tsv\", sep='\\t')\n",
    "    qb_data[\"qb_label\"] = qb_data[\"qb_mode\"]\n",
    "    qb_label = qb_data[[\"geneID\", \"qb_label\"]]\n",
    "\n",
    "    # Read the input readcount file\n",
    "    input_readcount = pd.read_csv(f\"{readcount_path}_real_alignBiasafterFilter.phasedByshapeit2.cleaned.tsv\", sep='\\t')\n",
    "    input_readcount = input_readcount[[\"geneID\", \"chrN\", \"pos\", \"refCount\", \"altCount\"]]\n",
    "\n",
    "    # Read the genetic features file\n",
    "    genetic_features = pd.read_csv(f\"{readcount_path}.meta.w_error.tsv\", sep='\\t')\n",
    "    genetic_features = genetic_features[[\"geneID\", \"pos\", \"MAF\", \"min_MAF\", \"diff_MAF\", \"d\", \"r2\", \"log10_distance\"]]\n",
    "\n",
    "    # Perform inner join of the genetic features and the input readcount\n",
    "    input_df = pd.merge(input_readcount, genetic_features, how=\"inner\", on=[\"geneID\", \"pos\"])\n",
    "    input_df[\"individual\"] = sample\n",
    "\n",
    "    # Read the ancestry file and get the first word\n",
    "    ancestry_file = f'/data2/1000Genome/{sample}/ancestry'\n",
    "    with open(ancestry_file, 'r') as file:\n",
    "        ancestry = file.readline().strip()\n",
    "\n",
    "    input_df[\"ancestry\"] = ancestry\n",
    "\n",
    "    # Format the geneID column\n",
    "    input_df[\"geneID\"] = input_df[\"geneID\"].str.split(\".\").str[0]\n",
    "\n",
    "    # Merge input_df with qb_label\n",
    "    input_df = pd.merge(input_df, qb_label, how=\"inner\", on=[\"geneID\"])\n",
    "\n",
    "    # Save the final DataFrame as a TSV file\n",
    "    output_file = f\"{qb_path}_NN_input.tsv\"\n",
    "    input_df.to_csv(output_file, sep='\\t', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Find only the samples with the 'beastie' directory\n",
    "# base_dir = '/data2/1000Genome/'\n",
    "# samples = [\n",
    "#     sample for sample in os.listdir(base_dir)\n",
    "#     if os.path.isdir(os.path.join(base_dir, sample)) and\n",
    "#     os.path.isdir(f'/data2/1000Genome/{sample}/beastie')\n",
    "# ]\n",
    "\n",
    "# # Process each sample with a progress bar\n",
    "# for sample in tqdm(samples, desc=\"Processing samples\"):\n",
    "#     process_sample(sample)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Randomly select some individuals for a training set and testing set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_and_split_samples(ancestry_groups, num_individuals_train=80, num_individuals_test=20, base_dir='/data2/1000Genome/'):\n",
    "    # Ensure ancestry_groups is a list\n",
    "    if isinstance(ancestry_groups, str):\n",
    "        ancestry_groups = [ancestry_groups]\n",
    "    \n",
    "    # Calculate per-group requirements\n",
    "    num_groups = len(ancestry_groups)\n",
    "    num_train_per_group = num_individuals_train // num_groups\n",
    "    num_test_per_group = num_individuals_test // num_groups\n",
    "    \n",
    "    # Validate that the train and test counts are evenly divisible\n",
    "    if num_individuals_train % num_groups != 0 or num_individuals_test % num_groups != 0:\n",
    "        raise ValueError(\"Train and test counts must be evenly divisible by the number of ancestry groups.\")\n",
    "    \n",
    "    # Collect output file paths for each ancestry group\n",
    "    ancestry_files = {group: [] for group in ancestry_groups}\n",
    "    for sample in os.listdir(base_dir):\n",
    "        sample_dir = os.path.join(base_dir, sample)\n",
    "        ancestry_file = os.path.join(sample_dir, 'ancestry')\n",
    "        output_file = os.path.join(sample_dir, 'qb', f\"{sample}_NN_input.tsv\")\n",
    "        \n",
    "        # Check if both ancestry file and output file exist\n",
    "        if os.path.isfile(ancestry_file) and os.path.isfile(output_file):\n",
    "            with open(ancestry_file, 'r') as file:\n",
    "                ancestry = file.readline().strip()\n",
    "            # Collect the file path if it matches any specified ancestry group\n",
    "            if ancestry in ancestry_files:\n",
    "                ancestry_files[ancestry].append(output_file)\n",
    "    \n",
    "    # Initialize empty lists for train and test files\n",
    "    train_files = []\n",
    "    test_files = []\n",
    "    \n",
    "    # Check and select files for each ancestry group\n",
    "    for group, files in ancestry_files.items():\n",
    "        if len(files) < (num_train_per_group + num_test_per_group):\n",
    "            raise ValueError(f\"Not enough individuals in {group} ancestry group. Required: {num_train_per_group + num_test_per_group}, Found: {len(files)}\")\n",
    "        \n",
    "        # Randomly select individuals for train and test\n",
    "        selected_files = random.sample(files, num_train_per_group + num_test_per_group)\n",
    "        train_files.extend(selected_files[:num_train_per_group])\n",
    "        test_files.extend(selected_files[num_train_per_group:num_train_per_group + num_test_per_group])\n",
    "    \n",
    "    # Combine data for the training set\n",
    "    train_df = pd.DataFrame()\n",
    "    for file_path in train_files:\n",
    "        df = pd.read_csv(file_path, sep='\\t')\n",
    "        train_df = pd.concat([train_df, df], ignore_index=True)\n",
    "    \n",
    "    # Combine data for the test set\n",
    "    test_df = pd.DataFrame()\n",
    "    for file_path in test_files:\n",
    "        df = pd.read_csv(file_path, sep='\\t')\n",
    "        test_df = pd.concat([test_df, df], ignore_index=True)\n",
    "    \n",
    "    return train_df, test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example usage\n",
    "train_df, test_df = select_and_split_samples(['GBR', 'CEU', 'TSI', 'YRI'], num_individuals_train=80, num_individuals_test=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>geneID</th>\n",
       "      <th>chrN</th>\n",
       "      <th>pos</th>\n",
       "      <th>refCount</th>\n",
       "      <th>altCount</th>\n",
       "      <th>MAF</th>\n",
       "      <th>min_MAF</th>\n",
       "      <th>diff_MAF</th>\n",
       "      <th>d</th>\n",
       "      <th>r2</th>\n",
       "      <th>log10_distance</th>\n",
       "      <th>individual</th>\n",
       "      <th>ancestry</th>\n",
       "      <th>qb_label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ENSG00000188976</td>\n",
       "      <td>1</td>\n",
       "      <td>881627</td>\n",
       "      <td>93</td>\n",
       "      <td>67</td>\n",
       "      <td>0.3668</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>HG00261</td>\n",
       "      <td>GBR</td>\n",
       "      <td>0.431798</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ENSG00000188976</td>\n",
       "      <td>1</td>\n",
       "      <td>889876</td>\n",
       "      <td>7</td>\n",
       "      <td>3</td>\n",
       "      <td>0.0159</td>\n",
       "      <td>0.0159</td>\n",
       "      <td>0.3509</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.027</td>\n",
       "      <td>3.916401</td>\n",
       "      <td>HG00261</td>\n",
       "      <td>GBR</td>\n",
       "      <td>0.431798</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ENSG00000187961</td>\n",
       "      <td>1</td>\n",
       "      <td>896064</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0378</td>\n",
       "      <td>0.0159</td>\n",
       "      <td>0.0219</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>HG00261</td>\n",
       "      <td>GBR</td>\n",
       "      <td>0.313768</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ENSG00000187961</td>\n",
       "      <td>1</td>\n",
       "      <td>897730</td>\n",
       "      <td>11</td>\n",
       "      <td>2</td>\n",
       "      <td>0.0388</td>\n",
       "      <td>0.0378</td>\n",
       "      <td>0.0010</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.000</td>\n",
       "      <td>3.221675</td>\n",
       "      <td>HG00261</td>\n",
       "      <td>GBR</td>\n",
       "      <td>0.313768</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ENSG00000187961</td>\n",
       "      <td>1</td>\n",
       "      <td>900741</td>\n",
       "      <td>9</td>\n",
       "      <td>3</td>\n",
       "      <td>0.0010</td>\n",
       "      <td>0.0010</td>\n",
       "      <td>0.0378</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>3.478711</td>\n",
       "      <td>HG00261</td>\n",
       "      <td>GBR</td>\n",
       "      <td>0.313768</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            geneID  chrN     pos  refCount  altCount     MAF  min_MAF  \\\n",
       "0  ENSG00000188976     1  881627        93        67  0.3668      NaN   \n",
       "1  ENSG00000188976     1  889876         7         3  0.0159   0.0159   \n",
       "2  ENSG00000187961     1  896064         0         1  0.0378   0.0159   \n",
       "3  ENSG00000187961     1  897730        11         2  0.0388   0.0378   \n",
       "4  ENSG00000187961     1  900741         9         3  0.0010   0.0010   \n",
       "\n",
       "   diff_MAF    d     r2  log10_distance individual ancestry  qb_label  \n",
       "0       NaN  NaN    NaN             NaN    HG00261      GBR  0.431798  \n",
       "1    0.3509  1.0  0.027        3.916401    HG00261      GBR  0.431798  \n",
       "2    0.0219  NaN    NaN             NaN    HG00261      GBR  0.313768  \n",
       "3    0.0010  1.0  1.000        3.221675    HG00261      GBR  0.313768  \n",
       "4    0.0378  1.0  0.000        3.478711    HG00261      GBR  0.313768  "
      ]
     },
     "execution_count": 172,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>geneID</th>\n",
       "      <th>chrN</th>\n",
       "      <th>pos</th>\n",
       "      <th>refCount</th>\n",
       "      <th>altCount</th>\n",
       "      <th>MAF</th>\n",
       "      <th>min_MAF</th>\n",
       "      <th>diff_MAF</th>\n",
       "      <th>d</th>\n",
       "      <th>r2</th>\n",
       "      <th>log10_distance</th>\n",
       "      <th>individual</th>\n",
       "      <th>ancestry</th>\n",
       "      <th>qb_label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ENSG00000177757</td>\n",
       "      <td>1</td>\n",
       "      <td>753474</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.1620</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>HG00116</td>\n",
       "      <td>GBR</td>\n",
       "      <td>0.530147</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ENSG00000225880</td>\n",
       "      <td>1</td>\n",
       "      <td>761752</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.1292</td>\n",
       "      <td>0.1292</td>\n",
       "      <td>0.0328</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>HG00116</td>\n",
       "      <td>GBR</td>\n",
       "      <td>0.502619</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ENSG00000225880</td>\n",
       "      <td>1</td>\n",
       "      <td>762589</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0.1282</td>\n",
       "      <td>0.1282</td>\n",
       "      <td>0.0010</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.922725</td>\n",
       "      <td>HG00116</td>\n",
       "      <td>GBR</td>\n",
       "      <td>0.502619</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ENSG00000188976</td>\n",
       "      <td>1</td>\n",
       "      <td>887801</td>\n",
       "      <td>75</td>\n",
       "      <td>58</td>\n",
       "      <td>0.0477</td>\n",
       "      <td>0.0477</td>\n",
       "      <td>0.0805</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>HG00116</td>\n",
       "      <td>GBR</td>\n",
       "      <td>0.460115</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ENSG00000188976</td>\n",
       "      <td>1</td>\n",
       "      <td>888639</td>\n",
       "      <td>78</td>\n",
       "      <td>75</td>\n",
       "      <td>0.0467</td>\n",
       "      <td>0.0467</td>\n",
       "      <td>0.0010</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.923244</td>\n",
       "      <td>HG00116</td>\n",
       "      <td>GBR</td>\n",
       "      <td>0.460115</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            geneID  chrN     pos  refCount  altCount     MAF  min_MAF  \\\n",
       "0  ENSG00000177757     1  753474         0         1  0.1620      NaN   \n",
       "1  ENSG00000225880     1  761752         1         0  0.1292   0.1292   \n",
       "2  ENSG00000225880     1  762589         1         2  0.1282   0.1282   \n",
       "3  ENSG00000188976     1  887801        75        58  0.0477   0.0477   \n",
       "4  ENSG00000188976     1  888639        78        75  0.0467   0.0467   \n",
       "\n",
       "   diff_MAF    d   r2  log10_distance individual ancestry  qb_label  \n",
       "0       NaN  NaN  NaN             NaN    HG00116      GBR  0.530147  \n",
       "1    0.0328  NaN  NaN             NaN    HG00116      GBR  0.502619  \n",
       "2    0.0010  NaN  NaN        2.922725    HG00116      GBR  0.502619  \n",
       "3    0.0805  NaN  NaN             NaN    HG00116      GBR  0.460115  \n",
       "4    0.0010  1.0  1.0        2.923244    HG00116      GBR  0.460115  "
      ]
     },
     "execution_count": 173,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### modify the testset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_data_for_cnn_individual(data, feature_columns, label_col='qb_label'):\n",
    "    # Step 1: Determine the maximum number of hets per gene across individuals\n",
    "    max_hets_per_gene = data.groupby(['individual', 'geneID']).size().max()\n",
    "    \n",
    "    # Step 2: Calculate relative position for each gene by individual\n",
    "    # Sort by geneID and position for proper ordering\n",
    "    data = data.sort_values(by=['individual', 'geneID', 'pos'])\n",
    "    #data['relative_pos'] = data.groupby(['individual', 'geneID'])['pos'].diff().fillna(0)\n",
    "    data['log10_distance'] = data['log10_distance'].fillna(0)\n",
    "    # fill in d' values with 0.5 if nan\n",
    "    data['d'] = data['d'].fillna(0.5)\n",
    "    # fill in r2 values with 0.2-0.3 uniform distribution if nan\n",
    "    data['r2'] = data['r2'].fillna(np.random.uniform(0.2, 0.3))\n",
    "    # fill in NAn with 0 for min_MAF\n",
    "    data['min_MAF'] = data['min_MAF'].fillna(0)\n",
    "    # fill in NAn with 0 for diff_MAF\n",
    "    data['diff_MAF'] = data['diff_MAF'].fillna(0)\n",
    "    \n",
    "    # Step 3: Filter relevant columns - dropping geneID, chrN, and pos\n",
    "    X_data = []\n",
    "    y_data = []\n",
    "    \n",
    "    # Group by individual and geneID to form CNN inputs\n",
    "    grouped = data.groupby(['individual', 'geneID'])\n",
    "    \n",
    "    for (individual, geneID), group in grouped:\n",
    "        # Extract the feature columns for the current individual and gene\n",
    "        X = group[feature_columns].to_numpy()\n",
    "        \n",
    "        # Pad the feature array to have the same number of rows\n",
    "        padded_X = np.zeros((max_hets_per_gene, len(feature_columns)))\n",
    "        padded_X[:X.shape[0], :] = X  # Fill in the available data\n",
    "        \n",
    "        # Get the label (assumes it's the same across all rows in the group)\n",
    "        y = group[label_col].iloc[0]\n",
    "        \n",
    "        # Append to the lists\n",
    "        X_data.append(padded_X)\n",
    "        y_data.append(y)\n",
    "    \n",
    "    # Convert the lists to numpy arrays suitable for CNN input\n",
    "    X_data = np.array(X_data)\n",
    "    y_data = np.array(y_data)\n",
    "    \n",
    "    return X_data, y_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X shape: (589222, 62, 8)\n",
      "y shape: (589222,)\n"
     ]
    }
   ],
   "source": [
    "data_train = train_df\n",
    "feature_columns = ['refCount', 'altCount', 'MAF', 'min_MAF', 'diff_MAF', 'log10_distance', 'd', 'r2']\n",
    "X_train, y_train = prepare_data_for_cnn_individual(data_train, feature_columns)\n",
    "print(\"X shape:\", X_train.shape)\n",
    "print(\"y shape:\", y_train.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X test shape: (149173, 62, 8)\n",
      "y test shape: (149173,)\n"
     ]
    }
   ],
   "source": [
    "data_test = test_df\n",
    "X_test, y_test = prepare_data_for_cnn_individual(data_test, feature_columns)\n",
    "print(\"X test shape:\", X_test.shape)\n",
    "print(\"y test shape:\", y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>geneID</th>\n",
       "      <th>chrN</th>\n",
       "      <th>pos</th>\n",
       "      <th>refCount</th>\n",
       "      <th>altCount</th>\n",
       "      <th>MAF</th>\n",
       "      <th>min_MAF</th>\n",
       "      <th>diff_MAF</th>\n",
       "      <th>d</th>\n",
       "      <th>r2</th>\n",
       "      <th>log10_distance</th>\n",
       "      <th>individual</th>\n",
       "      <th>ancestry</th>\n",
       "      <th>qb_label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ENSG00000188976</td>\n",
       "      <td>1</td>\n",
       "      <td>881627</td>\n",
       "      <td>93</td>\n",
       "      <td>67</td>\n",
       "      <td>0.3668</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>HG00261</td>\n",
       "      <td>GBR</td>\n",
       "      <td>0.431798</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ENSG00000188976</td>\n",
       "      <td>1</td>\n",
       "      <td>889876</td>\n",
       "      <td>7</td>\n",
       "      <td>3</td>\n",
       "      <td>0.0159</td>\n",
       "      <td>0.0159</td>\n",
       "      <td>0.3509</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.027</td>\n",
       "      <td>3.916401</td>\n",
       "      <td>HG00261</td>\n",
       "      <td>GBR</td>\n",
       "      <td>0.431798</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ENSG00000187961</td>\n",
       "      <td>1</td>\n",
       "      <td>896064</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0378</td>\n",
       "      <td>0.0159</td>\n",
       "      <td>0.0219</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>HG00261</td>\n",
       "      <td>GBR</td>\n",
       "      <td>0.313768</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ENSG00000187961</td>\n",
       "      <td>1</td>\n",
       "      <td>897730</td>\n",
       "      <td>11</td>\n",
       "      <td>2</td>\n",
       "      <td>0.0388</td>\n",
       "      <td>0.0378</td>\n",
       "      <td>0.0010</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.000</td>\n",
       "      <td>3.221675</td>\n",
       "      <td>HG00261</td>\n",
       "      <td>GBR</td>\n",
       "      <td>0.313768</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ENSG00000187961</td>\n",
       "      <td>1</td>\n",
       "      <td>900741</td>\n",
       "      <td>9</td>\n",
       "      <td>3</td>\n",
       "      <td>0.0010</td>\n",
       "      <td>0.0010</td>\n",
       "      <td>0.0378</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>3.478711</td>\n",
       "      <td>HG00261</td>\n",
       "      <td>GBR</td>\n",
       "      <td>0.313768</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            geneID  chrN     pos  refCount  altCount     MAF  min_MAF  \\\n",
       "0  ENSG00000188976     1  881627        93        67  0.3668      NaN   \n",
       "1  ENSG00000188976     1  889876         7         3  0.0159   0.0159   \n",
       "2  ENSG00000187961     1  896064         0         1  0.0378   0.0159   \n",
       "3  ENSG00000187961     1  897730        11         2  0.0388   0.0378   \n",
       "4  ENSG00000187961     1  900741         9         3  0.0010   0.0010   \n",
       "\n",
       "   diff_MAF    d     r2  log10_distance individual ancestry  qb_label  \n",
       "0       NaN  NaN    NaN             NaN    HG00261      GBR  0.431798  \n",
       "1    0.3509  1.0  0.027        3.916401    HG00261      GBR  0.431798  \n",
       "2    0.0219  NaN    NaN             NaN    HG00261      GBR  0.313768  \n",
       "3    0.0010  1.0  1.000        3.221675    HG00261      GBR  0.313768  \n",
       "4    0.0378  1.0  0.000        3.478711    HG00261      GBR  0.313768  "
      ]
     },
     "execution_count": 162,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GeneCNN(nn.Module):\n",
    "    def __init__(self, input_features):\n",
    "        super(GeneCNN, self).__init__()\n",
    "        \n",
    "        # Convolutional and pooling layers\n",
    "        ## the number of filters in the 1st conv layer is 32 (common choice 16-64)\n",
    "        self.conv1 = nn.Conv1d(in_channels = input_features, out_channels=32, kernel_size=2)\n",
    "        self.pool1 = nn.MaxPool1d(kernel_size=2)\n",
    "        \n",
    "        ## the number of filters in the 2nd conv layer is 32 (common choice 16-64)\n",
    "        self.conv2 = nn.Conv1d(in_channels=32, out_channels=64, kernel_size=2)\n",
    "        self.pool2 = nn.MaxPool1d(kernel_size=2)\n",
    "        \n",
    "        ## the number of filters in the 3rd conv layer is 64 (common choice 16-64)\n",
    "        self.conv3 = nn.Conv1d(in_channels=64, out_channels=128, kernel_size=2)\n",
    "        \n",
    "        # Adaptive pooling layer to reduce size to 1\n",
    "        self.global_pool = nn.AdaptiveAvgPool1d(1)\n",
    "        \n",
    "        # Fully connected layers\n",
    "        self.fc1 = nn.Linear(128, 64)  # Adjusted input size to match the output channels from global_pool\n",
    "        self.fc2 = nn.Linear(64, 8)\n",
    "        self.fc3 = nn.Linear(8, 1)  # Single output neuron for regression\n",
    "        \n",
    "        # Dropout layer for regularization\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "\n",
    "        # # Add two batch normalization layers\n",
    "        # self.bn1 = nn.BatchNorm1d(128)\n",
    "        # self.bn2 = nn.BatchNorm1d(8)\n",
    "\n",
    "        # # Apply He initialization\n",
    "        # init.kaiming_uniform_(self.fc1.weight)\n",
    "        # init.kaiming_uniform_(self.fc2.weight)\n",
    "        # init.kaiming_uniform_(self.fc3.weight, nonlinearity=\"sigmoid\")\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Transpose to match Conv1d input format: (batch_size, features, length)\n",
    "        x = x.transpose(1, 2)\n",
    "        \n",
    "        # Convolution and pooling layers\n",
    "        x = self.pool1(F.relu(self.conv1(x)))  # Conv1 and pooling\n",
    "        #print(f\"Shape after conv1 and pool1: {x.shape}\")\n",
    "        \n",
    "        x = self.pool2(F.relu(self.conv2(x)))  # Conv2 and pooling\n",
    "        #print(f\"Shape after conv2 and pool2: {x.shape}\")\n",
    "        \n",
    "        x = F.relu(self.conv3(x))              # Conv3\n",
    "        #print(f\"Shape after conv3: {x.shape}\")\n",
    "        \n",
    "        x = self.global_pool(x)                # Adaptive pooling to size 1\n",
    "        #print(f\"Shape after global_pool: {x.shape}\")\n",
    "        \n",
    "        # Flatten and pass through fully connected layers\n",
    "        x = x.view(x.size(0), -1)              # Flatten\n",
    "        #print(f\"Shape after flattening: {x.shape}\")\n",
    "        \n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.dropout(x)\n",
    "        x = F.relu(self.fc2(x))\n",
    "        \n",
    "        # Output layer for regression (no activation or add sigmoid if bounded)\n",
    "        x = self.fc3(x)\n",
    "        \n",
    "        return x\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8"
      ]
     },
     "execution_count": 164,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GeneCNN(\n",
      "  (conv1): Conv1d(8, 32, kernel_size=(2,), stride=(1,))\n",
      "  (pool1): MaxPool1d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (conv2): Conv1d(32, 64, kernel_size=(2,), stride=(1,))\n",
      "  (pool2): MaxPool1d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (conv3): Conv1d(64, 128, kernel_size=(2,), stride=(1,))\n",
      "  (global_pool): AdaptiveAvgPool1d(output_size=1)\n",
      "  (fc1): Linear(in_features=128, out_features=64, bias=True)\n",
      "  (fc2): Linear(in_features=64, out_features=8, bias=True)\n",
      "  (fc3): Linear(in_features=8, out_features=1, bias=True)\n",
      "  (dropout): Dropout(p=0.5, inplace=False)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# Instantiate the model, loss function, and optimizer\n",
    "max_hets_per_gene = X_train.shape[2]  # Use the maximum number of hets per gene\n",
    "model = GeneCNN(input_features = max_hets_per_gene) # 6 features\n",
    "\n",
    "# Model summary\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming X_train and y_train are initially your full training data\n",
    "# Set aside a portion for validation\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.25, random_state=42, shuffle=True)\n",
    "\n",
    "# Convert them to PyTorch tensors\n",
    "X_train = torch.tensor(X_train, dtype=torch.float32)\n",
    "y_train = torch.tensor(y_train, dtype=torch.float32).unsqueeze(1)\n",
    "X_val = torch.tensor(X_val, dtype=torch.float32)\n",
    "y_val = torch.tensor(y_val, dtype=torch.float32).unsqueeze(1)\n",
    "\n",
    "X_test = torch.tensor(X_test, dtype=torch.float32)\n",
    "y_test = torch.tensor(y_test, dtype=torch.float32).unsqueeze(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a dataset and a DataLoader with a smaller batch size\n",
    "train_dataset = TensorDataset(X_train, y_train)\n",
    "val_dataset = TensorDataset(X_val, y_val)\n",
    "train_loader = DataLoader(train_dataset, batch_size=128, shuffle=True)  # Adjust batch size as needed\n",
    "val_loader = DataLoader(val_dataset, batch_size=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NaNs in X_train: tensor(False)\n",
      "NaNs in y_train: tensor(False)\n",
      "Infs in X_train: tensor(False)\n",
      "Infs in y_train: tensor(False)\n",
      "NaNs in X_val: tensor(False)\n",
      "NaNs in y_val: tensor(False)\n",
      "Infs in X_val: tensor(False)\n",
      "Infs in y_val: tensor(False)\n"
     ]
    }
   ],
   "source": [
    "# Check for NaNs or Infs in input and target data\n",
    "print(\"NaNs in X_train:\", torch.isnan(X_train).any())\n",
    "print(\"NaNs in y_train:\", torch.isnan(y_train).any())\n",
    "print(\"Infs in X_train:\", torch.isinf(X_train).any())\n",
    "print(\"Infs in y_train:\", torch.isinf(y_train).any())\n",
    "\n",
    "print(\"NaNs in X_val:\", torch.isnan(X_val).any())\n",
    "print(\"NaNs in y_val:\", torch.isnan(y_val).any())\n",
    "print(\"Infs in X_val:\", torch.isinf(X_val).any())\n",
    "print(\"Infs in y_val:\", torch.isinf(y_val).any())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/scarlett/.local/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:60: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/100], Train RMSE: 0.0872, Validation RMSE: 0.0494\n",
      "Epoch [2/100], Train RMSE: 0.0517, Validation RMSE: 0.0457\n",
      "Epoch [3/100], Train RMSE: 0.0494, Validation RMSE: 0.0442\n",
      "Epoch [4/100], Train RMSE: 0.0461, Validation RMSE: 0.0444\n",
      "Epoch [5/100], Train RMSE: 0.0453, Validation RMSE: 0.0425\n",
      "Epoch [6/100], Train RMSE: 0.0447, Validation RMSE: 0.0433\n",
      "Epoch [7/100], Train RMSE: 0.0438, Validation RMSE: 0.0417\n",
      "Epoch [8/100], Train RMSE: 0.0435, Validation RMSE: 0.0414\n",
      "Epoch [9/100], Train RMSE: 0.0431, Validation RMSE: 0.0416\n",
      "Epoch [10/100], Train RMSE: 0.0427, Validation RMSE: 0.0415\n",
      "Epoch [11/100], Train RMSE: 0.0423, Validation RMSE: 0.0426\n",
      "Epoch [12/100], Train RMSE: 0.0421, Validation RMSE: 0.0418\n",
      "Epoch [13/100], Train RMSE: 0.0413, Validation RMSE: 0.0416\n",
      "Epoch [14/100], Train RMSE: 0.0413, Validation RMSE: 0.0416\n",
      "Epoch [15/100], Train RMSE: 0.0412, Validation RMSE: 0.0410\n",
      "Epoch [16/100], Train RMSE: 0.0410, Validation RMSE: 0.0418\n",
      "Epoch [17/100], Train RMSE: 0.0410, Validation RMSE: 0.0408\n",
      "Epoch [18/100], Train RMSE: 0.0410, Validation RMSE: 0.0410\n",
      "Epoch [19/100], Train RMSE: 0.0409, Validation RMSE: 0.0412\n",
      "Epoch [20/100], Train RMSE: 0.0408, Validation RMSE: 0.0410\n",
      "Epoch [21/100], Train RMSE: 0.0407, Validation RMSE: 0.0410\n",
      "Epoch [22/100], Train RMSE: 0.0404, Validation RMSE: 0.0406\n",
      "Epoch [23/100], Train RMSE: 0.0403, Validation RMSE: 0.0409\n",
      "Epoch [24/100], Train RMSE: 0.0402, Validation RMSE: 0.0405\n",
      "Epoch [25/100], Train RMSE: 0.0401, Validation RMSE: 0.0412\n",
      "Epoch [26/100], Train RMSE: 0.0401, Validation RMSE: 0.0408\n",
      "Epoch [27/100], Train RMSE: 0.0401, Validation RMSE: 0.0408\n",
      "Epoch [28/100], Train RMSE: 0.0400, Validation RMSE: 0.0414\n",
      "Epoch [29/100], Train RMSE: 0.0399, Validation RMSE: 0.0406\n",
      "Epoch [30/100], Train RMSE: 0.0398, Validation RMSE: 0.0407\n",
      "Epoch [31/100], Train RMSE: 0.0398, Validation RMSE: 0.0408\n",
      "Epoch [32/100], Train RMSE: 0.0397, Validation RMSE: 0.0407\n",
      "Epoch [33/100], Train RMSE: 0.0396, Validation RMSE: 0.0405\n",
      "Epoch [34/100], Train RMSE: 0.0397, Validation RMSE: 0.0408\n",
      "Epoch [35/100], Train RMSE: 0.0396, Validation RMSE: 0.0406\n",
      "Epoch [36/100], Train RMSE: 0.0396, Validation RMSE: 0.0407\n",
      "Epoch [37/100], Train RMSE: 0.0395, Validation RMSE: 0.0406\n",
      "Epoch [38/100], Train RMSE: 0.0395, Validation RMSE: 0.0406\n",
      "Epoch [39/100], Train RMSE: 0.0396, Validation RMSE: 0.0406\n",
      "Epoch [40/100], Train RMSE: 0.0395, Validation RMSE: 0.0408\n",
      "Epoch [41/100], Train RMSE: 0.0395, Validation RMSE: 0.0407\n",
      "Epoch [42/100], Train RMSE: 0.0394, Validation RMSE: 0.0407\n",
      "Epoch [43/100], Train RMSE: 0.0394, Validation RMSE: 0.0406\n",
      "Epoch [44/100], Train RMSE: 0.0394, Validation RMSE: 0.0406\n",
      "Epoch [45/100], Train RMSE: 0.0394, Validation RMSE: 0.0405\n",
      "Epoch [46/100], Train RMSE: 0.0394, Validation RMSE: 0.0405\n",
      "Epoch [47/100], Train RMSE: 0.0394, Validation RMSE: 0.0406\n",
      "Epoch [48/100], Train RMSE: 0.0394, Validation RMSE: 0.0407\n",
      "Epoch [49/100], Train RMSE: 0.0394, Validation RMSE: 0.0406\n",
      "Epoch [50/100], Train RMSE: 0.0394, Validation RMSE: 0.0406\n",
      "Epoch [51/100], Train RMSE: 0.0394, Validation RMSE: 0.0406\n",
      "Epoch [52/100], Train RMSE: 0.0394, Validation RMSE: 0.0406\n",
      "Epoch [53/100], Train RMSE: 0.0394, Validation RMSE: 0.0406\n",
      "Epoch [54/100], Train RMSE: 0.0394, Validation RMSE: 0.0406\n",
      "Epoch [55/100], Train RMSE: 0.0394, Validation RMSE: 0.0406\n",
      "Epoch [56/100], Train RMSE: 0.0394, Validation RMSE: 0.0406\n",
      "Epoch [57/100], Train RMSE: 0.0394, Validation RMSE: 0.0406\n",
      "Epoch [58/100], Train RMSE: 0.0394, Validation RMSE: 0.0406\n",
      "Epoch [59/100], Train RMSE: 0.0394, Validation RMSE: 0.0406\n",
      "Epoch [60/100], Train RMSE: 0.0394, Validation RMSE: 0.0406\n",
      "Epoch [61/100], Train RMSE: 0.0394, Validation RMSE: 0.0406\n",
      "Epoch [62/100], Train RMSE: 0.0393, Validation RMSE: 0.0406\n",
      "Epoch [63/100], Train RMSE: 0.0394, Validation RMSE: 0.0406\n",
      "Epoch [64/100], Train RMSE: 0.0393, Validation RMSE: 0.0406\n",
      "Epoch [65/100], Train RMSE: 0.0394, Validation RMSE: 0.0406\n",
      "Epoch [66/100], Train RMSE: 0.0394, Validation RMSE: 0.0406\n",
      "Epoch [67/100], Train RMSE: 0.0393, Validation RMSE: 0.0406\n",
      "Epoch [68/100], Train RMSE: 0.0394, Validation RMSE: 0.0406\n",
      "Epoch [69/100], Train RMSE: 0.0394, Validation RMSE: 0.0406\n",
      "Epoch [70/100], Train RMSE: 0.0393, Validation RMSE: 0.0406\n",
      "Epoch [71/100], Train RMSE: 0.0393, Validation RMSE: 0.0406\n",
      "Epoch [72/100], Train RMSE: 0.0394, Validation RMSE: 0.0406\n",
      "Epoch [73/100], Train RMSE: 0.0393, Validation RMSE: 0.0406\n",
      "Epoch [74/100], Train RMSE: 0.0394, Validation RMSE: 0.0406\n",
      "Epoch [75/100], Train RMSE: 0.0394, Validation RMSE: 0.0406\n",
      "Epoch [76/100], Train RMSE: 0.0394, Validation RMSE: 0.0406\n",
      "Epoch [77/100], Train RMSE: 0.0393, Validation RMSE: 0.0406\n",
      "Epoch [78/100], Train RMSE: 0.0394, Validation RMSE: 0.0406\n",
      "Epoch [79/100], Train RMSE: 0.0394, Validation RMSE: 0.0406\n",
      "Epoch [80/100], Train RMSE: 0.0393, Validation RMSE: 0.0406\n",
      "Epoch [81/100], Train RMSE: 0.0394, Validation RMSE: 0.0406\n",
      "Epoch [82/100], Train RMSE: 0.0394, Validation RMSE: 0.0406\n",
      "Epoch [83/100], Train RMSE: 0.0393, Validation RMSE: 0.0406\n",
      "Epoch [84/100], Train RMSE: 0.0393, Validation RMSE: 0.0406\n",
      "Epoch [85/100], Train RMSE: 0.0393, Validation RMSE: 0.0406\n",
      "Epoch [86/100], Train RMSE: 0.0393, Validation RMSE: 0.0406\n",
      "Epoch [87/100], Train RMSE: 0.0394, Validation RMSE: 0.0406\n",
      "Epoch [88/100], Train RMSE: 0.0394, Validation RMSE: 0.0406\n",
      "Epoch [89/100], Train RMSE: 0.0394, Validation RMSE: 0.0406\n",
      "Epoch [90/100], Train RMSE: 0.0394, Validation RMSE: 0.0406\n",
      "Epoch [91/100], Train RMSE: 0.0394, Validation RMSE: 0.0406\n",
      "Epoch [92/100], Train RMSE: 0.0394, Validation RMSE: 0.0406\n",
      "Epoch [93/100], Train RMSE: 0.0394, Validation RMSE: 0.0406\n",
      "Epoch [94/100], Train RMSE: 0.0394, Validation RMSE: 0.0406\n",
      "Epoch [95/100], Train RMSE: 0.0394, Validation RMSE: 0.0406\n",
      "Epoch [96/100], Train RMSE: 0.0394, Validation RMSE: 0.0406\n",
      "Epoch [97/100], Train RMSE: 0.0394, Validation RMSE: 0.0406\n",
      "Epoch [98/100], Train RMSE: 0.0394, Validation RMSE: 0.0406\n",
      "Epoch [99/100], Train RMSE: 0.0393, Validation RMSE: 0.0406\n",
      "Epoch [100/100], Train RMSE: 0.0394, Validation RMSE: 0.0406\n"
     ]
    }
   ],
   "source": [
    "# Check if GPU is available\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Initialize the model\n",
    "model = GeneCNN(input_features=max_hets_per_gene).to(device)  # Change input_features to the correct feature count\n",
    "\n",
    "# Define loss function, optimizer, and optional scheduler\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.0005)\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', patience=3, factor=0.5, verbose=True)\n",
    "\n",
    "# Lists to store the training and validation losses\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "\n",
    "# Training loop\n",
    "epochs = 100\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    train_loss_epoch = 0.0\n",
    "    \n",
    "    for X_batch, y_batch in train_loader:\n",
    "        X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Forward pass\n",
    "        outputs = model(X_batch)\n",
    "        train_mse_loss = criterion(outputs, y_batch)  # MSE Loss\n",
    "        train_rmse_loss = torch.sqrt(train_mse_loss)  # RMSE\n",
    "        \n",
    "        # Backward pass\n",
    "        train_mse_loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "        optimizer.step()\n",
    "        \n",
    "        train_loss_epoch += train_mse_loss.item()  # Accumulate MSE loss\n",
    "    \n",
    "    # Validation\n",
    "    model.eval()\n",
    "    val_loss_epoch = 0.0\n",
    "    with torch.no_grad():\n",
    "        for X_val_batch, y_val_batch in val_loader:\n",
    "            X_val_batch, y_val_batch = X_val_batch.to(device), y_val_batch.to(device)\n",
    "            val_outputs = model(X_val_batch)\n",
    "            val_mse_loss = criterion(val_outputs, y_val_batch)\n",
    "            val_loss_epoch += val_mse_loss.item()  # Accumulate MSE loss\n",
    "\n",
    "    # Calculate average RMSE for the epoch\n",
    "    avg_train_rmse = torch.sqrt(torch.tensor(train_loss_epoch / len(train_loader))).item()\n",
    "    avg_val_rmse = torch.sqrt(torch.tensor(val_loss_epoch / len(val_loader))).item()\n",
    "\n",
    "    # Store the RMSE losses for each epoch\n",
    "    train_losses.append(avg_train_rmse)\n",
    "    val_losses.append(avg_val_rmse)\n",
    "    \n",
    "    # Step the scheduler based on validation loss\n",
    "    scheduler.step(avg_val_rmse)\n",
    "    \n",
    "    print(f\"Epoch [{epoch+1}/{epochs}], Train RMSE: {avg_train_rmse:.4f}, Validation RMSE: {avg_val_rmse:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### RELU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x500 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(range(1, epochs + 1), train_losses, label='Training RMSE')\n",
    "plt.plot(range(1, epochs + 1), val_losses, label='Validation RMSE')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('RMSE')\n",
    "plt.title('Training and Validation RMSE')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final Test RMSE: 0.0416\n"
     ]
    }
   ],
   "source": [
    "# Create a DataLoader for the test set with a smaller batch size\n",
    "test_dataset = TensorDataset(X_test, y_test)\n",
    "test_loader = DataLoader(test_dataset, batch_size=64)  # Adjust batch size as needed\n",
    "\n",
    "# Final evaluation on test set\n",
    "model.eval()\n",
    "test_loss_total = 0.0\n",
    "with torch.no_grad():\n",
    "    for X_batch, y_batch in test_loader:\n",
    "        X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
    "        test_outputs = model(X_batch)\n",
    "        test_loss = criterion(test_outputs, y_batch)  # MSE Loss\n",
    "        test_loss_total += test_loss.item() * X_batch.size(0)  # Sum up the batch loss (weighted by batch size)\n",
    "\n",
    "# Calculate the average loss and take the square root for RMSE\n",
    "average_test_mse = test_loss_total / len(test_loader.dataset)\n",
    "average_test_rmse = torch.sqrt(torch.tensor(average_test_mse)).item()\n",
    "print(f\"Final Test RMSE: {average_test_rmse:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simulator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [],
   "source": [
    "def simulate_reads_single_gene(p=0.5, read_coverage=100, phasing_error=0.04, num_hets=10, max_hets=64, data=None):\n",
    "    np.random.seed(42)  # For reproducibility\n",
    "\n",
    "    # Initialize lists to store simulated data\n",
    "    ref_counts = []\n",
    "    alt_counts = []\n",
    "    switches = []\n",
    "\n",
    "    # Sample MAF and log10_distance for each het site\n",
    "    if data is None:\n",
    "        raise ValueError(\"MAF and log10_distance data must be provided.\")\n",
    "    \n",
    "\n",
    "    # Sample MAF and log10_distance for each het site in the gene\n",
    "    samples = data.sample(n=num_hets, replace=True)\n",
    "    # fill in missing data\n",
    "    samples['log10_distance'] = samples['log10_distance'].fillna(0)\n",
    "    # fill in d' values with 0.5 if nan\n",
    "    samples['d'] = samples['d'].fillna(0.5)\n",
    "    # fill in r2 values with 0.2-0.3 uniform distribution if nan\n",
    "    samples['r2'] = samples['r2'].fillna(np.random.uniform(0.2, 0.3))\n",
    "    # fill in NAn with 0 for min_MAF\n",
    "    samples['min_MAF'] = samples['min_MAF'].fillna(0)\n",
    "    # fill in NAn with 0 for diff_MAF\n",
    "    samples['diff_MAF'] = samples['diff_MAF'].fillna(0)\n",
    "\n",
    "    maf_values = samples['MAF'].values\n",
    "    min_maf_values = samples['min_MAF'].values\n",
    "    diff_maf_values = samples['diff_MAF'].values\n",
    "    log10_distance_values = samples['log10_distance'].values\n",
    "    d_values = samples['d'].values\n",
    "    r2_values = samples['r2'].values\n",
    "\n",
    "    # Convert log10_distance to distance and handle NaN by replacing with mean distance\n",
    "    #distance_values = np.where(~np.isnan(log10_distance_values), 10**log10_distance_values, np.nan)\n",
    "    #mean_distance = np.nanmean(distance_values)\n",
    "    #distance_values = np.where(np.isnan(distance_values), mean_distance, distance_values)\n",
    "\n",
    "    current_phase = False  # Start without phase flipping\n",
    "\n",
    "    # Loop over each heterozygous site\n",
    "    for het in range(num_hets):\n",
    "        # Simulate binomial read counts based on the binomial probability\n",
    "        ref_count = np.random.binomial(n=read_coverage, p=p)  # Total ref count for this het\n",
    "        alt_count = read_coverage - ref_count  # Total alt count for this het\n",
    "\n",
    "        # Generate a random phasing error to determine if a phase switch occurs\n",
    "        phase_switch = np.random.rand() < phasing_error\n",
    "        \n",
    "        # If a phasing error occurred, toggle the current phase\n",
    "        if phase_switch:\n",
    "            current_phase = not current_phase\n",
    "\n",
    "        # If current phase is switched, swap the ref and alt counts\n",
    "        if current_phase:\n",
    "            ref_count, alt_count = alt_count, ref_count\n",
    "            switches.append(1)  # Indicate a switch\n",
    "        else:\n",
    "            switches.append(0)  # No switch\n",
    "\n",
    "        # Store accumulated values for this het site\n",
    "        ref_counts.append(ref_count)\n",
    "        alt_counts.append(alt_count)\n",
    "        # mafs.append(maf_values[het])\n",
    "\n",
    "    # feature_columns = ['refCount', 'altCount', 'MAF', 'min_MAF', 'diff_MAF', 'log10_distance', 'd', 'r2']\n",
    "    # Create DataFrame for model input with the four features\n",
    "    data = {\n",
    "        'refCount': ref_counts,\n",
    "        'altCount': alt_counts,\n",
    "        'MAF': maf_values,\n",
    "        'min_MAF': min_maf_values,\n",
    "        'diff_MAF': diff_maf_values,\n",
    "        'log10_distance': log10_distance_values,\n",
    "        'd': d_values,\n",
    "        'r2': r2_values,\n",
    "        'switch': switches\n",
    "    }\n",
    "    df_input = pd.DataFrame(data)\n",
    "    \n",
    "    # Pad the DataFrame to have max_hets rows\n",
    "    padded_data = np.zeros((max_hets, len(df_input.columns) - 1))  # Exclude the 'switch' column from padding\n",
    "    padded_data[:df_input.shape[0], :] = df_input.drop(columns=['switch']).values  # Fill in available data and zero-pad remaining rows\n",
    "    \n",
    "    # Convert padded data to numpy arrays similar to `prepare_data_for_cnn_individual` output\n",
    "    X_data = np.array([padded_data])  # Add batch dimension to match the expected CNN input format\n",
    "    y_data = np.array([p])  # Here, the `p` is used as a placeholder for the label\n",
    "    \n",
    "    return X_data, y_data, df_input  # Return DataFrame for visualization as well\n",
    "\n",
    "\n",
    "\n",
    "def evaluate_simulation(X_sim, y_sim, model, device='cpu'):\n",
    "    \"\"\"\n",
    "    Runs the model on simulated data, calculates the predicted binomial p values, \n",
    "    and returns the RMSE compared to the true p values.\n",
    "\n",
    "    Parameters:\n",
    "    - X_sim: np.array, simulated input features\n",
    "    - y_sim: np.array, true binomial p values\n",
    "    - model: PyTorch model, the trained model to use for predictions\n",
    "    - device: str, device to run the model on ('cpu' or 'cuda')\n",
    "\n",
    "    Returns:\n",
    "    - predicted_p_values: np.array, the predicted binomial p values\n",
    "    - rmse: float, root mean squared error between the predicted and true p values\n",
    "    \"\"\"\n",
    "\n",
    "    # Convert X_sim and y_sim to PyTorch tensors\n",
    "    X_sim_tensor = torch.tensor(X_sim, dtype=torch.float32).to(device)\n",
    "    y_sim_tensor = torch.tensor(y_sim, dtype=torch.float32).unsqueeze(1).to(device)\n",
    "\n",
    "    # Set the model to evaluation mode\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        # Run the model to get the predicted binomial p values\n",
    "        predicted_p = model(X_sim_tensor)\n",
    "        \n",
    "        # Print the predicted binomial p values for inspection\n",
    "        print(\"Predicted binomial p values:\")\n",
    "        print(predicted_p.cpu().numpy())\n",
    "        \n",
    "        # Calculate the Mean Squared Error (MSE)\n",
    "        mse_loss = nn.MSELoss()(predicted_p, y_sim_tensor)\n",
    "        \n",
    "        # Calculate the RMSE\n",
    "        rmse = torch.sqrt(mse_loss).item()\n",
    "        print(f\"Root Mean Squared Error (RMSE): {rmse:.4f}\")\n",
    "    \n",
    "    # Return the predicted p values and RMSE\n",
    "    return predicted_p.cpu().numpy(), rmse\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted binomial p values:\n",
      "[[0.5018013]]\n",
      "Root Mean Squared Error (RMSE): 0.0018\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>refCount</th>\n",
       "      <th>altCount</th>\n",
       "      <th>MAF</th>\n",
       "      <th>min_MAF</th>\n",
       "      <th>diff_MAF</th>\n",
       "      <th>log10_distance</th>\n",
       "      <th>d</th>\n",
       "      <th>r2</th>\n",
       "      <th>switch</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>7</td>\n",
       "      <td>13</td>\n",
       "      <td>0.2893</td>\n",
       "      <td>0.2893</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>1.995635</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>11</td>\n",
       "      <td>9</td>\n",
       "      <td>0.1203</td>\n",
       "      <td>0.1143</td>\n",
       "      <td>0.0060</td>\n",
       "      <td>2.107210</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.912000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5</td>\n",
       "      <td>15</td>\n",
       "      <td>0.0636</td>\n",
       "      <td>0.0636</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>4.511456</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>12</td>\n",
       "      <td>8</td>\n",
       "      <td>0.4652</td>\n",
       "      <td>0.1876</td>\n",
       "      <td>0.2776</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.500</td>\n",
       "      <td>0.215599</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>8</td>\n",
       "      <td>12</td>\n",
       "      <td>0.4911</td>\n",
       "      <td>0.4583</td>\n",
       "      <td>0.0328</td>\n",
       "      <td>3.943890</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.876000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>9</td>\n",
       "      <td>11</td>\n",
       "      <td>0.1256</td>\n",
       "      <td>0.1256</td>\n",
       "      <td>0.3388</td>\n",
       "      <td>3.283979</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.146000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>0.4851</td>\n",
       "      <td>0.0895</td>\n",
       "      <td>0.3956</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.500</td>\n",
       "      <td>0.215599</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>11</td>\n",
       "      <td>9</td>\n",
       "      <td>0.4205</td>\n",
       "      <td>0.0954</td>\n",
       "      <td>0.3251</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.500</td>\n",
       "      <td>0.215599</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>9</td>\n",
       "      <td>11</td>\n",
       "      <td>0.3141</td>\n",
       "      <td>0.3131</td>\n",
       "      <td>0.0010</td>\n",
       "      <td>5.762318</td>\n",
       "      <td>0.705</td>\n",
       "      <td>0.433000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>0.3062</td>\n",
       "      <td>0.2028</td>\n",
       "      <td>0.1034</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.500</td>\n",
       "      <td>0.215599</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   refCount  altCount     MAF  min_MAF  diff_MAF  log10_distance      d  \\\n",
       "0         7        13  0.2893   0.2893    0.0000        1.995635  1.000   \n",
       "1        11         9  0.1203   0.1143    0.0060        2.107210  1.000   \n",
       "2         5        15  0.0636   0.0636    0.0000        4.511456  1.000   \n",
       "3        12         8  0.4652   0.1876    0.2776        0.000000  0.500   \n",
       "4         8        12  0.4911   0.4583    0.0328        3.943890  1.000   \n",
       "5         9        11  0.1256   0.1256    0.3388        3.283979  1.000   \n",
       "6        10        10  0.4851   0.0895    0.3956        0.000000  0.500   \n",
       "7        11         9  0.4205   0.0954    0.3251        0.000000  0.500   \n",
       "8         9        11  0.3141   0.3131    0.0010        5.762318  0.705   \n",
       "9        10        10  0.3062   0.2028    0.1034        0.000000  0.500   \n",
       "\n",
       "         r2  switch  \n",
       "0  1.000000       0  \n",
       "1  0.912000       0  \n",
       "2  1.000000       0  \n",
       "3  0.215599       0  \n",
       "4  0.876000       0  \n",
       "5  0.146000       0  \n",
       "6  0.215599       0  \n",
       "7  0.215599       0  \n",
       "8  0.433000       0  \n",
       "9  0.215599       0  "
      ]
     },
     "execution_count": 180,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Simulate data using the function\n",
    "X_sim, y_sim, df_vis = simulate_reads_single_gene(\n",
    "    p=0.5, read_coverage=20, phasing_error=0.04, num_hets=10, max_hets=64, data=train_df\n",
    ")\n",
    "\n",
    "# Call the evaluation function\n",
    "predicted_p_values, rmse = evaluate_simulation(X_sim, y_sim, model, device)\n",
    "\n",
    "# Visualize the simulated data if needed\n",
    "df_vis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted binomial p values:\n",
      "[[0.49537563]]\n",
      "Root Mean Squared Error (RMSE): 0.0046\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>refCount</th>\n",
       "      <th>altCount</th>\n",
       "      <th>MAF</th>\n",
       "      <th>min_MAF</th>\n",
       "      <th>diff_MAF</th>\n",
       "      <th>log10_distance</th>\n",
       "      <th>d</th>\n",
       "      <th>r2</th>\n",
       "      <th>switch</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>44</td>\n",
       "      <td>56</td>\n",
       "      <td>0.2893</td>\n",
       "      <td>0.2893</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>1.995635</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>52</td>\n",
       "      <td>48</td>\n",
       "      <td>0.1203</td>\n",
       "      <td>0.1143</td>\n",
       "      <td>0.0060</td>\n",
       "      <td>2.107210</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.912000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>58</td>\n",
       "      <td>42</td>\n",
       "      <td>0.0636</td>\n",
       "      <td>0.0636</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>4.511456</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>51</td>\n",
       "      <td>49</td>\n",
       "      <td>0.4652</td>\n",
       "      <td>0.1876</td>\n",
       "      <td>0.2776</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.500</td>\n",
       "      <td>0.215599</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>56</td>\n",
       "      <td>44</td>\n",
       "      <td>0.4911</td>\n",
       "      <td>0.4583</td>\n",
       "      <td>0.0328</td>\n",
       "      <td>3.943890</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.876000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>50</td>\n",
       "      <td>50</td>\n",
       "      <td>0.1256</td>\n",
       "      <td>0.1256</td>\n",
       "      <td>0.3388</td>\n",
       "      <td>3.283979</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.146000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>49</td>\n",
       "      <td>51</td>\n",
       "      <td>0.4851</td>\n",
       "      <td>0.0895</td>\n",
       "      <td>0.3956</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.500</td>\n",
       "      <td>0.215599</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>46</td>\n",
       "      <td>54</td>\n",
       "      <td>0.4205</td>\n",
       "      <td>0.0954</td>\n",
       "      <td>0.3251</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.500</td>\n",
       "      <td>0.215599</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>43</td>\n",
       "      <td>57</td>\n",
       "      <td>0.3141</td>\n",
       "      <td>0.3131</td>\n",
       "      <td>0.0010</td>\n",
       "      <td>5.762318</td>\n",
       "      <td>0.705</td>\n",
       "      <td>0.433000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>57</td>\n",
       "      <td>43</td>\n",
       "      <td>0.3062</td>\n",
       "      <td>0.2028</td>\n",
       "      <td>0.1034</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.500</td>\n",
       "      <td>0.215599</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   refCount  altCount     MAF  min_MAF  diff_MAF  log10_distance      d  \\\n",
       "0        44        56  0.2893   0.2893    0.0000        1.995635  1.000   \n",
       "1        52        48  0.1203   0.1143    0.0060        2.107210  1.000   \n",
       "2        58        42  0.0636   0.0636    0.0000        4.511456  1.000   \n",
       "3        51        49  0.4652   0.1876    0.2776        0.000000  0.500   \n",
       "4        56        44  0.4911   0.4583    0.0328        3.943890  1.000   \n",
       "5        50        50  0.1256   0.1256    0.3388        3.283979  1.000   \n",
       "6        49        51  0.4851   0.0895    0.3956        0.000000  0.500   \n",
       "7        46        54  0.4205   0.0954    0.3251        0.000000  0.500   \n",
       "8        43        57  0.3141   0.3131    0.0010        5.762318  0.705   \n",
       "9        57        43  0.3062   0.2028    0.1034        0.000000  0.500   \n",
       "\n",
       "         r2  switch  \n",
       "0  1.000000       0  \n",
       "1  0.912000       0  \n",
       "2  1.000000       0  \n",
       "3  0.215599       0  \n",
       "4  0.876000       0  \n",
       "5  0.146000       0  \n",
       "6  0.215599       0  \n",
       "7  0.215599       0  \n",
       "8  0.433000       0  \n",
       "9  0.215599       0  "
      ]
     },
     "execution_count": 181,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Simulate data using the function\n",
    "X_sim, y_sim, df_vis = simulate_reads_single_gene(\n",
    "    p=0.5, read_coverage=100, phasing_error=0.04, num_hets=10, max_hets=64, data=train_df\n",
    ")\n",
    "\n",
    "# Call the evaluation function\n",
    "predicted_p_values, rmse = evaluate_simulation(X_sim, y_sim, model, device)\n",
    "\n",
    "# Visualize the simulated data if needed\n",
    "df_vis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted binomial p values:\n",
      "[[0.9043281]]\n",
      "Root Mean Squared Error (RMSE): 0.8043\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>refCount</th>\n",
       "      <th>altCount</th>\n",
       "      <th>MAF</th>\n",
       "      <th>min_MAF</th>\n",
       "      <th>diff_MAF</th>\n",
       "      <th>log10_distance</th>\n",
       "      <th>d</th>\n",
       "      <th>r2</th>\n",
       "      <th>switch</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>6</td>\n",
       "      <td>94</td>\n",
       "      <td>0.2893</td>\n",
       "      <td>0.2893</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>1.995635</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>11</td>\n",
       "      <td>89</td>\n",
       "      <td>0.1203</td>\n",
       "      <td>0.1143</td>\n",
       "      <td>0.0060</td>\n",
       "      <td>2.107210</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.912000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4</td>\n",
       "      <td>96</td>\n",
       "      <td>0.0636</td>\n",
       "      <td>0.0636</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>4.511456</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>13</td>\n",
       "      <td>87</td>\n",
       "      <td>0.4652</td>\n",
       "      <td>0.1876</td>\n",
       "      <td>0.2776</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.500</td>\n",
       "      <td>0.215599</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7</td>\n",
       "      <td>93</td>\n",
       "      <td>0.4911</td>\n",
       "      <td>0.4583</td>\n",
       "      <td>0.0328</td>\n",
       "      <td>3.943890</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.876000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>8</td>\n",
       "      <td>92</td>\n",
       "      <td>0.1256</td>\n",
       "      <td>0.1256</td>\n",
       "      <td>0.3388</td>\n",
       "      <td>3.283979</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.146000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>9</td>\n",
       "      <td>91</td>\n",
       "      <td>0.4851</td>\n",
       "      <td>0.0895</td>\n",
       "      <td>0.3956</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.500</td>\n",
       "      <td>0.215599</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>11</td>\n",
       "      <td>89</td>\n",
       "      <td>0.4205</td>\n",
       "      <td>0.0954</td>\n",
       "      <td>0.3251</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.500</td>\n",
       "      <td>0.215599</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>8</td>\n",
       "      <td>92</td>\n",
       "      <td>0.3141</td>\n",
       "      <td>0.3131</td>\n",
       "      <td>0.0010</td>\n",
       "      <td>5.762318</td>\n",
       "      <td>0.705</td>\n",
       "      <td>0.433000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>10</td>\n",
       "      <td>90</td>\n",
       "      <td>0.3062</td>\n",
       "      <td>0.2028</td>\n",
       "      <td>0.1034</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.500</td>\n",
       "      <td>0.215599</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   refCount  altCount     MAF  min_MAF  diff_MAF  log10_distance      d  \\\n",
       "0         6        94  0.2893   0.2893    0.0000        1.995635  1.000   \n",
       "1        11        89  0.1203   0.1143    0.0060        2.107210  1.000   \n",
       "2         4        96  0.0636   0.0636    0.0000        4.511456  1.000   \n",
       "3        13        87  0.4652   0.1876    0.2776        0.000000  0.500   \n",
       "4         7        93  0.4911   0.4583    0.0328        3.943890  1.000   \n",
       "5         8        92  0.1256   0.1256    0.3388        3.283979  1.000   \n",
       "6         9        91  0.4851   0.0895    0.3956        0.000000  0.500   \n",
       "7        11        89  0.4205   0.0954    0.3251        0.000000  0.500   \n",
       "8         8        92  0.3141   0.3131    0.0010        5.762318  0.705   \n",
       "9        10        90  0.3062   0.2028    0.1034        0.000000  0.500   \n",
       "\n",
       "         r2  switch  \n",
       "0  1.000000       0  \n",
       "1  0.912000       0  \n",
       "2  1.000000       0  \n",
       "3  0.215599       0  \n",
       "4  0.876000       0  \n",
       "5  0.146000       0  \n",
       "6  0.215599       0  \n",
       "7  0.215599       0  \n",
       "8  0.433000       0  \n",
       "9  0.215599       0  "
      ]
     },
     "execution_count": 182,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Simulate data using the function\n",
    "X_sim, y_sim, df_vis = simulate_reads_single_gene(\n",
    "    p=0.1, read_coverage=100, phasing_error=0.04, num_hets=10, max_hets=64, data=train_df\n",
    ")\n",
    "\n",
    "# Call the evaluation function\n",
    "predicted_p_values, rmse = evaluate_simulation(X_sim, y_sim, model, device)\n",
    "\n",
    "# Visualize the simulated data if needed\n",
    "df_vis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'simulate_reads_single_gene' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Simulate data using the function\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m X_sim, y_sim, df_vis \u001b[38;5;241m=\u001b[39m \u001b[43msimulate_reads_single_gene\u001b[49m(\n\u001b[1;32m      3\u001b[0m     p\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.1\u001b[39m, read_coverage\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m100\u001b[39m, phasing_error\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.5\u001b[39m, num_hets\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10\u001b[39m, max_hets\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m64\u001b[39m, data\u001b[38;5;241m=\u001b[39mtrain_df\n\u001b[1;32m      4\u001b[0m )\n\u001b[1;32m      6\u001b[0m \u001b[38;5;66;03m# Call the evaluation function\u001b[39;00m\n\u001b[1;32m      7\u001b[0m predicted_p_values, rmse \u001b[38;5;241m=\u001b[39m evaluate_simulation(X_sim, y_sim, model, device)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'simulate_reads_single_gene' is not defined"
     ]
    }
   ],
   "source": [
    "# Simulate data using the function\n",
    "X_sim, y_sim, df_vis = simulate_reads_single_gene(\n",
    "    p=0.1, read_coverage=100, phasing_error=0.5, num_hets=10, max_hets=64, data=train_df\n",
    ")\n",
    "\n",
    "# Call the evaluation function\n",
    "predicted_p_values, rmse = evaluate_simulation(X_sim, y_sim, model, device)\n",
    "\n",
    "# Visualize the simulated data if needed\n",
    "df_vis\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
